docid,docsrc,docno,title,text,section_ann
2156,iir,iir-2156,1 Boolean Retrieval,"        1 1Boolean retrieval The meaning of the term information retrieval can be very broad. Just getting a credit card out of your wallet so that you can type in the card number is a form of information retrieval. However, as an academic field of study, information retrieval might be defined thus:  Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers). As defined in this way, information retrieval used to be an activity that only a few people engaged in: reference librarians, paralegals, and similar professional searchers. Now the world has changed, and hundreds of millions of people engage in information retrieval every day when they use a web search engine or search their email.1Information retrieval is fast becoming the dominant form of information access, overtaking traditional database style searching (the sort that is going on when a clerk says to you: “I’m sorry, I can only look up your order if you can give me your Order ID”). IR can also cover other kinds of data and information problems beyond that specified in the core definition above. The term “unstructured data” refers to data which does not have clear, semantically overt, easy- for-a-computer structure. It is the opposite of structured data, the canonical example of which is a relational database, of the sort companies usually use to maintain product inventories and personnel records. In reality, almost no data are truly “unstructured”. This is definitely true of all text data if you count the latent linguistic structure of human languages. But even accepting that the intended notion of structure is overt structure, most text has structure, such as headings and paragraphs and footnotes, which is commonly represented in documents by explicit markup (such as the coding underlying web 1. In modern parlance, the word “search” has tended to replace “(information) retrieval”; the term “search” is quite ambiguous, but in context we use the two synonymously.     21 Boolean retrieval pages). IR is also used to facilitate “semi structured” search such as finding a document where the title contains Java and the body contains threading. The field of information retrieval also covers supporting users in browsing or filtering document collections or further processing a set of retrieved documents. Given a set of documents, clustering is the task of coming up with a good grouping of the documents based on their contents. It is similar to arranging books on a bookshelf according to their topic. Given a set of topics, standing information needs, or other categories (such as suitability of texts for different age groups), classification is the task of deciding which class(es), if any, each of a set of documents belongs to. It is often approached by first manually classifying some documents and then hoping to be able to classify new documents automatically. Information retrieval systems can also be distinguished by the scale at which they operate, and it is useful to distinguish three prominent scales. In web search, the system has to provide search over billions of documents stored on millions of computers. Distinctive issues are needing to gather documents for indexing, being able to build systems that work efficiently at this enormous scale, and handling particular aspects of the web, such as the exploitation of hypertext and not being fooled by site providers manipulating page content in an attempt to boost their search engine rankings, given the commercial importance of the web. We focus on all these issues in Chapters 19–21. At the other extreme is personal information retrieval. In the last few years, consumer operating systems have integrated information retrieval (such as Apple’s Mac OS X Spotlight or Windows Vista’s Instant Search). Email programs usually not only provide search but also text classification: they at least provide a spam (junk mail) filter, and commonly also provide either manual or automatic means for classifying mail so that it can be placed directly into particular folders. Distinctive issues here include handling the broad range of document types on a typical personal computer, and making the search system maintenance free and sufficiently lightweight in terms of startup, processing, and disk space usage that it can run on one machine without annoying its owner. In between is the space of enterprise, institutional, and domain-specific search, where retrieval might be provided for collections such as a corporation’s internal documents, a database of patents, or research articles on biochemistry. In this case, the documents will typically be stored on centralized file systems and one or a handful of dedicated machines will provide search over the collection. This book contains techniques of value over this whole spectrum, but our coverage of some aspects of parallel and distributed search in web-scale search systems is comparatively light owing to the relatively small published literature on the details of such systems. However, outside of a handful of web search companies, a software developer is most likely to encounter the personal search and enterprise scenarios.     1.1 An example information retrieval problem 3 In this chapter we begin with a very simple example of an information retrieval problem, and introduce the idea of a term-document matrix (Section 1.1) and the central inverted index data structure (Section 1.2). We will then examine the Boolean retrieval model and how Boolean queries are processed (Sections 1.3 and 1.4). 1.1 An example information retrieval problem A fat book which many people own is Shakespeare’s Collected Works. Suppose you wanted to determine which plays of Shakespeare contain the words Brutus AND Caesar AND NOT Calpurnia. One way to do that is to start at the beginning and to read through all the text, noting for each play whether it contains Brutus and Caesar and excluding it from consideration if it contains Calpurnia. The simplest form of document retrieval is for a computer to do this sort of linear scan through documents. This process is commonly referred to as grepping through text, after the Unix command grep, which performs this process. Grepping through text can be a very effective process, especially given the speed of modern computers, and often allows useful possibilities for wildcard pattern matching through the use of regular expressions. With modern computers, for simple querying of modest collections (the size of Shakespeare’s Collected Works is a bit under one million words of text in total), you really need nothing more. But for many purposes, you do need more: 1. To process large document collections quickly. The amount of online data has grown at least as quickly as the speed of computers, and we would now like to be able to search collections that total in the order of billions to trillions of words. 2. To allow more flexible matching operations. For example, it is impractical to perform the query Romans  countrymen with grep, where  might be defined as “within 5 words” or “within the same sentence”. 3. To allow ranked retrieval: in many cases you want the best answer to an information need among many documents that contain certain words. The way to avoid linearly scanning the texts for each query is to index the documents in advance. Let us stick with Shakespeare’s Collected Works, and use it to introduce the basics of the Boolean retrieval model. Suppose we record for each document – here a play of Shakespeare’s – whether it contains each word out of all the words Shakespeare used (Shakespeare used about 32,000 different words). The result is a binary term-document incidence matrix, as in Figure 1.1.Terms are the indexed units (further discussed in Section 2.2); they are usually words, and for the moment you can think of  ",1.1
2360,ies,ies-2360,1 Introduction," 1 Introduction
 ",
2369,mir2,mir2-2369,1 Introduction," Chapter 1
Introduction
 ",
2357,foa,foa-2357,1 Overview," Overview  Its about nothing.  Finding Out About. Reproduced by permission of The New Yorker *  ""What's the final episode  of ""Seinfeld' abovtV 1.1 Finding Out About - A Cognitive Activity  We are all forced to make decisions regularly, sometimes on the spur of  the moment. But the rest of the time we have enough warning that it is possible to collect our thoughts and do some research that makes our  * Robert Mankoff, 0 The New Yorker, 26 January 1998. FINDING OUT ABOUT  decision as sound as it can be. This book is a closer look at the process of finding out about (FOA), research activities that allow a decision-maker to draw on others' knowledge. It is written from a technical perspective, in terms of computational tools that speed the FOA activity in the modern era of the distributed networks of knowledge collectively known as the World Wide Web (WWW). It shows you how to build many of the tools that are useful for searching collections of text and other media. The primary argument advanced is that progress requires that we appreciate the cognitive foundation we bring to this task as academics, as language users, and even as adaptive organisms.  As organisms, we have evolved a wide range of strategies for seeking useful information about our environment. We use the term ""cognitive"" to highlight the use of internal representations that help even the simplest organisms perceive and respond to their world; as the organisms get less simple, their cognitive structures increase in complexity. Whether done by simple or complex organisms, however, the process of finding out about is a very active one - making initial guesses about good paths, using complex sets of features to decide if we seem to be on the right path, and proceeding forward.  As humans, we are especially expert at searching through one of the most complex environments of all: language. Its system of linguistic features is not derived from the natural world, at least not directly. It is a constructed, cultural system that has worked well since (by definition! ) prehistoric times. In part, languages remain useful because they are capable of change when necessary. ",
2358,foa,foa-2358,1.1 Finding Out About – a cognitive activity," Overview  Its about nothing.  Finding Out About. Reproduced by permission of The New Yorker *  ""What's the final episode  of ""Seinfeld' abovtV 1.1 Finding Out About - A Cognitive Activity  We are all forced to make decisions regularly, sometimes on the spur of  the moment. But the rest of the time we have enough warning that it is possible to collect our thoughts and do some research that makes our  * Robert Mankoff, 0 The New Yorker, 26 January 1998. FINDING OUT ABOUT  decision as sound as it can be. This book is a closer look at the process of finding out about (FOA), research activities that allow a decision-maker to draw on others' knowledge. It is written from a technical perspective, in terms of computational tools that speed the FOA activity in the modern era of the distributed networks of knowledge collectively known as the World Wide Web (WWW). It shows you how to build many of the tools that are useful for searching collections of text and other media. The primary argument advanced is that progress requires that we appreciate the cognitive foundation we bring to this task as academics, as language users, and even as adaptive organisms.  As organisms, we have evolved a wide range of strategies for seeking useful information about our environment. We use the term ""cognitive"" to highlight the use of internal representations that help even the simplest organisms perceive and respond to their world; as the organisms get less simple, their cognitive structures increase in complexity. Whether done by simple or complex organisms, however, the process of finding out about is a very active one - making initial guesses about good paths, using complex sets of features to decide if we seem to be on the right path, and proceeding forward.  As humans, we are especially expert at searching through one of the most complex environments of all: language. Its system of linguistic features is not derived from the natural world, at least not directly. It is a constructed, cultural system that has worked well since (by definition! ) prehistoric times. In part, languages remain useful because they are capable of change when necessary.  New features and new objects are noticed, and it becomes necessary for us to express new things about them, to form our reactions to them, and to express these reactions to one another.  Our first experience of language, as children and as a species, was oral - we spoke and listened. As children we learn Sprachspiele (word or language games) [Wittgenstein, 1953] - how to use language to get what we want. A baby saying ""Juice!"" is using the exclamation as a tool to make adults move; that's what a word means. Such a functional notion of language, in terms of the jobs it accomplishes, will prove central to our conception of what keywords in documents and queries mean as part of the FOA task.  Beyond the oral uses of language, as a species we have also learned the advantages of writing down important facts we might otherwise forget. OVERVIEW  Writing down a list of things to do, which we might forget tomorrow, extends our limited memory. Some of these advantages accrue to even a single individual: We use language personally, to organize our thoughts and to conceive strategies.  Even more important, we use writing to say things to others. Writing down important, memorable facts in a consistent, conventional manner, so that others can understand what we mean and vice versa, further amplifies the linguistic advantage. As a society, we value reading and writing skills because they let us interpret shared symbols and coordinate our actions. In advanced cultures' scholarship, entire curricula can be defined in terms of what Robert McHenry (Editor-in-Chief of Encyclopedia Britannica) calls ""KnowingHow to Know""1  It is easiest to think of the organism's or human's search as being for   a valuable object, sweet pieces of fruit in the jungle, or (in modern times) a grocer that sells them. But as language has played an increasingly important role in our society, searching for valuable written passages becomes an end unto itself. Especially as members of the academic community, we are likely to go to libraries seeking others' writings as part of our search. Here we find rows upon rows of books, each full of facts the author thought important, and endorsed by a librarian who has selected it. The authors are typically people far from our own time and place, using language similar but not identical to our own.  Of course the library contains many such books on many, many topics. We must Find Out About a topic of special interest, looking only for those things that are relevant to our search. This basic skill is a fundamental part of an academic's job:  ï  We look for references in order to write a term paper.  ï  We read a textbook, looking for help in answering an exercise.  ï  We comb through scientific journals to see if a question has already been answered.  We know that if we find the right reference, the right paper, the right paragraph, our job will be made much easier. Language has become not only the means of our search, but its object as well  wwwjustanother. com/howtoknow FINDING OUT ABOUT  Today we can also search the World Wide Web (WWW) for others' opinions of music, movies, or software. Of course these examples are much less of an ""academic exercise""; Finding Out About such information commodities, and doing it consistently and well, is a skill on which the modern information society places high value indeed. But while the infrastructure forming the modern WWW is quite recent, the promise offered by truly connecting all the world's knowledge has been anticipated for some time, for example, by H. G. Wells [Wells, 1938].  Many of the FOA searching techniques we will discuss in this text have been designed to operate on vast collections of apparently ""dead"" linguistic objects: files full of old email messages, CD-ROMs full of manuals or literature, Web servers fiill of technical reports, and so on. But at their core, each of these collections is evidence of real, vital attempts to communicate. Typically an author (explicitly or implicitly) anticipates the interests of some imagined audience and produces text that is a balance between what the author wants to say and what he or she thinks the audience wants to hear. A textual corpus will contain many such documents, written by many different authors, in many styles and for many different purposes. A person searching through such a corpus comes with his or her own purposes and may well use language in a different way from any of the authors. But each individual linguistic expression - the authors' attempts to write, the searchers' attempts to express their questions and then read the authors' documents - must be appreciated for the word games [Wittgenstein, 1953] that they are. FOA is centrally concerned with meaning: the semantics of the words, sentences, questions, and documents involved. We cannot tell if a document is about a topic unless we understand (at least something of) the semantics of the document and the topic. This is the notion of about-ness most typical within the tradition of library science [Hutchins, 1978].  This means that our attempts to engineer good technical solutions must be informed by, and can contribute to, a broader philosophy of language. For examples it will turn out that FOA's concern with the semantics of entire documents is well complemented by techniques from computational linguistics, which have tended to focus on syntactic analysis of individual sentences. But even more exciting is the fact that the recent OVERVIEW       5  Questioner  Question answerer  Question  Answer  Assessment  FIGURE 1.1 The FOA Conversation Loop.  availability of new types of electronic artifacts - from email messages and WWW corpora to the browsing behaviors of millions of users all trying to FOA - brings an empirical grounding for new theories of language that may well be revolutionary.  At its core, the FOA process of browsing readers can be imagined to involve three phases:  1.  asking a question;  2.  constructing an answer; and  3.  assessing the answer.  This conversational loop is sketched in Figure 1.1.  Step 1. Asking a Question  The first step is initiated by people who (anticipating our interest in building a search engine) we'll call users, and their questions. We don't know a lot about these people, but we do know they are in a particular frame of mind, a special cognitive state; they may be aware^ of a specific  gap in their knowledge (or they be only vaguely puzzled), and they're motivated to fill it. They want to FOA ... some topic.  Supposing for a moment that we were there to ask, the users may not  even be able to characterize the topic, that is, to articulate their knowledge gap. More precisely, they may not be able to fully define characteristics of the ""answer"" they seek. A paradoxical feature of the FOA problem is  Meta-cognition about  ignorance FINDING OUT ABOUT  that if users knew their question,  precisely, they might not even need the search engine we are designing: Forming a clearly posed question is often the hardest part of answering it! In any case, well call this somewhat befuddled but not uncommon cognitive state the users' information need.  While a bit confused about their particular question, the users are not without resources. First, they can typically take their ill-defined, internal cognitive state and turn it into an external expression of their question, in some language. We'll call their expression the query, and the language in which it is constructed the query language.  Step 2. Constructing an Answer  So much for the source of the question; whence the answer? If the question is being asked of a person, we must worry about equally complex characteristics of the answerer's cognitive state:  ï  Can they translate the user's ill-formed question into a better one?  ï  Do they know the answer themselves?  ï  Are they able to verbalize this answer?  ï  Can they give the answer in terms the user will understand?  ï  Can they provide the necessary background knowledge for the user to understand the answer itself?  We will refer to the question-answerer as the search engine, a computer program that algorithmically performs this task. Immediately each  of the concerns (just listed) regarding the human answerer's cognitive state translates into extremely ambitious demands we might make of our computer system.  Throughout most of this book, we will avoid such ambitious issues and instead consider a very restricted form of the FOA problem: We will assume that the search engine has available to it only a set of preexisting, ""canned"" passages of text and that its response is limited to identifying one or more of these passages and presenting them to the users; see Figure 1.2. We will call each of these passages a document and the entire set of documents the corpus. Especially when the corpus is very large (e.g., assume it contains millions or even billions of documents), selecting a very small set (say 10 to 20) of these as potentially good answers to OVERVIEW  Corpus FIGURE 1.2 Retrieval of Documents in Response to a Query  be retrieved will prove sufficiently difficult (and practically important) that we will focus on it for the first few chapters of this book. In the final chapters however, we will consider how this basic functionality can be extended towards tools for ""Searching for an education"" (cf. Section 8.3.9).  Step 3. Assessing the Answer  Imagine a special instance of the FOA problem: You are the user, waiting in line to ask a question of a professor. You're confused about a topic that is sure to be on the final exam. When you finally get your chance to ask your question, we'll assume that the professor does nothing but select the three or four preformed pearls of wisdom he or she thinks come closest to your need, delivers these ""documents,"" and sends you on your way. ""But wait!"" you want to say. ""That isn't what I meant"" Or, ""Let me ask it another way."" Or, ""That helps, but I still have this problem.""  The third and equally important phase of the FOA process ""closes the loop"" between asker and answerer, whereby the user (asker) provides an assessment of how relevant they find the answer provided. If after your first question and the professor's initial answer you are summarily ushered out of the office, you have a perfect right to be angry because the FOA process has been violated. FOA is a dialog between asker and answerer; it does not end with the search engine's first delivery of an answer. This initial exchange is only the first iteration of an ongoing conversation by which asker and answerer mutually negotiate a satisfactory exchange. In the process, the asker may recognize elements of the answer he or she 8       FINDING OUT ABOUT  FIGURE 1.3 Assessment of the Retrieval  What FOA data can we observe?  seeks and be able to reexpress the information need in terms of threads taken from previous answers.  Because the question-answerer has been restricted to a simple set of documents, the asker's relevance feedback must be similarly constrained; for each of the documents retrieved by the search engine, the asker reacts by saying whether or not the document is relevant. Returning to the student/professor scenario, we can imagine this as the student saying ""Thanks, that helps"" after those pearls that do and remaining silent or saying, ""Huh?"" or ""What does that have to do with anything?!"" or ""No, that's not what I meant!"" otherwise. More precisely, relevance feedback gives askers the opportunity to provide more information with their reaction to each retrieved document - whether it  is relevant (©), irrelevant (©), or neutral (#). This is shown as a Venn diagram-like labeling of the set of retrieved documents in Figure 1.3. We'll worry about just how to solicit and make use of relevance feedback judgments in Chapter 4.t
foa-0003	 ",
2370,mir2,mir2-2370,1.1 Information Retrieval," 1.1 Information Retrieval
Information retrieval (IR) is a broad area of Computer Science focused primarily on
providing the users with easy access to information of their interest, as follows.
Information retrieval deals with the representation, storage, organization
of, and access to information items such as documents, Web pages, online
catalogs, structured and semi-structured records, multimedia objects. The
representation and organization of the information items should be such
as to provide the users with easy access to information of their interest.
In terms of scope, the area has grown well beyond its early goals of indexing text
and searching for useful documents in a collection. Nowadays, research in IR includes
modeling, Web search, text classification, systems architecture, user interfaces, data
visualization, filtering, languages.
In terms of research, the area may be studied from two rather distinct and complementary
points of view: a computer-centered one and a human-centered one. In
the computer-centered view, IR consists mainly of building up efficient indexes, processing
user queries with high performance, and developing ranking algorithms to
improve the results. In the human-centered view, IR consists mainly of studying the
behavior of the user, of understanding their main needs, and of determining how such
understanding affects the organization and operation of the retrieval system. In this
book, we focus mainly on the computer-centered view of IR, which is dominant in
academia and in the market place.
1.1.1 Early Development ",
2361,ies,ies-2361,1.1 What Is Information Retrieval," 1.1 What Is Information Retrieval?
Information retrieval (IR) is concerned with representing, searching, and manipulating large
collections of electronic text and other human-language data. IR systems and services are now
widespread, with millions of people depending on them daily to facilitate business, education,
and entertainment. Web search engines — Google, Bing, and others — are by far the most
popular and heavily used IR services, providing access to up-to-date technical information,
locating people and organizations, summarizing news and events, and simplifying comparison
shopping. Digital library systems help medical and academic researchers learn about new journal
articles and conference presentations related to their areas of research. Consumers turn to local
search services to find retailers providing desired products and services. Within large companies,
enterprise search systems act as repositories for e-mail, memos, technical reports, and other
business documents, providing corporate memory by preserving these documents and enabling
access to the knowledge contained within them. Desktop search systems permit users to search
their personal e-mail, documents, and files.
1.1.1 Web Searc ",
2371,mir2,mir2-2371,1.1.1 Early Developments," 1.1.1 Early Developments
For more than 5,000 years, man has organized information for later retrieval and
searching. In its most usual form, this has been done by compiling, storing, organizing,  2 INTRODUCTION
and indexing clay tablets, hieroglyphics, papyrus rolls, and books. For holding the
various items, special purpose buildings called libraries, from the Latin word liber for
book, or bibliothekes, from the Greek word biblion for papyrus roll, are used.
The oldest known library was created in Elba, in the “Fertile Crescent”, currently
northern Syria, some time between 3,000 and 2,500 BC. In the seventh century BC,
Assyrian king Ashurbanipal created the library of Nineveh, on the Tigris River (today,
north of Iraq), which contained more than 30,000 clay tablets at the time of its
destruction in 612 BC. By 300 BC, Ptolemy Soter, a Macedonian general, created
the Great Library in Alexandria – the Egyptian city at the mouth of the Nile named
after the Macedonian king Alexander the Great (356-323 BC). For seven centuries
the Great Library, jointly with other major libraries in the city, made Alexandria the
intellectual capital of the Western world [1164].
Since then, libraries have expanded and flourished. Nowadays, they are everywhere.
They constitute the collective memory of the human race and their popularity
is in the rising. In 2008 alone, people in the US visited their libraries some 1.3 billion
times and checked out more than 2 billion items – an increase in both yearly figures
of more than 10 percent [155].
Since the volume of information in libraries is always growing, it is necessary to
build specialized data structures for fast search – the indexes. In one form or another,
indexes are at the core of every modern information retrieval system. They provide
fast access to the data and allow speeding up query processing, as we discuss in
Chapter 9.
For centuries indexes have been created manually as sets of categories. Each
category in the index is typically composed of labels that identify its associated topics
and of pointers to the documents that discuss those topics. While these indexes are
usually designed by library and information science researchers, the advent of modern
computers has allowed the construction of large indexes automatically, which has
accelerated the development of the area of Information Retrieval (IR).
Early developments in IR date back to research efforts conducted in the 50’s by pioneers
such as Hans Peter Luhn, Eugene Garfield, Philip Bagley, and Calvin Moores,
this last one having allegedly coined the term information retrieval [1692]. In 1955,
Allen Kent and colleagues published a paper describing the precision and recall metrics
[903], which was followed by the publication in 1962 of the Cranfield studies by
Cyril Cleverdon [394, 395]. In 1963, Joseph Becker and Robert Hayes published the
first book on information retrieval [164]. Throughout the 60’s, Gerard Salton and
Karen Sparck Jones, among others, shaped the field by developing the fundamental
concepts that led to the modern technologies of ranking in IR. In 1968, the first IR
book authored by Salton was published. In 1971, N. Jardine and C.J. Van Rijsbergen
articulated the “cluster hypothesis” [827]. In 1978, the first ACM Conference on IR
(ACM SIGIR) was held in Rochester, New York. In 1979, C.J. Van Rijsbergen published
Information Retrieval [1624], which focused on probabilistic models. In 1983,
Salton and McGill published Introduction to Modern Information Retrieval [1414], a
classic book on IR focused on vector models. Since then, the IR community has grown
to include thousands of professors, researchers, students, engineers, and practitioners
throughout the world. The main conference in the area, the ACM International Conference
on Information Retrieval (ACM SIGIR), now attracts hundreds of attendees
and receives hundreds of submitted papers on an yearly basis. ",
2362,ies,ies-2362,1.1.1 Web Search," 1.1.1 Web Search
Regular users of Web search engines casually expect to receive accurate and near-instantaneous
answers to questions and requests merely by entering a short query — a few words — into a text
box and clicking on a search button. Underlying this simple and intuitive interface are clusters
of computers, comprising thousands of machines, working cooperatively to generate a ranked
list of those Web pages that are likely to satisfy the information need embodied in the query.
These machines identify a set of Web pages containing the terms in the query, compute a score
for each page, eliminate duplicate and redundant pages, generate summaries of the remaining
pages, and finally return the summaries and links back to the user for browsing.
In order to achieve the subsecond response times expected from Web search engines, they
incorporate layers of caching and replication, taking advantage of commonly occurring queries
and exploiting parallel processing, allowing them to scale as the number of Web pages and
users increase. In order to produce accurate results, they store a “snapshot” of the Web. This
snapshot must be gathered and refreshed constantly by a Web crawler, also running on a cluster  1.1 What Is Information Retrieval? 3
of hundreds or thousands of machines, and downloading periodically — perhaps once a week —
a fresh copy of each page. Pages that contain rapidly changing information of high quality, such
as news services, may be refreshed daily or hourly.
Consider a simple example. If you have a computer connected to the Internet nearby, pause
for a minute to launch a browser and try the query “information retrieval” on one of the major
commercial Web search engines. It is likely that the search engine responded in well under a
second. Take some time to review the top ten results. Each result lists the URL for a Web page
and usually provides a title and a short snippet of text extracted from the body of the page.
Overall, the results are drawn from a variety of different Web sites and include sites associated
with leading textbooks, journals, conferences, and researchers. As is common for informational
queries such as this one, the Wikipedia article1 may be present. Do the top ten results contain
anything inappropriate? Could their order be improved? Have a look through the next ten
results and decide whether any one of them could better replace one of the top ten results.
Now, consider the millions of Web pages that contain the words “information” and “retrieval”.
This set of pages includes many that are relevant to the subject of information retrieval but are
much less general in scope than those that appear in the top ten, such as student Web pages and
individual research papers. In addition, the set includes many pages that just happen to contain
these two words, without having any direct relationship to the subject. From these millions of
possible pages, a search engine’s ranking algorithm selects the top-ranked pages based on a
variety of features, including the content and structure of the pages (e.g., their titles), their
relationship to other pages (e.g., the hyperlinks between them), and the content and structure
of the Web as a whole. For some queries, characteristics of the user such as her geographic
location or past searching behavior may also play a role. Balancing these features against each
other in order to rank pages by their expected relevance to a query is an example of relevance
ranking. The efficient implementation and evaluation of relevance ranking algorithms under a
variety of contexts and requirements represents a core problem in information retrieval, and
forms the central topic of this book.
 ",
2359,foa,foa-2359,1.1.1 Working within the IR Tradition," 1.1.1 Working within the IR Tradition  If it seems to you that the last section has sidestepped many of the most difficult issues underlying FOA, you're right! Later chapters will return to redress some of these omissions, but the immediate goal of Chapters 2 to 4 is to ""operationalize'3 FOA to resemble a well-studied problem within computer science, typically referred to as information retrieval (IR). IR is a field that has existed since computers were first OVERVIEW       9  used to count words [Belkin and Croft, 1987]. Even earlier, the related discipline of library science had developed many automated techniques for efficiently storing, cataloging, and retrieving physical materials so that browsing patrons could find them; many of these methods can be applied to the digital documents held within computers. IR has also borrowed heavily from the field of linguistics, especially computational linguistics.  The primary journals in the field and most important conferences^ Other places to in IR have continued to publish and meet since the 1960s, but the field has taken on new momentum within the last decade. Computers capable of searching and retrieving from the entire biomedical literature, across an entire nation's judicial system, or from all of the major newspaper and magazine articles, have created new markets among doctors, lawyers, journalists, students, everyone! And of course, the Internet, within just a few years, has generated many, many other examples of textual collections and people interested in searching through them.  The long tradition of IR is therefore the primary perspective from which we will approach FOA. Of course, every tradition brings with it tacit assumptions and preconceived notions that can hinder progress. In some ways, an elementary  1.1.1 Working within the IR Tradition  If it seems to you that the last section has sidestepped many of the most difficult issues underlying FOA, you're right! Later chapters will return to redress some of these omissions, but the immediate goal of Chapters 2 to 4 is to ""operationalize'3 FOA to resemble a well-studied problem within computer science, typically referred to as information retrieval (IR). IR is a field that has existed since computers were first OVERVIEW       9  used to count words [Belkin and Croft, 1987]. Even earlier, the related discipline of library science had developed many automated techniques for efficiently storing, cataloging, and retrieving physical materials so that browsing patrons could find them; many of these methods can be applied to the digital documents held within computers. IR has also borrowed heavily from the field of linguistics, especially computational linguistics.  The primary journals in the field and most important conferences^ Other places to in IR have continued to publish and meet since the 1960s, but the field has taken on new momentum within the last decade. Computers capable of searching and retrieving from the entire biomedical literature, across an entire nation's judicial system, or from all of the major newspaper and magazine articles, have created new markets among doctors, lawyers, journalists, students, everyone! And of course, the Internet, within just a few years, has generated many, many other examples of textual collections and people interested in searching through them.  The long tradition of IR is therefore the primary perspective from which we will approach FOA. Of course, every tradition brings with it tacit assumptions and preconceived notions that can hinder progress. In some ways, an elementary ",
2372,mir2,mir2-2372,1.1.2 Information Retrieval in Libraries and Digital Libraries," 1.1.2 Information Retrieval in Libraries and Digital Libraries
Libraries were among the first institutions to adopt IR systems for retrieving information.
Usually, library systems were initially developed by academic institutions
and later by commercial vendors. In the first generation, such systems consisted of
an automation of existing processes such as card catalogs searching, restricted to author
names and titles. In the second generation, increased search functionality was
added to include subject headings, keywords, and query operators. In the third generation,
which is currently being deployed, the focus has been on improved graphical
interfaces, electronic forms, hypertext features, and open system architectures.
Traditional library management system vendors include Endeavor Information
Systems Inc., Innovative Interfaces Inc., and EOS International. Among systems developed
with a research focus, we distinguish MELVYL developed by the California
Digital Library at University of California, and the Cheshire system developed originally
at UC Berkeley and lately in cooperation with the University of Liverpool.
Further details on these library systems can be found in Chapter 16.
1.1.3 IR at the Center of the Stage
Despite its maturity, until recently, IR was seen as a narrow area of interest restricted
mainly to librarians and information experts. Such a tendentious vision prevailed
for many years, despite the rapid dissemination, among users of modern personal
computers, of IR tools for multimedia and hypertext applications. In the beginning
of the 1990s, a single fact changed once and for all these perceptions – the introduction
of the World Wide Web.
The Web, invented in 1989 by Tim Berners-Lee, has become a universal repository
of human knowledge and culture. Its success is based on the conception of a standard
user interface which is always the same, no matter the computational environment
used to run the interface, and which allows any user to create their own documents. As
a result, millions of users have created billions of documents that compose the largest
human repository of knowledge in history. An immediate consequence is that finding
useful information on the Web is not always a simple task and usually requires posing
a query to a search engine, i.e., running a search. And search is all about IR and its
technologies. Thus, almost overnight, IR has gained a place with other technologies
at the center of the stage.
1.2 The IR Problem
Users of modern IR systems, such as search engine users, have information needs
of varying complexity. In the simplest case, they are looking for the link to the
homepage of a company, government, or institution. In the more sophisticated cases,
they are looking for information required to execute tasks associated with their jobs
or immediate needs. An example of a more complex information need is as follows:
Find all documents that address the role of the Federal Government in
financing the operation of the National Railroad Transportation Corporation
(AMTRAK).1
1This is topic 168 of the TREC reference collection, see Chapter 4. ",
2363,ies,ies-2363,1.1.2 Other Search Applications," 1.1.2 Other Search Applications
Desktop and file system search provides another example of a widely used IR application. A
desktop search engine provides search and browsing facilities for files stored on a local hard disk
and possibly on disks connected over a local network. In contrast to Web search engines, these
systems require greater awareness of file formats and creation times. For example, a user may
wish to search only within their e-mail or may know the general time frame in which a file was
created or downloaded. Since files may change rapidly, these systems must interface directly
with the file system layer of the operating system and must be engineered to handle a heavy
update load.  4 Information Retrieval: Implementing and Evaluating Search Engines · c MIT Press, 2010 · DRAFT
Lying between the desktop and the general Web, enterprise-level IR systems provide document
management and search services across businesses and other organizations. The details of these
systems vary widely. Some are essentially Web search engines applied to the corporate intranet,
crawling Web pages visible only within the organization and providing a search interface similar
to that of a standard Web search engine. Others provide more general document- and contentmanagement
services, with facilities for explicit update, versioning, and access control. In many
industries, these systems help satisfy regulatory requirements regarding the retention of e-mail
and other business communications.
Digital libraries and other specialized IR systems support access to collections of high-quality
material, often of a proprietary nature. This material may consist of newspaper articles, medical
journals, maps, or books that cannot be placed on a generally available Web site due to
copyright restrictions. Given the editorial quality and limited scope of these collections, it is
often possible to take advantage of structural features — authors, titles, dates, and other publication
data — to narrow search requests and improve retrieval effectiveness. In addition, digital
libraries may contain electronic text generated by optical character recognition (OCR) systems
from printed material; character recognition errors associated with the OCR output create yet
another complication for the retrieval process.
 ",
2373,mir2,mir2-2373,"1.1.3	IR at the Center of the Stage"," 1.1.2 Information Retrieval in Libraries and Digital Libraries
Libraries were among the first institutions to adopt IR systems for retrieving information.
Usually, library systems were initially developed by academic institutions
and later by commercial vendors. In the first generation, such systems consisted of
an automation of existing processes such as card catalogs searching, restricted to author
names and titles. In the second generation, increased search functionality was
added to include subject headings, keywords, and query operators. In the third generation,
which is currently being deployed, the focus has been on improved graphical
interfaces, electronic forms, hypertext features, and open system architectures.
Traditional library management system vendors include Endeavor Information
Systems Inc., Innovative Interfaces Inc., and EOS International. Among systems developed
with a research focus, we distinguish MELVYL developed by the California
Digital Library at University of California, and the Cheshire system developed originally
at UC Berkeley and lately in cooperation with the University of Liverpool.
Further details on these library systems can be found in Chapter 16.
1.1.3 IR at the Center of the Stage
Despite its maturity, until recently, IR was seen as a narrow area of interest restricted
mainly to librarians and information experts. Such a tendentious vision prevailed
for many years, despite the rapid dissemination, among users of modern personal
computers, of IR tools for multimedia and hypertext applications. In the beginning
of the 1990s, a single fact changed once and for all these perceptions – the introduction
of the World Wide Web.
The Web, invented in 1989 by Tim Berners-Lee, has become a universal repository
of human knowledge and culture. Its success is based on the conception of a standard
user interface which is always the same, no matter the computational environment
used to run the interface, and which allows any user to create their own documents. As
a result, millions of users have created billions of documents that compose the largest
human repository of knowledge in history. An immediate consequence is that finding
useful information on the Web is not always a simple task and usually requires posing
a query to a search engine, i.e., running a search. And search is all about IR and its
technologies. Thus, almost overnight, IR has gained a place with other technologies
at the center of the stage.
 ",
2364,ies,ies-2364,1.1.3 Other IR Applications," 1.1.3 Other IR Applications
While search is the central task within the area of information retrieval, the field covers a
wide variety of interrelated problems associated with the storage, manipulation, and retrieval
of human-language data:
• Document routing, filtering, and selective dissemination reverse the typical IR process.
Whereas a typical search application evaluates incoming queries against a given document
collection, a routing, filtering, or dissemination system compares newly created or
discovered documents to a fixed set of queries supplied in advance by users, identifying
those that match a given query closely enough to be of possible interest to the users. A
news aggregator, for example, might use a routing system to separate the day’s news into
sections such as “business,” “politics,” and “lifestyle,” or to send headlines of interest
to particular subscribers. An e-mail system might use a spam filter to block unwanted
messages. As we shall see, these two problems are essentially the same, although differing
in application-specific and implementation details.
• Text clustering and categorization systems group documents according to shared properties.
The difference between clustering and categorization stems from the information
provided to the system. Categorization systems are provided with training data illustrating
the various classes. Examples of “business,” “politics,” and “lifestyle” articles
might be provided to a categorization system, which would then sort unlabeled articles
into the same categories. A clustering system, in contrast, is not provided with training
examples. Instead, it sorts documents into groups based on patterns it discovers itself.  1.2 Information Retrieval Systems 5
• Summarization systems reduce documents to a few key paragraphs, sentences, or phrases
describing their content. The snippets of text displayed with Web search results represent
one example.
• Information extraction systems identify named entities, such as places and dates, and
combine this information into structured records that describe relationships between
these entities — for example, creating lists of books and their authors from Web data.
• Topic detection and tracking systems identify events in streams of news articles and
similar information sources, tracking these events as they evolve.
• Expert search systems identify members of organizations who are experts in a specified
area.
• Question answering systems integrate information from multiple sources to provide
concise answers to specific questions. They often incorporate and extend other IR
technologies, including search, summarization, and information extraction.
• Multimedia information retrieval systems extend relevance ranking and other IR
techniques to images, video, music, and speech.
Many IR problems overlap with the fields of library and information science, as well as with
other major subject areas of computer science such as natural language processing, databases,
and machine learning.
Of the topics listed above, techniques for categorization and filtering have the widest applicability,
and we provide an introduction to these areas. The scope of this book does not allow us
to devote substantial space to the other topics. However, all of them depend upon and extend
the basic technologies we cover.
1.2 Information Retrieval Systems
Most IR systems share a basic architecture and organization that is adapted to the requirements
of specific applications. Most of the concepts discussed in this book are presented in the context
of this architecture. In addition, like any technical field, information retrieval has its own jargon.
Words are sometimes used in a narrow technical sense that differs from their ordinary English
meanings. In order to avoid confusion and to provide context for the remainder of the book, we
briefly outline the fundamental terminology and technology of the subject.
1.2.1 Basic IR System Architecture
Figure 1.1 illustrates the major components in an IR system. Before conducting a search, a user
has an information need, which underlies and drives the search process. We sometimes refer
to this information need as a topic, particularly when it is presented in written form as part ",
2385,iir,iir-2385,1.2 A first take at building an inverted index," 1.2 A first take at building an inverted index To gain the speed benefits of indexing at retrieval time, we have to build the index in advance. The major steps in this are: 1. Collect the documents to be indexed: Friends, Romans, countrymen. So let it be with Caesar ... 2. Tokenize the text, turning each document into a list of tokens: Friends Romans countrymen So . . . 3. Some information retrieval researchers prefer the term inverted file, but expressions like index construction and index compression are much more common than inverted file construction and inverted file compression. For consistency, we use (inverted) index throughout this book. 4. In a (non-positional) inverted index, a posting is just a document ID, but it is inherently associated with a term, via the postings list it is placed on; sometimes we will also talk of a (term, docID) pair as a posting.   1.2 A first take at building an inverted index 7 Brutus −→ 1 2 4 11 31 45 173 174 Caesar −→ 1 2 4 5 6 16 57 132 . .. Calpurnia −→ 2 31 54 101 . . . |{z } | {z } Dictionary Postings ◮Figure 1.3 The two parts of an inverted index. The dictionary is commonly kept in memory, with pointers to each postings list, which is stored on disk. 3. Do linguistic preprocessing, producing a list of normalized tokens, which are the indexing terms: friend roman countryman so ... 4. Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings. We will define and discuss the earlier stages of processing, that is, steps 1–3, in Section 2.2 (page 22). Until then you can think of tokens and normalized tokens as also loosely equivalent to words. Here, we assume that the first 3 steps have already been done, and we examine building a basic inverted index by sort-based indexing. Within a document collection, we assume that each document has a unique serial number, known as the document identifier (docID). During index con-DOCID struction, we can simply assign successive integers to each new document when it is first encountered. The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure 1.4. The core indexing step is sorting this list so that the terms are alphabetical, giving us the representation in the middle column of Figure 1.4. Multiple occurrences of the same term from the same document are then merged.5Instances of the same term are then grouped, and the result is split into a dictionary and postings, as shown in the right column of Figure 1.4. Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. The dictionary also records some statistics, such as the number of documents which contain each term (the document frequency, which is here also the length of each postings list). This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the 5. Unix users can note that these steps are similar to use of the sort and then uniq commands.      81 Boolean retrieval Doc 1 Doc 2 I did enact Julius Caesar: I was killed i’ the Capitol; Brutus killed me. So let it be with Caesar. The noble Brutus hath told you Caesar was ambitious: term docID I 1 did 1 enact 1 julius 1 caesar 1 I 1 was 1 killed 1 i’ 1 the 1 capitol 1 brutus 1 killed 1 me 1 so 2 let 2 it 2 be 2 with 2 caesar 2 the 2 noble 2 brutus 2 hath 2 told 2 you 2 caesar 2 was 2 ambitious 2 =⇒ term docID ambitious 2 be 2 brutus 1 brutus 2 capitol 1 caesar 1 caesar 2 caesar 2 did 1 enact 1 hath 1 I 1 I 1 i’ 1 it 2 julius 1 killed 1 killed 1 let 2 me 1 noble 2 so 2 the 1 the 2 told 2 you 2 was 1 was 2 with 2 =⇒ term doc. freq. →postings lists ambitious 1 →2 be 1 →2 brutus 2 →1→2 capitol 1 →1 caesar 2 →1→2 did 1 →1 enact 1 →1 hath 1 →2 I 1 →1 i’ 1 →1 it 1 →2 julius 1 →1 killed 1 →1 let 1 →2 me 1 →1 noble 1 →2 so 1 →2 the 2 →1→2 told 1 →2 you 1 →2 was 2 →1→2 with 1 →2 ◮Figure 1.4 Building an index by sorting and grouping. The sequence of terms in each document, tagged by their documentID (left) is sorted alphabetically (middle). Instances of the same term are then grouped by word and then by documentID. The terms and documentIDs are then separated out (right). The dictionary stores the terms, and has a pointer to the postings list for each term. It commonly also stores other summary information such as, here, the document frequency of each term. We use this information for improving query time efficiency and, later, for weighting in ranked retrieval models. Each postings list stores the list of documents in which a term occurs, and may store other information such as the term frequency (the frequency of each term in each document) or the position(s) of the term in each document.   1.2 A first take at building an inverted index 9 search engine at query time, and it is a statistic later used in many ranked retrieval models. The postings are secondarily sorted by docID. This provides the basis for efficient query processing. This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search. In the resulting index, we pay for storage of both the dictionary and the postings lists. The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter 5we will examine how each can be optimized for storage and access efficiency. What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few. For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays. Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section 2.3), which require additional pointers. Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory caches. Extra pointers can in practice be encoded into the lists as offsets. If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse. We can also use a hybrid scheme with a linked list of fixed length arrays for each term. When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure 1.3), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory. ?Exercise 1.1 [⋆] Draw the inverted index that would be built for the following document collection. (See Figure 1.3 for an example.) Doc 1 new home sales top forecasts Doc 2 home sales rise in july Doc 3 increase in home sales in july Doc 4 july new home sales rise Exercise 1.2 [⋆] Consider these documents: Doc 1 breakthrough drug for schizophrenia Doc 2 new schizophrenia drug Doc 3 new approach for treatment of schizophrenia Doc 4 new hopes for schizophrenia patients a. Draw the term-document incidence matrix for this document collection.  ",1.2
2365,ies,ies-2365,1.2 Information Retrieval Systems," 1.2 Information Retrieval Systems 5
• Summarization systems reduce documents to a few key paragraphs, sentences, or phrases
describing their content. The snippets of text displayed with Web search results represent
one example.
• Information extraction systems identify named entities, such as places and dates, and
combine this information into structured records that describe relationships between
these entities — for example, creating lists of books and their authors from Web data.
• Topic detection and tracking systems identify events in streams of news articles and
similar information sources, tracking these events as they evolve.
• Expert search systems identify members of organizations who are experts in a specified
area.
• Question answering systems integrate information from multiple sources to provide
concise answers to specific questions. They often incorporate and extend other IR
technologies, including search, summarization, and information extraction.
• Multimedia information retrieval systems extend relevance ranking and other IR
techniques to images, video, music, and speech.
Many IR problems overlap with the fields of library and information science, as well as with
other major subject areas of computer science such as natural language processing, databases,
and machine learning.
Of the topics listed above, techniques for categorization and filtering have the widest applicability,
and we provide an introduction to these areas. The scope of this book does not allow us
to devote substantial space to the other topics. However, all of them depend upon and extend
the basic technologies we cover.
1.2 Information Retrieval Systems
Most IR systems share a basic architecture and organization that is adapted to the requirements
of specific applications. Most of the concepts discussed in this book are presented in the context
of this architecture. In addition, like any technical field, information retrieval has its own jargon.
Words are sometimes used in a narrow technical sense that differs from their ordinary English
meanings. In order to avoid confusion and to provide context for the remainder of the book, we
briefly outline the fundamental terminology and technology of the subject.
 ",
2374,mir2,mir2-2374,1.2 The IR Problem," 1.2 The IR Problem
Users of modern IR systems, such as search engine users, have information needs
of varying complexity. In the simplest case, they are looking for the link to the
homepage of a company, government, or institution. In the more sophisticated cases,
they are looking for information required to execute tasks associated with their jobs
or immediate needs. An example of a more complex information need is as follows:
Find all documents that address the role of the Federal Government in
financing the operation of the National Railroad Transportation Corporation
(AMTRAK).1
1This is topic 168 of the TREC reference collection, see Chapter 4.  4 INTRODUCTION
This full description of the user need does not necessarily provide the best formulation
for querying the IR system. Instead, the user might want to first translate this
information need into a query, or sequence of queries, to be posed to the system. In
its most common form, this translation yields a set of keywords, or index terms, which
summarize the user information need. Given the user query, the key goal of the IR
system is to retrieve information that is useful or relevant to the user. The emphasis
is on the retrieval of information as opposed to the retrieval of data.
To be effective in its attempt to satisfy the user information need, the IR system
must somehow ‘interpret’ the contents of the information items i.e., the documents in
a collection, and rank them according to a degree of relevance to the user query. This
‘interpretation’ of a document content involves extracting syntactic and semantic
information from the document text and using this information to match the user
information need.
The IR Problem: the primary goal of an IR system is to retrieve all the
documents that are relevant to a user query while retrieving as few nonrelevant
documents as possible.
The difficulty is knowing not only how to extract information from the documents
but also knowing how to use it to decide relevance. That is, the notion of relevance
is of central importance in IR.
One main issue is that relevance is a personal assessment that depends on the task
being solved and its context. For example, relevance can change with time (e.g., new
information becomes available), with location (e.g., the most relevant answer is the
closest one), or even with the device (e.g., the best answer is a short document that
is easier to download and visualize). In this sense, no IR system can provide perfect
answers to all users all the time.
 ",
2366,ies,ies-2366,1.2.1 Basic IR System Architecture," 1.2.1 Basic IR System Architecture
Figure 1.1 illustrates the major components in an IR system. Before conducting a search, a user
has an information need, which underlies and drives the search process. We sometimes refer
to this information need as a topic, particularly when it is presented in written form as part  6 Information Retrieval: Implementing and Evaluating Search Engines · c MIT Press, 2010 · DRAFT
Information Need
Index
User
Query
Result
Documents
Deletions Additions Search Engine
Figure 1.1 Components of an IR system.
of a test collection for IR evaluation. As a result of her information need, the user constructs
and issues a query to the IR system. Typically, this query consists of a small number of terms,
with two to three terms being typical for a Web search. We use “term” instead of “word”,
because a query term may in fact not be a word at all. Depending on the information need,
a query term may be a date, a number, a musical note, or a phrase. Wildcard operators and
other partial-match operators may also be permitted in query terms. For example, the term
“inform*” might match any word starting with that prefix (“inform”, “informs”, “informal”,
“informant”, “informative”, etc.).  1.2 Information Retrieval Systems 7
Although users typically issue simple keyword queries, IR systems often support a richer
query syntax, frequently with complex Boolean and pattern matching operators (Chapter 5).
These facilities may be used to limit a search to a particular Web site, to specify constraints
on fields such as author and title, or to apply other filters, restricting the search to a subset of
the collection. A user interface mediates between the user and the IR system, simplifying the
query-creation process when these richer query facilities are required.
The user’s query is processed by a search engine, which may be running on the user’s local
machine, on a large cluster of machines in a remote geographic location, or anywhere in between.
A major task of a search engine is to maintain and manipulate an inverted index for a document
collection. This index forms the principal data structure used by the engine for searching and
relevance ranking. As its basic function, an inverted index provides a mapping between terms
and the locations in the collection in which they occur. Because the size of an inverted list is on
the same order of magnitude as the document collection itself, care must be taken that index
access and update operations are performed efficiently.
To support relevance ranking algorithms, the search engine maintains collection statistics
associated with the index, such as the number of documents containing each term and the
length of each document. In addition, the search engine usually has access to the original
content of the documents, in order to report meaningful results back to the user.
Using the inverted index, collection statistics, and other data, the search engine accepts
queries from its users, processes these queries, and returns ranked lists of results. To perform
relevance ranking, the search engine computes a score, sometimes called a retrieval status value
(RSV), for each document. After sorting documents according to their scores, the result list
may be subjected to further processing, such as the removal of duplicate or redundant results.
For example, a Web search engine might report only one or two results from a single host or
domain, eliminating the others in favor of pages from different sources. The problem of scoring
documents with respect to a user’s query is one of the most fundamental in the field.
 ",
2375,mir2,mir2-2375,1.2.1 The User’s Task," 1.2.1 The User’s Task
The user of a retrieval system has to translate their information need into a query in
the language provided by the system. With an IR system, such as a search engine, this
usually implies specifying a set of words that convey the semantics of the information
need. We say that the user is searching or querying for information of their interest.
While searching for information of interest is the main retrieval task on the Web,
search can also be used for satisfying other user needs distinct from information
access, such as the buying of goods and the placing of reservations, as we discuss in
section 1.4.3.
Consider now a user who has an interest that is either poorly defined or inherently
broad, such that the query to specify is unclear. To illustrate, the user might be
interested in documents about car racing in general and might decide to glance related
documents about Formula 1 racing, Formula Indy, and the ‘24 Hours of Le Mans.’
We say that the user is browsing or navigating the documents in the collection, not
searching. It is still a process of retrieving information, but one whose main objectives
are less clearly defined in the beginning. The task in this case is more related to
exploratory search and resembles a process of quasi-sequential search for information
of interest.
In this book, we make a clear distinction between the different tasks the user of
the retrieval system might be engaged in. The task might be then of two distinct  types: searching and browsing, as illustrated in Figure 1.1. This two different tasks
are covered in detail in Chapter 2.
 ",
2367,ies,ies-2367,1.2.2 Documents and Update," 1.2.2 Documents and Update
Throughout this book we use “document” as a generic term to refer to any self-contained unit
that can be returned to the user as a search result. In practice, a particular document might be
an e-mail message, a Web page, a news article, or even a video. When predefined components of
a larger object may be returned as individual search results, such as pages or paragraphs from a
book, we refer to these components as elements. When arbitrary text passages, video segments,
or similar material may be returned from larger objects, we refer to them as snippets.
For most applications, the update model is relatively simple. Documents may be added or
deleted in their entirety. Once a document has been added to the search engine, its contents
are not modified. This update model is sufficient for most IR applications. For example, when
a Web crawler discovers that a page has been modified, the update may be viewed as a deletion
of the old page and an addition of the new page. Even in the context of file system search,
in which individual blocks of a file may be arbitrarily modified, most word processors and  Information Retrieval: Implementing and Evaluating Search Engines · c MIT Press, 2010 · DRAFT
other applications dealing with textual data rewrite entire files when a user saves changes. One
important exception is e-mail applications, which often append new messages to the ends of
mail folders. Because mail folders can be large in size and can grow quickly, it may be important
to support append operations.
 ",
2376,mir2,mir2-2376,1.2.2 Information versus Data Retrieval," 1.2.2 Information versus Data Retrieval
Data retrieval, in the context of an IR system, consists mainly of determining which
documents of a collection contain the keywords in the user query which, most frequently,
is not enough to satisfy the user information need. In fact, the user of an
IR system is concerned more with retrieving information about a subject than with
retrieving data that satisfies a given query. For instance, a user of an IR system is
willing to accept documents that contain synonyms of the query terms in the result
set, even when those documents do not contain any query terms. That is, in an IR
system the retrieved objects might be inaccurate and small errors are likely to go
unnoticed.
In a data retrieval system, on the contrary, a single erroneous object among a
thousand retrieved objects means total failure. A data retrieval system, such as a
relational database, deals with data that has a well defined structure and semantics,
while an IR system deals with natural language text which is not well structured.
Data retrieval, while providing a solution to the user of a database system, does not
solve the problem of retrieving information about a subject or topic.
1.3 The IR System
In this section we provide a high level view of the software architecture of an IR
system. We also intro ",
2368,ies,ies-2368,1.2.3 Performance Evaluation," 1.2.3 Performance Evaluation
There are two principal aspects to measuring IR system performance: efficiency and effectiveness.
Efficiency may be measured in terms of time (e.g., seconds per query) and space (e.g.,
bytes per document). The most visible aspect of efficiency is the response time (also known as
latency) experienced by a user between issuing a query and receiving the results. When many
simultaneous users must be supported, the query throughput, measured in queries per second,
becomes an important factor in system performance. For a general-purpose Web search engine,
the required throughput may range well beyond tens of thousands of queries per second. Effi-
ciency may also be considered in terms of storage space, measured by the bytes of disk and
memory required to store the index and other data structures. In addition, when thousands of
machines are working cooperatively to generate a search result, their power consumption and
carbon footprint become important considerations.
Effectiveness is more difficult to measure than efficiency, since it depends entirely on human
judgment. The key idea behind measuring effectiveness is the notion of relevance: A document
is considered relevant to a given query if its contents (completely or partially) satisfy the information
need represented by the query. To determine relevance, a human assessor reviews a
document/topic pair and assigns a relevance value. The relevance value may be binary (“relevant”
or “not relevant”) or graded (e.g., “perfect”, “excellent”, “good”, “fair”, “acceptable”,
“not relevant”, “harmful”).
The fundamental goal of relevance ranking is frequently expressed in terms of the Probability
Ranking Principle (PRP), which we phrase as follows:
If an IR system’s response to each query is a ranking of the documents in the collection
in order of decreasing probability of relevance, then the overall effectiveness of the system
to its users will be maximized.
This assumption is well established in the field of IR and forms the basis of the standard IR
evaluation methodology. Nonetheless, it overlooks important aspects of relevance that must be
considered in practice. In particular, the basic notion of relevance may be extended to consider
the size and scope of the documents returned. The specificity of a document reflects the degree
to which its contents are focused on the information need. A highly specific document consists
primarily of material related to the information need. In a marginally specific document, most
of the material is not related to the topic. The exhaustivity of a document reflects the degree
to which it covers the information related to the need. A highly exhaustive document provides
full coverage; a marginally exhaustive document may cover only limited aspects. Specificity  1.3 Working with Electronic Text 9
and exhaustivity are independent dimensions. A large document may provide full coverage but
contain enough extraneous material that it is only marginally specific.
When relevance is viewed in the context of a complete ranked document list, the notion of
novelty comes to light. Once the user examines the top-ranked document and learns its relevant
content, her information need may shift. If the second document contains little or no novel
information, it may not be relevant with respect to this revised information need.
1.3 Working with Electronic Text
Human-language data in the form of electronic text represents the raw material of information
retrieval. Building an IR system requires an understanding of both electronic text formats and
the characteristics of the text they encode.
1.3.1 Text Formats
The works of William Shakespeare provide a ready example of a large body of English-language
text with many electronic versions freely available on the Web. Shakespeare’s canonical works
include 37 plays and more than a hundred sonnets and poems. Figure 1.2 shows the start of the
first act of one play, Macbeth.
This figure presents the play as it might appear on a printed page. From the perspective of an
IR system, there are two aspects of this page that must be considered when it is represented in
electronic form, and ultimately when it is indexed by the system. The first aspect, the content
of the page, is the sequence of words in the order they might normally be read: “Thunder and
lightning. Enter three Witches First Witch When shall we...” The second aspect is the structure
of the page: the breaks between lines and pages, the labeling of speeches with speakers, the
stage directions, the act and scene numbers, and even the page number.
The content and structure of electronic text may be encoded in myriad document formats
supported by various word processing programs and desktop publishing systems. These formats
include Microsoft Word, HTML, XML, XHTML, LATEX, MIF, RTF, PDF, PostScript, SGML,
and others. In some environments, such as file system search, e-mail formats and even program
source code formats would be added to this list. Although a detailed description of these formats
is beyond our scope, a basic understanding of their impact on indexing and retrieval is important.
Two formats are of special interest to us. The first, HTML (HyperText Markup Language), is
the fundamental format for Web pages. Of particular note is its inherent support for hyperlinks,
which explicitly represent relationships between Web pages and permit these relationships to
be exploited by Web search systems. Anchor text often accompanies a hyperlink, partially
describing the content of the linked page. ",
2157,iir,iir-2157,1.3 Processing Boolean queries," 1.3 Processing Boolean queries How do we process a query using an inverted index and the basic Boolean retrieval model ? Consider processing the simple conjunctive query: SIMPLE CONJUNCTIVE QUERIES (1.1) Brutus AND Calpurnia over the inverted index partially shown in Figure 1.3 (page 7). We: 1\. Locate Brutus in the Dictionary 2\. Retrieve its postings 3\. Locate Calpurnia in the Dictionary 4\. Retrieve its postings 5\. Intersect the two postings lists, as shown in Figure 1.5. The intersection operation is the crucial one: we need to efficiently intersect postings lists so as to be able to quickly find documents that contain both terms. (This operation is sometimes referred to as merging postings lists: this slightly counterintuitive name reflects using the term merge algorithm for a general family of algorithms that combine multiple sorted lists by interleaved advancing of pointers through each; here we are merging the lists with a logical AND operation.) There is a simple and effective method of intersecting postings lists using the merge algorithm (see Figure 1.6): we maintain pointers into both lists   1.3 Processing Boolean queries 11 INTERSECT(p1,p2) 1answer ← h i 2while p16=NIL and p26=NIL 3do if docID(p1) = docID(p2) 4then ADD(answer,docID(p1)) 5p1←next(p1) 6p2←next(p2) 7else if docID(p1)&lt;docID(p2) 8then p1←next(p1) 9else p2←next(p2) 10 return answer ◮Figure 1.6 Algorithm for the intersection of two postings lists p1and p2. and walk through the two postings lists simultaneously, in time linear in the total number of postings entries. At each step, we compare the docID pointed to by both pointers. If they are the same, we put that docID in the results list, and advance both pointers. Otherwise we advance the pointer pointing to the smaller docID. If the lengths of the postings lists are x and y, the intersection takes O(x+y)operations. Formally, the complexity of querying is Θ(N), where Nis the number of documents in the collection.6 Our indexing methods gain us just a constant, not a difference in Θtime complexity compared to a linear scan, but in practice the constant is huge. To use this algorithm, it is crucial that postings be sorted by a single global ordering. Using a numeric sort by docID is one simple way to achieve this. We can extend the intersection operation to process more complicated queries like: (1.2) (Brutus OR Caesar)AND NOT Calpurnia Query optimization is the process of selecting how to organize the work of answering a query so that the least total amount of work needs to be done by the system. A major element of this for Boolean queries is the order in which postings lists are accessed. What is the best order for query processing? Consider a query that is an AND of tterms, for instance: (1.3) Brutus AND Caesar AND Calpurnia For each of the tterms, we need to get its postings, then AND them together. The standard heuristic is to process terms in order of increasing document 6. The notation Θ(·)is used to express an asymptotically tight bound on the complexity of an algorithm. Informally, this is often written as O(·), but this notation really expresses an asymptotic upper bound, which need not be tight (Cormen et al. 1990).      12 1 Boolean retrieval INTERSECT(ht1, . . . , tni) 1terms ←SORTBYINCREASINGFREQUENCY(ht1, . . . , tni) 2result ←postings(f irst(terms)) 3terms ←rest(terms) 4while terms 6=NIL and result 6=NIL 5do result ←INTERSECT(result,postings(f irst(terms))) 6terms ←rest(terms) 7return result ◮Figure 1.7 Algorithm for conjunctive queries that returns the set of documents containing each term in the input list of terms. frequency: if we start by intersecting the two smallest postings lists, then all intermediate results must be no bigger than the smallest postings list, and we are therefore likely to do the least amount of total work. So, for the postings lists in Figure 1.3 (page 7), we execute the above query as: (1.4) (Calpurnia AND Brutus)AND Caesar This is a first justification for keeping the frequency of terms in the dictionary: it allows us to make this ordering decision based on in-memory data before accessing any postings list. Consider now the optimization of more general queries, such as: (1.5) (madding OR crowd)AND (ignoble OR strife)AND (killed OR slain) As before, we will get the frequencies for all terms, and we can then (conservatively) estimate the size of each OR by the sum of the frequencies of its disjuncts. We can then process the query in increasing order of the size of each disjunctive term. For arbitrary Boolean queries, we have to evaluate and temporarily store the answers for intermediate expressions in a complex expression. However, in many circumstances, either because of the nature of the query language, or just because this is the most common type of query that users submit, a query is purely conjunctive. In this case, rather than viewing merging postings lists as a function with two inputs and a distinct output, it is more efficient to intersect each retrieved postings list with the current intermediate result in memory, where we initialize the intermediate result by loading the postings list of the least frequent term. This algorithm is shown in Figure 1.7. The intersection operation is then asymmetric: the intermediate results list is in memory while the list it is being intersected with is being read from disk. Moreover the intermediate results list is always at least as short as the other list, and in many cases it is orders of magnitude shorter. The postings   1.3 Processing Boolean queries 13 intersection can still be done by the algorithm in Figure 1.6, but when the difference between the list lengths is very large, opportunities to use alternative techniques open up. The intersection can be calculated in place by destructively modifying or marking invalid items in the intermediate results list. Or the intersection can be done as a sequence of binary searches in the long postings lists for each posting in the intermediate results list. Another possibility is to store the long postings list as a hashtable, so that membership of an intermediate result item can be calculated in constant rather than linear or log time. However, such alternative techniques are difficult to combine with postings list compression of the sort discussed in Chapter 5. Moreover, standard postings list intersection operations remain necessary when both terms of a query are very common. ?Exercise 1.4 [⋆] For the queries below, can we still run through the intersection in time O(x+y), where xand yare the lengths of the postings lists for Brutus and Caesar? If not, what can we achieve? a. Brutus AND NOT Caesar b. Brutus OR NOT Caesar Exercise 1.5 [⋆] Extend the postings merge algorithm to arbitrary Boolean query formulas. What is its time complexity? For instance, consider: c. (Brutus OR Caesar)AND NOT (Antony OR Cleopatra) Can we always merge in linear time? Linear in what? Can we do better than this? Exercise 1.6 [⋆⋆] We can use distributive laws for AND and OR to rewrite queries. a. Show how to rewrite the query in Exercise 1.5 into disjunctive normal form using the distributive laws. b. Would the resulting query be more or less efficiently evaluated than the original form of this query? c. Is this result true in general or does it depend on the words and the contents of the document collection? Exercise 1.7 [⋆] Recommend a query processing order for d. (tangerine OR trees)AND (marmalade OR skies)AND (kaleidoscope OR eyes) given the following postings list sizes:     14 1 Boolean retrieval Term Postings size eyes 213312 kaleidoscope 87009 marmalade 107913 skies 271658 tangerine 46653 trees 316812 Exercise 1.8 [⋆] If the query is: e. friends AND romans AND (NOT countrymen) how could we use the frequency of countrymen in evaluating the best query evaluation order? In particular, propose a way of handling negation in determining the order of query processing. Exercise 1.9 [⋆⋆] For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it isn’t. Exercise 1.10 [⋆⋆] Write out a postings merge algorithm, in the style of Figure 1.6 (page 11), for an xOR y query. Exercise 1.11 [⋆⋆] How should the Boolean query x y be handled? Why is naive evaluation of this query normally very expensive? Write out a postings merge algorithm that evaluates this query efficiently.  ",1.3
2377,mir2,mir2-2377,1.3 The IR System," 1.3 The IR System
In this section we provide a high level view of the software architecture of an IR
system. We also introduce the processes of retrieval and ranking of documents in
response to a user query.
1.3.1 Software Architecture of the IR System
To describe the IR system, we use a simple and generic software architecture as
shown in Figure 1.2. The first step in setting up an IR system is to assemble the ",
2378,mir2,mir2-2378,1.3.1 Software Architecture of the IR System," 1.3.1 Software Architecture of the IR System
To describe the IR system, we use a simple and generic software architecture as
shown in Figure 1.2. The first step in setting up an IR system is to assemble the  document collection, which can be private or be crawled from the Web. In the second
case a crawler module is responsible for collecting the documents, as we discuss in
Chapter 12. The document collection is stored in disk storage usually referred to as
the central repository. The documents in the central repository need to be indexed
for fast retrieval and ranking. The most used index structure is an inverted index
composed of all the distinct words of the collection and, for each word, a list of the
documents that contain it. Inverted indexes are discussed in Chapter 9.
Figure 1.2: High level software architecture of an IR system. Crawling is an additional
module required by Web IR systems, such as the search engines.
Given that the document collection is indexed, the retrieval process can be initiated.
It consists of retrieving documents that satisfy either a user query or a click in
a hyperlink. In the first case, we say that the user is searching for information of interest;
in the second case, we say that the user is browsing for information of interest.
In the remaining of this section, we use retrieval as it applies to the searching process.
For a more detailed discussion on browsing and how it compares to searching, see
Chapter 2.
To search, the user first specifies a query that reflects their information need.
Next, the user query is parsed and expanded with, for instance, spelling variants of
a query word. The expanded query, which we refer to as the system query, is then
processed against the index to retrieve a subset of all documents. Following, the
retrieved documents are ranked and the top documents are returned to the user.  document collection, which can be private or be crawled from the Web. In the second
case a crawler module is responsible for collecting the documents, as we discuss in
Chapter 12. The document collection is stored in disk storage usually referred to as
the central repository. The documents in the central repository need to be indexed
for fast retrieval and ranking. The most used index structure is an inverted index
composed of all the distinct words of the collection and, for each word, a list of the
documents that contain it. Inverted indexes are discussed in Chapter 9.
Figure 1.2: High level software architecture of an IR system. Crawling is an additional
module required by Web IR systems, such as the search engines.
Given that the document collection is indexed, the retrieval process can be initiated.
It consists of retrieving documents that satisfy either a user query or a click in
a hyperlink. In the first case, we say that the user is searching for information of interest;
in the second case, we say that the user is browsing for information of interest.
In the remaining of this section, we use retrieval as it applies to the searching process.
For a more detailed discussion on browsing and how it compares to searching, see
Chapter 2.
To search, the user first specifies a query that reflects their information need.
Next, the user query is parsed and expanded with, for instance, spelling variants of
a query word. The expanded query, which we refer to as the system query, is then
processed against the index to retrieve a subset of all documents. Following, the
retrieved documents are ranked and the top documents are returned to the user. ",
2379,mir2,mir2-2379,1.3.2 The Retrieval and Ranking Processes," document collection, which can be private or be crawled from the Web. In the second
case a crawler module is responsible for collecting the documents, as we discuss in
Chapter 12. The document collection is stored in disk storage usually referred to as
the central repository. The documents in the central repository need to be indexed
for fast retrieval and ranking. The most used index structure is an inverted index
composed of all the distinct words of the collection and, for each word, a list of the
documents that contain it. Inverted indexes are discussed in Chapter 9.
Figure 1.2: High level software architecture of an IR system. Crawling is an additional
module required by Web IR systems, such as the search engines.
Given that the document collection is indexed, the retrieval process can be initiated.
It consists of retrieving documents that satisfy either a user query or a click in
a hyperlink. In the first case, we say that the user is searching for information of interest;
in the second case, we say that the user is browsing for information of interest.
In the remaining of this section, we use retrieval as it applies to the searching process.
For a more detailed discussion on browsing and how it compares to searching, see
Chapter 2.
To search, the user first specifies a query that reflects their information need.
Next, the user query is parsed and expanded with, for instance, spelling variants of
a query word. The expanded query, which we refer to as the system query, is then
processed against the index to retrieve a subset of all documents. Following, the
retrieved documents are ranked and the top documents are returned to the user.  the ranking process in great detail. The top ranked documents are then formatted
for presentation to the user. The formatting consists of retrieving the title of the
documents and generating snippets for them, i.e., text excerpts that contain the
query terms, which are then displayed to the user.
 ",
2158,iir,iir-2158,1.4 The extended Boolean model versus ranked retrieval," 1.4 The extended Boolean model versus ranked retrieval The Boolean retrieval model contrasts with ranked retrieval models such as the vector space model (Section 6.3), in which users largely use free text queries, that is, just typing one or more words rather than using a precise language with operators for building up query expressions, and the system decides which documents best satisfy the query. Despite decades of academic research on the advantages of ranked retrieval, systems implementing the Boolean retrieval model were the main or only search option provided by large commercial information providers for three decades until the early 1990s (approximately the date of arrival of the World Wide Web). However, these systems did not have just the basic Boolean operations (AND,OR, and NOT) which we have presented so far. A strict Boolean expression over terms with an unordered results set is too limited for many of the information needs that people have, and these systems implemented extended Boolean retrieval models by incorporating additional operators such as term proximity operators. A proximity operator is a way of specifying that two terms in a query   1.4 The extended Boolean model versus ranked retrieval 15 must occur close to each other in a document, where closeness may be measured by limiting the allowed number of intervening words or by reference to a structural unit such as a sentence or paragraph. ✎Example 1.1: Commercial Boolean searching: Westlaw. Westlaw (http://www.westlaw.com/) is the largest commercial legal search service (in terms of the number of paying subscribers), with over half a million subscribers performing millions of searches a day over tens of terabytes of text data. The service was started in 1975. In 2005, Boolean search (called “Terms and Connectors” by Westlaw) was still the default, and used by a large percentage of users, although ranked free text querying (called “Natural Language” by Westlaw) was added in 1992. Here are some example Boolean queries on Westlaw: Information need: Information on the legal theories involved in preventing the disclosure of trade secrets by employees formerly employed by a competing company. Query: ""trade secret"" /s disclos! /s prevent /s employe! Information need: Requirements for disabled people to be able to access a workplace. Query: disab! /p access! /s work-site work-place (employment /3 place) Information need: Cases about a host’s responsibility for drunk guests. Query: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest Note the long, precise queries and the use of proximity operators, both uncommon in web search. Submitted queries average about ten words in length. Unlike web search conventions, a space between words represents disjunction (the tightest binding operator), &amp; is AND and /s, /p, and /kask for matches in the same sentence, same paragraph or within kwords respectively. Double quotes give a phrase search (consecutive words); see Section 2.4 (page 39). The exclamation mark (!) gives a trailing wildcard query (see Section 3.2, page 51); thus liab! matches all words starting with liab. Additionally work-site matches any of worksite,work-site or work site; see Section 2.2.1 (page 22). Typical expert queries are usually carefully defined and incrementally developed until they obtain what look to be good results to the user. Many users, particularly professionals, prefer Boolean query models. Boolean queries are precise: a document either matches the query or it does not. This offers the user greater control and transparency over what is retrieved. And some domains, such as legal materials, allow an effective means of document ranking within a Boolean model: Westlaw returns documents in reverse chronological order, which is in practice quite effective. In 2007, the majority of law librarians still seem to recommend terms and connectors for high recall searches, and the majority of legal users think they are getting greater control by using them. However, this does not mean that Boolean queries are more effective for professional searchers. Indeed, experimenting on a Westlaw subcollection, Turtle (1994) found that free text queries produced better results than Boolean queries prepared by Westlaw’s own reference librarians for the majority of the information needs in his experiments. A general problem with Boolean search is that using AND operators tends to produce high precision but low recall searches, while using OR operators gives low precision but high recall searches, and it is difficult or impossible to find a satisfactory middle ground. In this chapter, we have looked at the structure and construction of a basic     16 1 Boolean retrieval inverted index, comprising a dictionary and postings lists. We introduced the Boolean retrieval model, and examined how to do efficient retrieval via linear time merges and simple query optimization. In Chapters 2–7we will consider in detail richer query models and the sort of augmented index structures that are needed to handle them efficiently. Here we just mention a few of the main additional things we would like to be able to do: 1\. We would like to better determine the set of terms in the dictionary and to provide retrieval that is tolerant to spelling mistakes and inconsistent choice of words. 2\. It is often useful to search for compounds or phrases that denote a concept such as “operating system”. As the Westlaw examples show, we might also wish to do proximity queries such as Gates NEAR Microsoft. To answer such queries, the index has to be augmented to capture the proximities of terms in documents. 3\. A Boolean model only records term presence or absence, but often we would like to accumulate evidence, giving more weight to documents that have a term several times as opposed to ones that contain it only once. To be able to do this we need term frequency information (the number of times a term occurs in a document) in postings lists. 4\. Boolean queries just retrieve a set of matching documents, but commonly we wish to have an effective method to order (or “rank”) the returned results. This requires having a mechanism for determining a document score which encapsulates how good a match a document is for a query. With these additional ideas, we will have seen most of the basic technology that supports ad hoc searching over unstructured information. Ad hoc searching over documents has recently conquered the world, powering not only web search engines but the kind of unstructured search that lies behind the large eCommerce websites. Although the main web search engines differ by emphasizing free text querying, most of the basic issues and technologies of indexing and querying remain the same, as we will see in later chapters. Moreover, over time, web search engines have added at least partial implementations of some of the most popular operators from extended Boolean models: phrase search is especially popular and most have a very partial implementation of Boolean operators. Nevertheless, while these options are liked by expert searchers, they are little used by most people and are not the main focus in work on trying to improve web search engine performance. ?Exercise 1.12 [⋆] Write a query using Westlaw syntax which would find any of the words professor, teacher, or lecturer in the same sentence as a form of the verb explain.     1.5 References and further reading 17 Exercise 1.13 [⋆] Try using the Boolean search features on a couple of major web search engines. For instance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar AND burglar, and (iii) burglar OR burglar. Look at the estimated number of results and top hits. Do they make sense in terms of Boolean logic? Often they haven’t for major search engines. Can you make sense of what is going on? What about if you try different words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR conquer. What bound should the number of results from the first two queries place on the third query? Is this bound observed? 1.5 References and further reading The practical pursuit of computerized information retrieval began in the late 1940s (Cleverdon 1991,Liddy 2005). A great increase in the production of scientific literature, much in the form of less formal technical reports rather than traditional journal articles, coupled with the availability of computers, led to interest in automatic document retrieval. However, in those days, document retrieval was always based on author, title, and keywords; full-text search came much later. The article of Bush (1945) provided lasting inspiration for the new field: “Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, ‘memex’ will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.” The term Information Retrieval was coined by Calvin Mooers in 1948/1950 (Mooers 1950). In 1958, much newspaper attention was paid to demonstrations at a conference (see Taube and Wooster 1958) of IBM “auto-indexing” machines, based primarily on the work of H. P. Luhn. Commercial interest quickly gravitated towards Boolean retrieval systems, but the early years saw a heady debate over various disparate technologies for retrieval systems. For example Mooers (1961) dissented: “It is a common fallacy, underwritten at this date by the investment of several million dollars in a variety of retrieval hardware, that the algebra of George Boole (1847) is the appropriate formalism for retrieval system design. This view is as widely and uncritically accepted as it is wrong.” The observation of AND vs. OR giving you opposite extremes in a precision/ recall tradeoff, but not the middle ground comes from (Lee and Fox 1988).  ",1.4
2380,mir2,mir2-2380,1.4 The Web," 1.4 The Web
In this section we discuss the creation of the Web and its major implication – the
advent of the e-publishing age. We also discuss how the Web changed search, i.e., the
major impacts of the Web on the search task. At the end, we cover practical issues,
such as security and copyright, which derive directly from the massive presence of
millions of users on the Web ",
2381,mir2,mir2-2381,1.4.1 A Brief History," 1.4.1 A Brief History
At the end of World War II, US President Franklin Roosevelt asked Vannevar Bush,
then occupying very high level government positions, for recommendations on applications
of technologies learnt during the war to peace times. Bush first produced a
report entitled “Science, The Endless Frontier” which directly influenced the creation
of the National Science Foundation. Following, he wrote “As We May Think” [303],
a remarkable paper that discussed new hardware and software gadgets that could be
invented in the upcoming years. In Bush’s words,
Whole new forms of encyclopedias will appear, ready-made with a mesh
of associative trails running through them, ready to be dropped into the
memex and there amplified [303].
“As We May Think” influenced people like Douglas Engelbart who, at the Fall
Joint Computer Conference in San Francisco in December of 1968, ran a demonstration
in which he introduced the first ever computer mouse, video conferencing,
teleconferencing, and hypertext. It was so incredible that it became known as “the
mother of all demos” [1690]. Of the innovations displayed, the one that interests
us the most here is hypertext. The term was coined by Ted Nelson in his Project
Xanadu [1691].
Hypertext allows the reader to jump from one electronic document to another,
which was one important property regarding the problem Tim Berners-Lee faced in
1989. At the time, Berners-Lee worked in Geneva at the CERN – Conseil Europ´een
pour la Recherche Nucl´eaire. There, researchers who wanted to share documentation
with others had to reformat their documents to make them compatible with an internal
publishing system [803]. It was annoying and generated many questions, many
of which ended up been directed towards Berners-Lee. He understood that a better
solution was required.
It just so happened that CERN was the largest Internet node in Europe. BernersLee
reasoned that it would be nice if the solution to the problem of sharing documents
were decentralized, such that the researchers could share their contributions freely.
He saw that a networked hypertext, through the Internet, would be a good solution
and started working on its implementation. In 1990, he wrote the HTTP protocol,
defined the HTML language, wrote the first browser, which he called “World Wide
Web”, and the first Web server. In 1991, he made his browser and server software
available in the Internet. The Web was born.
 ",
2382,mir2,mir2-2382,1.4.2 The e-Publishing Era," 1.4.2 The e-Publishing Era
Since its inception, the Web became a huge success. The number of Web pages now
far exceeds 20 billion2 [487] and the number of Web users in the world exceeds 1.7
billion [815]. Further, it is known that there are more than one trillion distinct URLs
on the Web [651], even if many of them are pointers to dynamic pages, not static
HTML pages. Further, a viable model of economic sustainability based on online
advertising was developed [801].
2In its blog at http://googleblog.blogspot.com/2008/07/we-knew-web-was-big.html Google announced
to have collected over 1 trillion distinct URLs.  10 INTRODUCTION
The advent of the Web changed the world in a way that few people could have
anticipated. Yet, one has to wonder on the characteristics of the Web that have made
it so successful. Is there a single characteristic of the Web that was most decisive
for its success? Tentative answers to this question include the simple HTML markup
language, the low access costs, the wide spread reach of the Internet, the interactive
browser interface, the search engines. However, while providing the fundamental
infrastructure for the Web, these technologies were not the root cause of its popularity.
What was it then?
To emphasize the point we make here, let us wander through the life of a writer
who lived two hundred years ago.
She finished the first draft of her novel between 1796 and 1797. The first
attempt of publication was refused. The originals were eventually lost, so
she rewrote the novel in 1812 and it was finally published in 1813. Her
authorship was made anonymous under the reference “By a Lady” [400].
Pride and Prejudice is likely one of the three best loved books in the
UK, jointly with The Lord of the Rings and the Harry Potter series of
books. It has been the subject of six TV productions and five film versions
[1694]. The last of these, starring Keira Knightley and Matthew
Macfadyen, grossed over 100 million dollars worldwide and provided Ms.
Knightley with an academy award nomination [1693].
Jane Austen published anonymously her entire life. Throughout the twentieth
century, Austen’s novels have never been out of print. A variety of
print editions have appeared as a tribute to the popularity of Pride and
Prejudice.
The fundamental shift in human relationships, introduced by the Web, was freedom
to publish. Jane Austen did not have that freedom, so she had to either convince
a publisher of the quality of her work or pay for the publication of an edition of it
herself. Since she could not pay for it, she had to be patient and wait for the publisher
to become convinced. It took 15 years.
In the world of the Web, this is no longer the case. People can now publish their
ideas on the Web and reach millions of people over night, without paying anything for
it and without having to convince the editorial board of a large publishing company.
That is, restrictions imposed by mass communication media companies and by natural
geographical barriers were almost entirely removed by the invention of the Web, which
has led to a freedom to publish that marks the birth of a new era. One which we refer
to as The e-Publishing Era.
 ",
2383,mir2,mir2-2383,1.4.3 How the Web Changed Search," 1.4.3 How the Web Changed Search
Web search is today the most prominent application of IR and its techniques. Indeed,
the ranking and indexing components of any search engine are fundamentally IR
pieces of technology. An immediate consequence is that the Web has had a major
impact in the development of IR, as we now discuss.
The first major impact of the Web on search is related to the characteristics of the
document collection itself. The Web collection is composed of documents (or pages)
distributed over millions of sites and connected through hyperlinks , i.e., links that  associate a piece of text of a page with other Web pages. The inherent distributed
nature of the Web collection requires collecting all documents and storing copies of
them in a central repository, prior to indexing. This new phase in the IR process,
introduced by the Web, is called crawling and is extensively discussed in Chapter 12.
The second major impact of the Web on search is related to the size of the collection
and the volume of user queries submitted on a daily basis. Given that the Web grew
larger and faster than any previous known text collection, the search engines have now
to handle a volume of text that far exceeds 20 billion pages [487], i.e., a volume of text
much larger than any previous text collection. Further, the volume of user queries is
also much larger than ever before, even if estimates vary widely. The combination of
a very large text collection with a very high query traffic has pushed the performance
and scalability of search engines to limits that largely exceed those of any previous IR
system [151]. That is, performance and scalability have become critical characteristics
of the IR system, much more than they used to be prior to the Web. While we do
not discuss performance and scalability of search engines in this book, the reader is
referred to Chapter 11 for references on the topic (see section on bibliography).
The third major impact of the Web on search is also related to the vast size of the
document collection. In a very large collection, predicting relevance is much harder
than before. Basically, any query retrieves a large number of documents that match
its terms, which means that there are many noisy documents in the set of retrieved
documents. That is, documents that seem related to the query but are actually not
relevant to it according to the judgement of a large fraction of the users are retrieved.
This problem first showed up in the early Web search engines and became more severe
as the Web grew. Fortunately, the Web also includes new sources of evidence not
present in standard document collections that can be used to alleviate the problem,
such as hyperlinks and user clicks in documents in the answer set. In Chapter 11 we
discuss the issue of predicting relevance on the Web.
Two other major impacts of the Web on search derive from the fact that the Web
is not just a repository of documents and data, but also a medium to do business.
One immediate implication is that the search problem has been extended beyond the
seeking of text information to also encompass other user needs such as the price of a
book, the phone number of a hotel, the link for downloading a software. Providing
effective answers to these types of information needs frequently requires identifying
structured data associated with the object of interest such as price, location, or descriptions
of some of its key characteristics. These new classes of queries are discussed
in Chapter 7.
The fifth and final impact of the Web on search derives from Web advertising
and other economic incentives. The continued success of the Web as an interactive
media for the masses created incentives for its economic exploration in the form
of, for instance, advertising and electronic commerce. These incentives led also to
the abusive availability of commercial information disguised in the form of purely
informational content, which is usually referred to as Web spam. The increasingly
pervasive presence of spam on the Web has made the quest for relevance even more
difficult than before, i.e., spam content is sometimes so compelling that it is confused
with truly relevant content. Because of that, it is not unreasonable to think that
spam makes relevance negative, i.e., the presence of spam makes the current ranking
algorithms produce answers sets that are worst than they would be if the Web were  spam free. This difficulty is so large that today we talk of Adversarial Web Retrieval,
as we discuss in the section on spam in Chapter 11.
 ",
2384,mir2,mir2-2384,1.4.4 Practical Issues on the Web," 1.4.4 Practical Issues on the Web
Electronic commerce is a major trend on the Web nowadays and one which has
benefited millions of people. In an electronic transaction, the buyer usually submits to
the vendor credit information to be used for charging purposes. In its most common
form, such information consists of a credit card number. For security reasons, this
information is usually encrypted, as done by institutions and companies that deploy
automatic authentication processes.
Besides security, another issue of major interest is privacy. Frequently, people are
willing to exchange information as long as it does not become public. The reasons are
many, but the most common one is to protect oneself against misuse of private information
by third parties. Thus, privacy is another issue which affects the deployment
of the Web and which has not been properly addressed yet.
Two other important issues are copyright and patent rights. It is far from clear
how the wide spread of data on the Web affects copyright and patent laws in the
various countries. This is important because it affects the business of building up
and deploying large digital libraries. For instance, is a site which supervises all the
information it posts acting as a publisher? And if so, is it responsible for misuse of
the information it posts (even if it is not the source)?
Additionally, other practical issues of interest include scanning, optical character
recognition (OCR), and cross-language retrieval (in which the query is in one language
but the documents retrieved are in another language). In this book, however, we do
not cover practical issues in detail because it is not our main focus. The interested
reader is referred to the book by Lesk [1005].
1.5 Organization of the Book
1.5.1 Focus of the Book
Despite the increased interest in information retrieval, modern textbooks on IR with
a broad (and extensive) coverage of the various topics in the field are still difficult
to find. In an attempt to partially fulfill this gap, this book presents an overall view
of research in IR from a computer scientist’s perspective. This means that the focus
of the book is on computer algorithms and techniques used in IR systems. A rather
distinct viewpoint is taken by librarians and information science researchers, who
adopt a human-centered interpretation of the IR problem. In this interpretation, the
focus is on trying to understand how people interpret and use information as opposed
to how to structure, store, and retrieve information automatically. While most of
this book is dedicated to the computer scientist’s viewpoint of the IR problem, the
human-centered viewpoint is discussed in the user interfaces chapter and to some
extent in the last two chapters.
We put great emphasis on the integration of the different areas which are closely
related to IR and thus, should be treated together. For that reason, besides covering ",
2217,mir,mir-2217,10 User Interfaces and Visualization, 10 User Interfaces and Visualization by Marti A. Hearst 10.1 Intr ,
2218,mir,mir-2218,10.1 Introduction," 10.1 Introduction This chapter discusses user interfaces for communication between human information seekers and information retrieval systems. Information seeking is an imprecise process. When users approach an information access system they often have only a fuzzy understanding of how they can achieve their goals. Thus the user interface should aid in the understanding and expression of information needs. It should also help users formulate their queries, select among available information sources, understand search results, and keep track of the progress of their search. The human-computer interface is less well understood than other aspects of information retrieval, in part because humans are more complex than computer systems, and their motivations and behaviors are more difficult to measure and characterize. The area is also undergoing rapid change, and so the discussion in this chapter will emphasize recent developments rather than established wisdom. The chapter will first outline the human side of the information seeking process and then focus on the aspects of this process that can best be supported by the user interface. Discussion will encompass current practice and technology, recently proposed innovative ideas, and suggestions for future areas of development. Section 10.2 outlines design principles for human-computer interaction and introduces notions related to information visualization. section 10.3 describes information seeking models, past and present. The next four sections describe user interface support for starting the search process, for query specification, for viewing retrieval results in context, and for interactive relevance feedback. The last major section, section 10.8, describes user interface techniques to support the information access process as a whole. Section 10.9 speculates on future developments and Section 10.10 provides suggestions for further reading. Figure 10.1 presents the flow of the chapter contents. 257   .  ",
2261,mir,mir-2261,10.10 Bibliographic Discussion," 10.10 Bibliographic Discussion The field of human-computer interaction is a broad one, and this chapter touches on only a small subset of pertinent issues. For further information, see the excellent texts on user interface design by Shneiderman [725], information seeking behavior by Marchionini [542], and digital libraries by Lesk [501]. An excellent book on visual design is that of Mullet and Sano [580]. TUfte has written thoughtprovoking and visually engaging books on the power of information visualization [769, 770] and a collection of papers on information visualization has been edited by Card et al. [141]. This chapter has discussed many ideas for improving the human- computer interaction experience for information seekers. This is the most rapidly    BIBLIOGRAPHIC DISCUSSION 323 developing area of information access today, and improvements in the interface are likely to lead the way toward better search results and better-enabled information creators and users. Research in the area of human-computer interaction is difficult because the field is relatively new, and because it can be difficult to obtain strong results when running user studies. These challenges should simply encourage those who really want to influence the information access systems of tomorrow. Acknowledgements The author gratefully acknowledges the generous and helpful comments on the contents of this chapter by Gary Marchionini and Ben Shneiderman, the excellent administrative assistance of Barbara Goto, and the great faith and patience of Ricardo Baeza-Yates and Berthier Ribeiro-Neto.  ",
2219,mir,mir-2219,10.2 Human-Computer Interaction," 10.2 Human-Computer Interaction What makes an effective human-computer interface? Ben Shneiderman, an expert in the field, writes [725, p.lO]: Well designed, effective computer systems generate positive feelings of success, competence, mastery, and clarity in the user community. When an interactive system is well-designed, the interface almost disappears, enabling users to concentrate on their work, exploration, or pleasure. As steps towards achieving these goals, Shneiderman lists principles for design of user interfaces. Those which are particularly important for information access include (slightly restated): provide informative feedback, permit easy reversal of actions, support an internal locus of control, reduce working memory load, and provide alternative interfaces for novice and expert users. Each of these principles should be instantiated differently depending on the particular interface application. Below we discuss those principles that are of special interest to information access systems. 10.2.1 Design Principles Offer informative feedback. This principle is especially import   HUMAN-COMPUTER INTERACTION 259 users with feedback about the relationship between their query specification and documents retrieved, about relationships among retrieved documents, and about relationships between retrieved documents and metadata describing collections. If the user has control of how and when feedback is provided, then the system provides an internal locus of control. Reduce working memory load. Information access is an iterative process, the goals of which shift and change as information is encountered. One key way information access interfaces can help with memory load is to provide mechanisms for keeping track of choices made during the search process, allowing users to return to temporarily abandoned strategies, jump from one strategy to the next, and retain information and context across search sessions. Another memory-aiding device is to provide browsable information that is relevant to the current stage of the information access process. This includes suggestions of related terms or metadata, and search starting points including lists of sources and topic lists. Provide alternative interfaces for novice and expert users. An important tradeoff in all user interface design is that of simplicity versus power. Simple interfaces are easier to learn, at the expense of less flexibility and sometimes less efficient use. Powerful interfaces allow a knowledgeable user to do more and have more control over the operation of the interface, but can be timeconsuming to learn and impose a memory burden on people who use the system only intermittently. A common solution is to use a 'scaffolding' technique [684]. The novice user is presented with a simple interface that can be learned quickly and that provides the basic functionality of the application, but is restricted in power and flexibility. Alternative interfaces are offered for more experienced users, giving them more control, more options, and more features, or potentially even entirely different interaction models. Good user interface design provides intuitive bridges between the simple and the advanced interfaces. Information access interfaces must contend with special kinds of simplicity /power tradeoffs. One such tradeoff is the amount of information shown about the workings of the search system itself. Users who are new to a system or to a particular collection may not know enough about the system or the domain associated with the collection to make choices among complex features. They may not know how best to weight terms, or in the case of relevance feedback, not know what the effects of reweighting terms would be. On the other hand, users that have worked with a system and gotten a feeling for a topic are likely to be able to choose among suggested terms to add to their query in an informed manner. Determining how much information to show the user of the system is a major design choice in information access interfaces. 10.2.2 The Role of Visualization The tools of computer interface design are familiar to most computer users today: windows, menus, icons, dialog boxes, and so on. These make use of bitmapped display and computer graphics to provide a more accessible interface  ",
2220,mir,mir-2220,10.2.1 Design Principles,"  HUMAN-COMPUTER INTERACTION 259 users with feedback about the relationship between their query specification and documents retrieved, about relationships among retrieved documents, and about relationships between retrieved documents and metadata describing collections. If the user has control of how and when feedback is provided, then the system provides an internal locus of control. Reduce working memory load. Information access is an iterative process, the goals of which shift and change as information is encountered. One key way information access interfaces can help with memory load is to provide mechanisms for keeping track of choices made during the search process, allowing users to return to temporarily abandoned strategies, jump from one strategy to the next, and retain information and context across search sessions. Another memory-aiding device is to provide browsable information that is relevant to the current stage of the information access process. This includes suggestions of related terms or metadata, and search starting points including lists of sources and topic lists. Provide alternative interfaces for novice and expert users. An important tradeoff in all user interface design is that of simplicity versus power. Simple interfaces are easier to learn, at the expense of less flexibility and sometimes less efficient use. Powerful interfaces allow a knowledgeable user to do more and have more control over the operation of the interface, but can be timeconsuming to learn and impose a memory burden on people who use the system only intermittently. A common solution is to use a 'scaffolding' technique [684]. The novice user is presented with a simple interface that can be learned quickly and that provides the basic functionality of the application, but is restricted in power and flexibility. Alternative interfaces are offered for more experienced users, giving them more control, more options, and more features, or potentially even entirely different interaction models. Good user interface design provides intuitive bridges between the simple and the advanced interfaces. Information access interfaces must contend with special kinds of simplicity /power tradeoffs. One such tradeoff is the amount of information shown about the workings of the search system itself. Users who are new to a system or to a particular collection may not know enough about the system or the domain associated with the collection to make choices among complex features. They may not know how best to weight terms, or in the case of relevance feedback, not know what the effects of reweighting terms would be. On the other hand, users that have worked with a system and gotten a feeling for a topic are likely to be able to choose among suggested terms to add to their query in an informed manner. Determining how much information to show the user of the system is a major design choice in information access interfaces.  ",
2221,mir,mir-2221,10.2.2 The Role of Visualization," 10.2.2 The Role of Visualization The tools of computer interface design are familiar to most computer users today: windows, menus, icons, dialog boxes, and so on. These make use of bitmapped display and computer graphics to provide a more accessible interface    260 USER INTERFACES AND VISUALIZATION than command-line-based displays. A less familiar but growing area is that of information visualization, which attempts to provide visual depictions of very large information spaces. Humans are highly attuned to images and visual information [769, 456, 483]. Pictures and graphics can be captivating and appealing, especially if well designed. A visual representation can communicate some kinds of information much more rapidly and effectively than any other method. Consider the difference between a written description of a person's face and a photograph of it, or the difference between a table of numbers containing a correlation and a scatter plot showing the same information. The growing prevalence of fast graphics processors and high resolution color monitors is increasing interest in information visualization. Scientific visualization, a rapidly advancing branch of this field, maps physical phenomena onto two- or three-dimensional representations [433]. An example of scientific visualization is a colorful image of the pattern of peaks and valleys on the ocean floor; this provides a view of physical phenomena for which a photograph cannot (currently) be taken. Instead, the image is constructed from data that represent the underlying phenomena. Visualization of inherently abstract information is more difficult, and visualization of textually represented information is especially challenging. Language is our main means of communicating abstract ideas for which there is no obvious physical manifestation. What does a picture look like that describes negotiations over a trade agreement in which one party demands concessions on environmental policies while the other requires help in strengthening its currency? Despite the difficulties, researchers are attempting to represent aspects of the information access process using information visualization techniques. Some of these will be described later in this chapter. Aside from using icons and color highlighting, the main information visualization techniques include brushing and linking [233, 773], panning and zooming [71], focus-plus-context [502], magic lenses [95], and the use of animation to retain context and help make occluded information visible [676, 143]. These techniques support dynamic, interactive use. Interactivity seems to be an especially important property for visualizing abstract information, although it has not played as large a role within scientific visualization. Brushing and linking refers to the connecting of two or more views of the same data, such that a change to the representation in one view affects the representation in the other views as well. For example, say a display consists of two parts: a histogram and a list of titles. The histogram shows, for a set of documents, how many documents were published each year. The title list shows the titles for the corresponding documents. Brushing and linking would allow the user to assign a color, say red, to one bar of the histogram, thus causing the titles in the list display that were published during the corresponding year to also be highlighted in red. Panning and zooming refers to the actions of a movie camera that can scan sideways across a scene (panning) or move in for a closeup or back away to get a wider view (zooming). For example, text clustering can be used to show a    HUMAN-COMPUTER INTERACTION 261 top-level view of the main themes in a document collection (see Figures 10.7 and 10.8). Zooming can be used to move 'closer,' showing individual documents as icons, and then zoom in closer still to see the text associated with an individual document. When zooming is used, the more detail that is visible about a particular item, the less can be seen about the surrounding items. Focus-plus-contextis used to partly alleviate this effect. The idea is to make one portion of the view the focus of attention \- larger, while simultaneously shrinking the surrounding objects. The farther an object is from the focus of attention, the smaller it is made to appear, like the effect seen in a fisheye camera lens (also in some door peepholes) . Magic lenses are directly manipulable transparent windows that, when overlapped on some other data type, cause a transformation to be applied to the underlying data, thus changing its appearance (see Figure 10.13). The most straightforward application of magic lenses is for drawing tasks, and it is especially useful if used as a two-handed interface. For example, the left hand can be used to position a color lens over a drawing of an object. The right hand is used to mouse-click on the lens, thus causing the appearance of the underlying object to be transformed to the color specified by the lens. Additionally, there are a large number of graphical methods for depicting trees and hierarchies, some of which make use of animation to show nodes that would otherwise be occluded (hidden from view by other nodes) [286, 364, 407, 478,676]. It is often useful to combine these techniques into an interface layout consisting of an overview plus details [321, 644]. An overview, such as a table-ofcontents of a large manual, is shown in one window. A mouse-click on the title of the chapter causes the text of the chapter itself to appear in another window, in a linking action (see Figure 10.19). Panning and zooming or focus- plus-context can be used to change the view of the contents within the overview window.  ",
2222,mir,mir-2222,10.2.3 Evaluating Interactive Systems," 10.2.3 Evaluating Interactive Systems From the viewpoint of user interface design, people have widely differing abilities, preferences, and predilections. Important differences for information access interfaces include relative spatial ability and memory, reasoning abilities, verbal aptitude, and (potentially) personality differences [227, 725]. Age and cultural differences can contribute to acceptance or rejection of interface techniques [557]. An interface innovation can be useful and pleasing for some users, and foreign and cumbersome for others. Thus software design should allow for flexibility in interaction style, and new features should not be expected to be equally helpful for all users. An important aspect of human-computer interaction is the methodology for evaluation of user interface techniques. Precision and recall measures have been widely used for comparing the ranking results of non-interactive systems, but are less appropriate for assessing interactive systems [470]. The standard evaluations    262 USER INTERFACES AND VISUALIZATION emphasize high recall levels; in the TREe tasks systems are compared to see how well they return the top 1000 documents (see chapter 3). However, in many interactive settings, users require only a few relevant documents and do not care about high recall to evaluate highly interactive information access systems, useful metrics beyond precision and recall include: time required to learn the system, time required to achieve goals on benchmark tasks, error rates, and retention of the use of the interface over time. Throughout this chapter, empirical results of user studies are presented whenever they are available. Empirical data involving human users is time consuming to gather and difficult to draw conclusions from. This is due in part to variation in users' characteristics and motivations, and in part to the broad scope of information access activities. Formal psychological studies usually only uncover narrow conclusions within restricted contexts. For example, quantities such as the length of time it takes for a user to select an item from a fixed menu under various conditions have been characterized empirically [142]' but variations in interaction behavior for complex tasks like information access are difficult to account for accurately. Nielsen [605] advocates a more informal evaluation approach (called heuristic evaluation) in which user interface affordances are assessed in terms of more general properties and without concern about statistically significant results.  ",
2223,mir,mir-2223,10.3 The Information Access Process," 10.3 The Information Access Process A person engaged in an information seeking process has one or more goals in mind and uses a search system as a tool to help achieve those goals. Goals requiring information access can range quite widely, from finding a plumber to keeping informed about a business competitor, from writing a publishable scholarly article to investigating an allegation of fraud. Information access tasks are used to achieve these goals. These tasks span the spectrum from asking specific questions to exhaustively researching a topic. Other tasks fall between these two extremes. A study of business analysts [614] found three main kinds of information seeking tasks: monitoring a well known topic over time (such as researching competitors' activities each quarter), following a plan or stereotyped series of searches to achieve a particular goal (such as keeping up to date on good business practices), and exploring a topic in an undirected fashion (as when getting to know an unfamiliar industry). Although the goals differ, there is a common core revolving around the information seeking component, which is our focus here. 10.3.1 Models of Interaction Most accounts of the information access process assume an interaction cycle consisting of query specification, receipt and examination of retrieval results, and then either stopping or reformulating the query and repeating the process J  ",
2224,mir,mir-2224,10.3.1 Models of Interaction," 10.3.1 Models of Interaction Most accounts of the information access process assume an interaction cycle consisting of query specification, receipt and examination of retrieval results, and then either stopping or reformulating the query and repeating the process J    THE INFORMATION ACCESS PROCESS 263 until a perfect result set is found [700, 726]. In more detail, the standard process can be described according to the following sequence of steps (see Figure 10.2): (1) Start with an information need. (2) Select a system and collections to search on. (3) Formulate a query. (4) Send the query to the system. (5) Receive the results in the form of information items. (6) Scan, evaluate, and interpret the results. (7) Either stop, or, (8) Reformulate the query and go to step 4\. This simple interaction model (used by Web search engines) is the only model that most information seekers see today. This model does not take into account the fact that many users dislike being confronted with a long disorganized list of retrieval results that do not directly address their information needs. It also contains an underlying assumption that the user's information need is static and the information seeking process is one of successively refining a query until it retrieves all and only those documents relevant to the original information need. Refcnnulate No Figure 10.2 A simplified diagram of the standard model of the information access processes.    264 USER INTERFACES AND VISUALIZATION In actuality, users learn during the search process. They scan information, read the titles in result sets, read the retrieved documents themselves, viewing lists of topics related to their query terms, and navigating within hyperlinked Web sites. The recent advent of hyperlinks as a pivotal part of the information seeking process makes it no longer feasible to ignore the role of scanning and navigation within the search process itself. In particular, today a near-miss is much more acceptable than it was with bibliographic search, since an information seeker using the Web can navigate hyperlinks from a near-miss in the hopes that a useful page will be a few links away. The standard model also downplays the interaction that takes place when the user scans terms suggested as a result of relevance feedback, scans thesaurus structures, or views thematic overviews of document collections. It deemphasizes the role of source selection, which is increasingly important now that, for the first time, tens of thousands of information collections are immediately reachable for millions of people. Thus, while useful for describing the basics of information access systems, this simple interaction model is being challenged on many fronts [65, 614, 105, 365, 1921\. Bates [651 proposes the 'berry-picking' model of information seeking, which has two main points. The first is that, as a result of reading and learning from the information encountered throughout the search process, the users' information needs, and consequently their queries, continually shift. Information encountered at one point in a search may lead in anew, unanticipated direction. The original goal may become partly fulfilled, thus lowering the priority of one goal in favor of another. This is posed in contrast to the assumption of 'standard' information retrieval that the user's information need remains the same throughout the search process. The second point is that users' information needs are not satisfied by a single, final retrieved set of documents, but rather by a series of selections and bits of information found along the way. This is in contrast to the assumption that the main goal of the search process is to hone down the set of retrieved documents into a perfect match of the original information need. The berry-picking model is supported by a number of observational studies [236, 105], including that of O'Day and Jeffries [614]. They found that the information seeking process consisted of a series of interconnected but diverse searches on one problem-based theme. They also found that search results for a goal tended to trigger new goals, and hence search in new directions, but that the context of the problem and the previous searches was carried from one stage of search to the next. They also found that the main value of the search resided in the accumulated learning and acquisition of information that occurred during the search process, rather than in the final results set. Thus, a user interface for information access should allow users to reassess their goals and adjust their search strategy accordingly. A related situation occurs when users encounter a 'trigger' that causes them to pursue a different strategy temporarily, perhaps to return to the current unfinished activity at a later time. An implication of these observations is that the user interface should support search strategies by making it easy to follow trails with unanticipated results. This can be accomplished in part by supplying ways to record the progress    THE INFORMATION ACCESS PROCESS 265 of the current strategy and to store, find, and reload intermediate results, and by supporting pursuit of multiple strategies simultaneously. The user interface should also support methods for monitoring the status of the current strategy in relation to the user's current task and high-level goals. One way to cast the activity of monitoring the progress of a search strategy relative to a goal or subgoal is in terms of a cost/benefit analysis, or an analysis of diminishing returns [690]. This kind of analysis assumes that at any point in the search process, the user is pursuing the strategy that has the highest expected utility. If, as a consequence of some local tactical choices, another strategy presents itself as being of higher utility than the current one, the current one is (temporarily or permanently) abandoned in favor of the new strategy. There are a number of theories and frameworks that contrast browsing, querying, navigating, and scanning along several dimensions [75, 159, 542, 804]. Here we assume that users scan information structure, be it titles, thesaurus terms, hyperlinks, category labels, or the results of clustering, and then either select a displayed item for some purpose (to read in detail, to use as input to a query, to navigate to a new page of information) or formulate a query (either by recalling potential words or by selecting categories or suggested terms that have been scanned). In both cases, a new set of information is then made viewable for scanning. Queries tend to produce new, ad hoc collections of information that have not been gathered together before, whereas selection retrieves information that has already been composed or organized. Navigation refers to following a chain of links, switching from one view to another, toward some goal, in a sequence of scan and select operations. Browsing refers to the casual, mainly undirected exploration of information structures, and is usually done in tandem with selection, although queries can also be used to create sub collections to browse through. An important aspect of the interaction process is that the output of one action should be easily used as the input to the next.  ",
2225,mir,mir-2225,10.3.2 Non-Search Parts of the Information Access Process," 10.3.2 Non-Search Parts of the Information Access Process The O'Day and Jeffries study [614] found that information seeking is only one part of the full work process their subjects were engaged in. In between searching sessions many different kinds of work was done with the retrieved information, including reading and annotating [617] and analysis. O'Day and Jeffries examined the analysis steps in more detail, finding that 80% of this work fell into six. main types: finding trends, making comparisons, aggregating information, identifying a critical subset, assessing, and interpreting. The remaining 20% consisted of cross-referencing, summarizing, finding evocative visualizations for reports, and miscellaneous activities. The Sensemaking work of Russell et at. [690] also discusses information work as a process in which information retrieval plays only a small part. They observe that most of the effort made in Sensemaking is in the synthesis of a good representation, or ways of thinking about, the problem at hand. They describe the process of formulating and crystallizing the important concepts for a given task.    266 USER INTERFACES AND VISUALIZATION From these observations it is convenient to divide the entire information access process into two main components: search/retrieval, and analysis/synthesis of results. User interfaces should allow both kinds of activity to be tightly interwoven. However, analysis/synthesis are activities that can be done independently of information seeking, and for our purposes it is useful to make a distinction between the two types of activities.  ",
2226,mir,mir-2226,10.3.3 Earlier Interface Studies," 10.3.3 Earlier Interface Studies The bulk of the literature on studies of human-computer information seeking behavior concerns information intermediaries using online systems consisting of bibliographic records (e.g., [546, 707, 104]), sometimes with costs assessed per time unit. Unfortunately, many of the assumptions behind those studies do not reflect the conditions of modern information access [335, 222]. The differences include the following: • The text being searched now is often full text rather than bibliographic citations. Because users have access to full text, rather than document surrogates, it is more likely that simple queries will find relevant answers directly as part of the search process. • Modern systems use statistical ranking (which is more effective when abstracts and full text are available than when only titles and citations are available) whereas most studies were performed on Boolean systems. • Much of modern searching is done by end users, many new to online searching, rather than professional intermediaries, which were the focus of many of the earlier studies. • Tens of thousands of sources are now available online on networked information systems, and many are tightly coupled via hyperlinks, as opposed to being stored in separate collections owned by separate services. Earlier studies generally used systems in which moving from one collection to another required prior knowledge of the collections and considerable time and effort to switch. A near miss is much more useful in this hyperlinked environment than in earlier systems, since hyperlinks allow users to navigate from the near miss directly to the source containing information of interest. In a card catalog environment, where documents are represented as isolated units, a near miss consists of finding a book in the general area of interest and then going to the bookshelf in the library to look for related books, or obtaining copies of many issues of a journal and scanning for related articles. • Finally, most users have access to bit-mapped displays allowing for direct manipulation, or at least form fillin. Most earlier studies and bibliographic systems were implemented on TTY displays, which require command-line based syntax and do a poor job of retaining context.    STARTING POINTS 267 Despite these significant differences, some general information seeking strategies have been identified that seem to transfer across systems. Additionally, although modern systems have remedied many of the problems of earlier online public access catalogs, they also introduce new problems of their own.  ",
2227,mir,mir-2227,10.4 Starting Points," 10.4 Starting Points Search interfaces must provide users with good ways to get started. An empty screen or a blank entry form does not provide clues to help a user decide how to start the search process. Users usually do not begin by creating a long, detailed expression of their information need. Studies show that users tend to start out with very short queries, inspect the results, and then modify those queries in an incremental feedback cycle [22)., The initial query can be seen as a kind of 'testing the water' to see what kinds of results are returned and get an idea of how to reformulate the query [804,65). Thus, one task of an information access interface is to help users select the sources and collections to search on. For example, there are many different information sources associated with cancer, and there are many different kinds of information a user might like to know about cancer. Guiding the user to the right set of starting points can help with the initial problem formulation. Traditional bibliographic search assumes that the user begins by looking through a list of names of sources and choosing which collections to search on, while Web search engines obliterate the distinctions between sources and plunge the user into the middle of a Web site with little information about the relationship of the search hit to the rest of the collection. In neither case is the interface to the available sources particularly helpful. In this section we will discuss four main types of starting points: lists, overviews, examples, and automated source selection. 10.4.1 Lists of Collections Typical online systems such as LEXIS-NEXIS require users to begin any inquiry with a scan through a long list of source names and guess which ones will be of interest. Usually little information beyond the name of the collection is provided online for these sources (see Figure 10.3). If the user  ",
2228,mir,mir-2228,10.4.1 Lists of Collections," 10.4.1 Lists of Collections Typical online systems such as LEXIS-NEXIS require users to begin any inquiry with a scan through a long list of source names and guess which ones will be of interest. Usually little information beyond the name of the collection is provided online for these sources (see Figure 10.3). If the user is not satisfied with the results on one collection, they must reissue the query on another collection. Frequent searchers eventually learn a set of sources that are useful for their domains of interest, either through experience, formal training, or recommendations from friends and colleagues. Often-used sources can be stored on a 'favorites' list, also known as a bookmark list or a hotlist on the Web. Recent research explores the maintenance of a personalized information profile for users or work groups, based on the kinds of information they've used in the past [277). However, when users want to search outside their domains of expertise, a list of familiar sources is not sufficient. Professional searchers such as librarians    268 USER INTERFACES AND VISUALIZATION Figure 10.3 The LEXIS-NEXIS source selection screen. le arn t h r oug h exp e r ien ce a nd years of trai n ing which sources are app ro priate for vario us information needs . The restricted nature of traditional interfaces to info rmation collections discourages exploration and discovery of new useful sources. How ever , recently researchers have devised a number of mechanisms to help users understand the contents of collections as a way of getting started in thei r search. 10 .4 .2 Overviews Faced with a large set of text collections , how can a user choose which to begin with? O ne approach is to study an overview of the contents of the collections . An overview can show the topic domains represented within the collections , to he lp u ser s se lect or eliminate so urces from consideration . An overview can help u ser s get sta rted , directing t hem into general neighborhoods, after which they can nav igate using more detailed descriptions. Shneiderman [724 ] advocates an interaction mo del in which the user begins with an overview of the information to be worked w ith , then pans and zooms to find areas of potential interest , and then view details. The process is repeated as often as necessary . Three ty pes of ove rviews are discussed in this subsection . The first is dis \p lay a n d nav igat ion of lar ge to p ica l category hierarchies associated with the doc \ume nts of a co llection . T he second is automatica lly de rived overviews, us ually created by unsupe rvised clustering techniques on the text of documents , that attempt to extract overall characterizing themes from collections. The third type  ",
2229,mir,mir-2229,10.4.2 Overviews,"  268 USER INTERFACES AND VISUALIZATION Figure 10.3 The LEXIS-NEXIS source selection screen. le arn t h r oug h exp e r ien ce a nd years of trai n ing which sources are app ro priate for vario us information needs . The restricted nature of traditional interfaces to info rmation collections discourages exploration and discovery of new useful sources. How ever , recently researchers have devised a number of mechanisms to help users understand the contents of collections as a way of getting started in thei r search. 10 .4 .2 Overviews Faced with a large set of text collections , how can a user choose which to begin with? O ne approach is to study an overview of the contents of the collections . An overview can show the topic domains represented within the collections , to he lp u ser s se lect or eliminate so urces from consideration . An overview can help u ser s get sta rted , directing t hem into general neighborhoods, after which they can nav igate using more detailed descriptions. Shneiderman [724 ] advocates an interaction mo del in which the user begins with an overview of the information to be worked w ith , then pans and zooms to find areas of potential interest , and then view details. The process is repeated as often as necessary . Three ty pes of ove rviews are discussed in this subsection . The first is dis \p lay a n d nav igat ion of lar ge to p ica l category hierarchies associated with the doc \ume nts of a co llection . T he second is automatica lly de rived overviews, us ually created by unsupe rvised clustering techniques on the text of documents , that attempt to extract overall characterizing themes from collections. The third type    STARTING POINTS 269 of overview is that created by applying a variant of co-citation analysis on connections or links between different entities within a collection. Other kinds of overviews are possible, for example, showing graphical depictions of bookshelves or piles of books [681, 46]. Category or Directory Overviews There exist today many large online text collections to which category labels have been assigned. Traditional online bibliographic systems have for decades assigned subject headings to books and other documents [752]. MEDLINE, a large collection of biomedical articles, has associated with it Medical Subject Headings (MeSH) consisting of approximately 18,000 categories [523J. The Association for Computing Machinery (ACM) has developed a hierarchy of approximately 1200 category (keyword) labels.] Yahoo![839], one of the most popular search sites on the World Wide Web, organizes Web pages into a hierarchy consisting of thousands of category labels. The popularity of Yahoo! and other Web directories suggests that hierarchically structured categories are useful starting points for users seeking information on the Web. This popularity may reflect a preference to begin at a logical starting point, such as the home page for a set of information, or it may reflect a desire to avoid having to guess which words will retrieve the desired information. (It may also reflect the fact that directory services attempt to cull out low quality Web sites.) The meanings of category labels differ somewhat among collections. Most are designed to help organize the documents and to aid in query specification. Unfortunately, users of online bibliographic catalogs rarely use the available subject headings [335, 222]. Hancock-Beaulieu and Drabenstott and Weller, among others, put much of the blame on poor (command line-based) user interfaces which provide little aid for selecting subject labels and require users to scroll through long alphabetic lists. Even with graphical Web interfaces, finding the appropriate place within a category hierarchy can be a time-consuming task, and once a collection has been found using such a representation, an alternative means is required for searching within the site itself. Most interfaces that depict category hierarchies graphically do so by associating a document directly with the node of the category hierarchy to which it has been assigned. For example, clicking on a category link in Yahoo! brings up a list of documents that have been assigned that category label. Conceptually, the document is stored within the category label. When navigating the results of a search in Yahoo!, the user must look through a list of category labels and guess which one is most likely to contain references to the topic of interest. A wrong path requires backing up and trying again, and remembering which pages contain which information. If the desired information is deep in the hierarchy, or t http://www.acm.org/class    270 USER INTERFACES AND VISUALIZATION ..... , ,. . Figure 10.4 The MeSHBrowse interface for viewing category labels hierarchically [453J. not available at all, this can be a time-consuming and frustrating process. Because documents are conceptually stored 'inside' categories, users cannot create queries based on combinations of categories using this interface. It is difficult to design a good interface to integrate category selection into query specification, in part because display of category hierarchies takes up large amounts of screen space. For example, Internet Grateful Medt is a Web-based service that allows an integration of search with display and selection of MeSH category labels. After the user types in the name of a potential category label, a long list of choices is shown in a page. To see more information about a given label, the user selects a link (e.g., Radiation Injuries). The causes the context of the query to disappear because a new Web page appears showing the ancestors of the term and its immediate descendants. If the user attempts to see the siblings of the parent term (Wounds and Injuries) then a new page appears that changes the context again. Radiation Injuries appears as one of many siblings and its children can no long be seen. To go back to the query, the illustration of the category hierarchy disappears. The MeSHBrowse system [453] allows users to interactively browse a subset of semantically associated links in the MeSH hierarchy. From a given starting point, clicking on a category causes the associated categories to be displayed in a two-dimensional tree representation. Thus only the relevant subset of the t http://igm.nlm.nih.gov:80/    STA R T ING POI NT S 271 _. v O . 00_ n ..... ~...,Oougo ~ s.-_ H .... c.. 1 """"'. 01 _ .. 5 .... _ V_ ,.......,.. ~ S '-Ch R _ E .. 313 .Ddoai •• l dis .... 119 oDnor .. l Dod~ Du : 3' b .... se dis.,S. ! 2219 c.rdlov'5cul.r d: 2... conn.ctiuf tissul 239S digpsUu. sJlst •• 77' •• r nos. thro,t t 1195 rndGerln, dls'.I51 ..... PIf' dis.,s. 76"" ht.d .nd n.ell dl ! 26U h ... tologic di .. . 316 Mluth dis •• ,. 162' .. sculllllskel.t.l I 3236 "",urologic dis • ., 11 p.lule dls' llS. ~~~ ;:~:1~:::::. tr.c l i ... 50ft US5U. afse , 56 t hor.x dlsr,s. 1537 urog.""! t.l t ... ct 21161 th.r.p~ (in gtl 11 .cupuncturll 1""6 Di.logicol thtl 12\. canet .. lh l.py 2 cotllput 5s1s 1 ..., CDnSl'rUIUu, tl 11\. couns.l1ng 51 d.todUution 171 dis •• ,. contra : 11161 drug th p~ 1212 drug th p~ S3 .dJoo.nt th •• II .nUDi.tic p, 311 .ntibNJUc tl 19 Iinttc""gul.nl 61 .ntlconuuls'l 15 .ntibyp.,.t.n ! "" Intiaicrobl. : 72 bon ...... ro. ! 15 ene rat re n UU 1 c h.aic.l . ~ .. n en proph~l ; 629 c h th.r. p~ Figure 10 .5 The HiBrowse interface for viewing category l ab els hi erarchicall y and according to facets [646 ]. hierarchy is shown at one time , making b rowsing of t h is very large hierarch y a mo re tract able endeavor. T he inte rface has the s pace limi tations inherent in a t w o- d imensional hierarchy display and do es no t pro vide mechanisms for search over an u nd er l y ing document collecti on. See F igur e 10.4. Th e HiBrows e system [646 1 r ep rese nt s ca t ego ry me tadata more efficiently by allo wing user s to display sever a l diffe rent subse ts of category metadata simultaneous l y. T h e user first selects which a t tribu te t ype (o r facet , as attributes are called in thi s system) t o displa y. For exampl e , the user may first choose the ' p hysica l disease ' v a lue fo r t he Disease facet . The categories that appear one level below th is a re shown alo n g wi th t he number of documents that contain each categor y. The user can t h en select other attribute types , such as Therapy and Group s (by a ge) . T h e number of documents that contain attributes from all three t ypes are shown. If t h e user now selects a refinement of one of the categories , such as t he ' c h ild ' v a lu e for t he Groups attribute , then the number of documen ts tha t cont ain all t h r ee selected facet t ypes are shown . At the same t ime, the number of documen ts con taining t h e subcategories found below ' physic a l dis eas e ' and ' t her a py (g en er a l)' . are updated to reflect this more restricted specificat ion . See Fig ure 10.5. A problem with the HiBrowse system is that it r equires users t o n avigat e t hr o u gh t h e ca tego ry hierarch y , rather than specify queries di re ct l y. I n ot h er w ords , que ry specifica tion is not t ight ly coupled with displa y of ca tegor y met adat a . As a so lu t ion t o ' some of t h ese problems , the Ca t -a -Cone int erface [ 3581 will b e described in sec tion 10.8 .    272 USER INTERFACES AND VISUALIZATION Automatically Derived Collection Overviews Many attempts to display overview information have focused on automatically extracting the most common general themes that occur within the collection. These themes are derived via the use of unsupervised analysis methods, usually variants of document clustering. Clustering organizes documents into groups based on similarity to one another; the centroids of the clusters determine the themes in the collections. The Scatter/Gather browsing paradigm [203, 202] clusters documents into topically-coherent groups, and presents descriptive textual summaries to the user. The summaries consist of topical terms that characterize each cluster generally, and a set of typical titles that hint at the contents of the cluster. Informed by the summaries, the user may select a subset of clusters that seem to be of most interest, and recluster their contents. Thus the user can examine the contents of each subcollection at progressively finer granularity of detail. The reclustering is computed on-the-fly; different themes are produced depending on the documents contained in the subcollection to which clustering is applied. The choice of clustering algorithm influences what clusters are produced, but no one algorithm has been shown to be particularly better than the rest when producing the same number of clusters [816]. A user study [640] showed that the use of Scatter/Gather on a large text collection successfully conveys some of the content and structure of the corpus. However, that study also showed that Scatter/Gather without a search facility was less effective than a standard similarity search for finding relevant documents for a query. That is, subjects allowed only to navigate, not to search over, a hierarchical structure of clusters covering the entire collection were less able to find documents relevant to the supplied query than subjects allowed to write queries and scan through retrieval results. It is possible to integrate Scatter/Gather with conventional search technology by applying clustering on the results of a query to organize the retrieved documents (see Figure 10.6). An offline experiment [359] suggests that clustering may be more effective if used in this manner. The study found that documents relevant to the query tend to fall mainly into one or two out of five clusters, if the clusters are generated from the top-ranked documents retrieved in response to the query. The study also showed that precision and recall were higher within the best cluster than within the retrieval results as a whole. The implication is that a user might save time by looking at the contents of the cluster with the highest proportion of relevant documents and at the same time avoiding those clusters with mainly non-relevant documents. Thus, clustering of retrieval results may be useful for helping direct users to a subset of the retrieval results that contain a large proportion of the relevant documents. General themes do seem to arise from document clustering.but the themes are highly dependent on the makeup of the documents within the clusters [359, 357]. The unsupervised nature of clustering can result in a display of topics at varying levels of description. For example, clustering a collection of documents about computer science might result in clusters containing documents about    ...1 ,,""_ .. 1 o StuSpegIed Banner , The o Key, Fiuds Scott o Fort McHenry a Henry Huley "" l'n..: .............. a 8urIt7n . EIIat o Stanw,dc, B ubara o Serle, Milton £ ~~ .f..~I.. I.1. a ltar a Gal axy. The o ectr apl actic tyItemI a intmtell ar ma tter "" ....... \........ STARTING P OINTS 273 air-.ICOCt Word poa Figure 10.6 Displa y of Scatter /Gather clu stering re trieval results [ 203]. art ificia l int elligence, com p u t e r t heor y, comp u t er gra p hics , compu t er archite cture , pr ogr a mmin g l an gu a ges, go vernm en t , and leg al issues. The la tter t w o t h emes ar e more gene ral th an t he o thers , bec au s e the y a re about t o p ic s outs i de t he ge n er al scope of comp ute r science . Thus clus tering can results in the j uxt a p osit ion of v e ry diff erent l e vels of descrip tion wi thin a single display. Sca t ter /Ga th er shows a t ext u a l representation of document clusters. Researc he rs have de v e lop ed sever a l approaches to map documents from their high dim ens ional repr esent a tion in do cumen t space into a 2D repr esen tation in which each do cum ent i s repr es ent ed as a small glyph or icon on a map or within an abstrac t 2D spac e. T h e funct ions for transformin g the data into the lower dimensional space differ , but the net effect is that each document is placed at one point in a sca t ter-plot-like representation of the space. Users are meant to detect themes or clusters in the arrangement of the glyphs. Systems employing such graphical displays include BE A D [ 156], the Galaxy of News [ 6 7 1]' and Th emeScapes [ 8 2 1]. The ThemeScapes view imposes a three-d imens iona l representation on the results of clustering (see Figure 10.7 ). The layout ma kes use of ' nega t iv e space ' to help emphasize the areas of concentration where the clus t ers o ccur. O ther sys t ems display inter-documen t similarity hierarchically [ 529, 14 ] , while still oth ers displa y re trieved documents in networks based on int er- do cu ment similari t y [ 262, 761 ]. Kohonen 's feature map algorithm has been used to create maps that graphic al ly characteriz e t h e overall conten t of a documen t collec tion or sub collection [ 5 20 , 163 ] (se e Figure 10.8 ). The regions of the 2D map var y in size and shape corresponding t o how f r eq uen t ly documents assigned to the corresponding themes occur wi thin t he colle ct ion . Regions are characterized by single words or phrases ,    274 USER INTERFACES AND VISUALIZATION Figure 10.7 A three-dimensional overview based on document clustering [821J. and adjacency of regions is meant to reflect semantic relatedness of the themes within the collection. A cursor moved over a document region causes the titles of the documents most strongly associated with that region to be displayed in a pop-up window. Documents can be associated with more than one region. Evaluations of Graphical Overviews Although intuitively appealing, graphical overviews of large document spaces have yet to be shown to be useful and understandable for users. In fact, evaluations that have been conducted so far provide negative evidence as to their usefulness. One study found that for non-expert users the results of clustering were difficult to use, and that graphical depictions (for example, representing clusters with circles and lines connecting documents) were much harder to use than textual representations (for example, showing titles and topical words, as in Scatter/Gather), because documents' contents are difficult to discern without actually reading some text [443]. Another recent study compared the Kohonen feature map overview representation on a browsing task to that of Yahoo! [163]. For one of the tasks, subjects were asked to find an 'interesting' Web page within the entertainment category of Yahoo! and of an organization of the same Web pages into a Kohonen map layout. The experiment varied whether subjects started in Yahoo! or in the graphical map. After completion of the browing task, subjects were asked to attempt to repeat the browse using the other tool. For the subjects that    • START ING POINT S 275 Fig ure 10.8 A t wo-di me ns ional overvie w c reate d us in g a K oh on en f e a tur e map le arn in g a lgor it h m on We b p a ges h a vi n g t o do with t he to pic Ent er tain m ent [163] . began wi th t he Koh on e n m a p visua lizat ion, 11 o ut of 1 5 f ou nd an int er es ting p a ge within te n m inut es. Ei gh t o f th ese w e r e ab le t o find t he sa me pa ge using Y ah o o!. Of th e s u b jects wh o sta rte d wi th Y ahoo ! , 1 4 o ut of 16 were ab le to find int e r e s tin g h om e p a ges . How ever , on ly two of t he 1 4 w e r e a b le t o find t h e pag e in t he gr a p hica l m ap displ ay ! Thi s is st ro ng ev ide nce aga ins t t he n a vi gabilit y of t he di spl a y a nd ce rta inly s ugges ts t hat t he sim ple la b el v iew p rov id ed by Yahoo! i s mor e u seful. Ho w ever , t he m ap di spla y ma y b e mor e us eful if t h e s yste m i s mo d ifie d to tig htly int egr a t e qu er yin g wi t h br ow sin g. T he s ub jects d id pr ef er so me as pects of t he ma p re p rese ntat ion . In p arti cu la r , some like d t he ease o f b ein g a b le to ju m p from one a rea t o a n o t he r wit hou t h av in g to bac k up as i s r equir ed in Y ah oo! , a nd so me liked t he f a ct t ha t the maps h a ve varyi ng level s of gra n ula rity . The s ub jects d is liked seve ra l aspe cts of the d i spla y. T he ex pe r ime n te rs found t hat so me s u b je c ts ex p res s ed a d esir e for a visib le hie r a r chi cal o rga nizat io n, ot he rs w an t ed a n a b ility to zoom in on a subarea to get more detail, a nd some user s di sliked h a vi n g to loo k thr ou gh th e e nt ire ma p to find a t heme , desi ring a n a lp ha betica l or de r ing ins t ead. Man y found t he sing le-te r m lab els to b e mi sle ad in g , in p ar t b ecau se t hey were a m b igu ous ( o ne r egion ca lled ' B I LL ' was t ho ug ht to corres po nd to a p er son ' s n am e r a th er t h a n cou nt ing m on ey ) . T he a utho rs concl uded t hat t his i nte rface i s m or e a p p ro p r i ate for cas ua l b rowsi ng t ha n for searc h. In gene ra l, un sup er v ised t he m a tic overvi ews a r e p erha ps m os t use fu l f o r g ivi ng user s a ' g ist ' of t he kin d s o f i n for mat ion th a t ca n b e    • 276 USER INTERFACES AND VISUALIZATION found within the document collection, but generally have not been shown to be helpful for use in the information access process. Co-citation Clustering for Overviews Citation analysis has .long been recognized as a way to show an overview of the contents of a collection [812]. The main idea is to determine 'centrally-located' documents based on co-citation patterns. There are different ways to determine citation patterns: one method is to measure how often two articles are cited together by a third. Another alternative is to pair articles that cite the same third article. In both cases the assumption is that the paired articles share some commonalities. After a matrix of co-citations is built, documents are clustered based on the similarity of their co-citation patterns. The resulting clusters are interpreted to indicate dominant themes within the collection. Clustering can focus on the authors of the documents rather than the contents, to attempt to identify central authors within a field. This idea has recently been implemented using Web-based documents in the Referral Web project [432]. The idea has also been applied to Web pages, using Web link structure to identify major topical themes among Web pages [485, 639]. A similar idea, but computed a different way, is used to explicitly identify pages that act as good starting points for particular topics (called 'authority pages' by Kleinberg [444]).  ",
2230,mir,mir-2230,"10.4.3 Examples, Dialogs, and Wizards"," 10.4.3 Examples, Dialogs, and Wizards Another way to help users get started is to start them off with an example of interaction with the system. This technique is also known as retrieval by reformulation. An early version of this idea is embodied in the Rabbit system [818] which provides graphical representations of example database queries. A general framework for a query is shown to the user who then modifies it to construct a partially complete description of what they want. The system then shows an example of the kind of information available that matches this partial description. For instance, if a user searching a computer products database indicates an interest in disks, an example item is retrieved with its disk descriptors filled in. The user can use or modify the displayed descriptors, and iterate the procedure. The idea of retrieval by reformulation has been developed further and extended to the domains of user interface development [581] and software engineering [669]. The Helgon system [255] is a modern variant of this idea applied to bibliographic database information. In Helgon, users begin by navigating a hierarchy of topics from which they select structured examples, according to their interests. If a feature of an example is inappropriately set, the user can modify the feature to indicate how it would appear in the desired information. Unfortunately, in tests with users, the system was found to be problematic. Users had problems with the organization of the hierarchy, and found that searching for a useful example by critiquing an existing one to be tedious. This result    STARTING POINTS 277 underscores an unfortunate difficulty with examples and dialogues: that of getting the user to the right starting dialogue or the right example strategy becomes a search problem in itself. (How to index prior examples is studied extensively in the case-based reasoning (CBR) literature [492,449].) A more dynamic variation on this theme is the interactive dialog. Dialogbased interfaces have been explored since the early days of information retrieval research, in an attempt to mimic the interaction provided by a human search intermediary (e.g., a reference librarian). Oddy did early work in the THOMAS system, which provided a question and answer session within a command-linebased interface [615]. More recently, Belkin et al. have defined quite elaborate dialog interaction models [75] although these have not been assessed empirically to date. The DLITE system interface [192] uses an animated focus-plus-context dialog as a way to acquaint users with standard sequences of operations within the system. Initially an outline of all of the steps of the dialog is shown as a list. The user can expand the explanation of any individual step by clicking on its description. The user can expand out the entire dialog to see what questions are coming next, and then collapse it again in order to focus on the current tactic. A more restricted form of dialog that has become widely used in commercial products is that of the wizard. This tool helps users in time-limited tasks, but does not attempt to overtly teach the processes required to complete the tasks. The wizard presents a step-by-step shortcut through the sequence of menu choices (or tactics) that a user would normally perform in order to get a job done, reducing user input to just a few choices with default settings [636]. A recent study [145] found wizards to be useful for goals that require many steps, for users who lack necessary domain knowledge (for example, a restaurant owner installing accounting software), and when steps must be completed in a fixed sequence (for example, a procedure for hiring personnel). Properties of successful wizards included allowing users to rerun a wizard and modify their previous work, showing an overview of the supported functions, and providing lucid descriptions and understandable outcomes for choices. Wizards were found not to be helpful when the interface did not solve a problem effectively (for example, a commercial wizard for setting up a desktop search index requests users to specify how large to make the index, but supplies no information about how to make this decision). Wizards were also found not to be helpful when the goal was to teach the user how to use the interface, and when the wizard was not user-tested. It maybe the case that information access is too variable a process for the use of wizards. A guided tour leads a user through a sequence of navigational choices through hypertext links, presenting the nodes in a logical order for some goal. In a dynamic tour, only relevant nodes are displayed, as opposed to the static case where the author decides what is relevant before the users have even formulated their queries [329]. A recent application is the Walden Paths project which enables teachers to define instructionally useful paths through pages found on the Web [289]. This approach has not been commonly used to date for    278 USER INTERFACES AND VISUALIZATION information access but could be a promising direction for acquainting users with search strategies in large hyperlinked systems.  ",
2231,mir,mir-2231,10.4.4 Automated Source Selection," 10.4.4 Automated Source Selection Human-computer interfaces for helping guide users to appropriate sources is a wide open area for research. It requires both eliciting the information need from users and understanding which needs can be satisfied by which sources. An ambitious approach is to build a model of the source and of the information need of the user and try to determine which fit together best. User modeling systems and intelligent tutoring systems attempt to do this both for general domains [204, 814] and for online help systems [378]. A simpler alternative is to create a representation of the contents of information sources and match this representation against the query specification. This approach is taken by GlOSS, a system which tries to determine in advance the best bibliographic database to send a search request to, based on the terms in the query [765]. The system uses a simple analysis of the combined frequencies of the query words within the individual collections. The SavvySearch system [383] takes this idea a step further, using actions taken by users after a query to decide how to decrease or increase the ranking of a search engine for a particular query (see also Chapter 13). The flip side to automatically selecting the best source for a query is to automatically send a query to multiple sources and then combine the results from the various systems in some way. Many metasearch engines exist on the Web. How to combine the results effectively is an active area of research, sometimes known as collection fusion [63, 767, 388]. 10.5 Query Specification To formulate a query, a user must select collections, metadata descriptions, or information sets against which the query is to be matched, and  ",
2232,mir,mir-2232,10.5 Query Specification," 10.5 Query Specification To formulate a query, a user must select collections, metadata descriptions, or information sets against which the query is to be matched, and must specify words, phrases, descriptors, or other kinds of information that can be compared to or matched against the information in the collections. As a result, the system creates a set of documents, metadata, or other information type that match the query specification in some sense and displays the results to the user in some form. Shneiderman [725] identifies five primary human-computer interaction styles. These are: command language, form fillin, menu selection, direct manipulation, and naturallanguage.§ Each technique has been used in query specification interfaces and each has advantages and disadvantages. These are described below in the context of Boolean query specification. § This list omits non-visual modalities, such as audio. I J  ",
2233,mir,mir-2233,10.5.1 Boolean Queries," 10.5.1 Boolean Queries In modern information access systems the matching process usually employs a statistical ranking algorithm. However, until recently most commercial full-text systems and most bibliographic systems supported only Boolean queries. Thus the focus of many information access studies has been on the problems users have in specifying Boolean queries. Unfortunately, studies have shown time and again that most users have great difficulty specifying queries in Boolean format and often misjudge what the results will be [111, 322, 558, 841]. Boolean queries are problematic for several reasons. Foremost among these is that most people find the basic syntax counter-intuitive. Many Englishspeaking users assume everyday semantics are associated with Boolean operators when expressed using the English words AND and OR, rather than their logical equivalents. To inexperienced users, using AND implies the widening of the scope of the query, because more kinds of information are being requested. For instance, 'dogs and cats' may imply a request for documents about dogs and documents about cats, rather than documents about both topics at once. 'Tea or coffee' can imply a mutually exclusive choice in everyday language. This kind of conceptual problem is well documented [111, 322, 558, 841]. In addition, most query languages that incorporate Boolean operators also require the user to specify complex syntax for other kinds of connectors and for descriptive metadata. Most users are not familiar with the use of parentheses for nested evaluation, nor with the notions associated with operator precedence. By serving a massive audience possessing little query-specification experience, the designers of World Wide Web search engines have had to come up with more intuitive approaches to query specification. Rather than forcing users to specify complex combinations of ANDs and ORs, they allow users to choose from a selection of common simple ways of combining query terms, including 'all the words' (place all terms in a conjunction) and 'any of the words' (place all terms in a disjunction). Another Web-based solution is to allow syntactically-based query specification, but to provide a simpler or more intuitive syntax. The '+' prefix operator gained widespread use with the advent of its use as a mandatory specifier in the Altavista Web search engine. Unfortunately, users can be misled to think it is an infix AND rather than a prefix mandatory operator, and thus assume that 'cat \+ dog' will only retrieve articles containing both terms (where in fact this query requires dog but allows cat to be optional). Another problem with pure Boolean systems is they do not rank the retrieved documents according to their degree of match to the query. In the pure Boolean framework a document either satisfies the query or it does not. Commercial systems usually resort to ordering documents according to some kind of descriptive metadata, usually in reverse chronological order. (Since these systems usually index timely data corresponding to newspaper and news wires, date of publication is often one of the most salient features of the document.) Webbased systems usually rank order the results of Boolean queries using statistical algorithms and Web-specific heuristics.  ",
2234,mir,mir-2234,10.5.2 From Command Lines to Forms and Menus," 10.5.2 From Command Lines to Forms and Menus Aside from conceptual misunderstandings of the logical meaning of AND and OR, another part of the problem with pure Boolean query specification in online bibliographic systems is the arbitrariness of the syntax and the contextlessness nature of the TTY-based interface in which they are predominantly available. Typically input is typed at a prompt and is of a form something like the following: COMMAND ATTRIBUTE value {BOOLEAN-OPERATOR ATTRIBUTE value} * e.g., FIND PA darwin AND TW species OR TW descent or FIND TW Mt St. Helens AND DATE 1981 (These examples are derived from the syntax of the telnet interface to the University of California Melvyl system [526J.) The user must remember the commands and attribute names, which are easily forgotten between usages of the system [553J. Compounding this problem, despite the fact that the command languages for the two main online bibliographic systems at UC Berkeley have different but very similar syntaxes, after more than ten years one of the systems still reports an error if the author field is specified as PA instead of PN, as is done in the other system. This lack of flexibility in the syntax is characteristic of interfaces designed to suit the system rather than its users. The new Web-based version of Melvyl] provides form fillin and menu selection so the user no longer has to remember the names and types of attributes available. Users select metadata types from list boxes and attributes are shown explicitly, allowing selection as an alternative to specification. For example, the 'search type' field is adjacent to an entry form in which users can enter keywords, and a choice between AND and NOT is provided adjacent to a list of the available document types (editorial, feature, etc.). Only the metadata associated with a given collection is shown in the context of search over that collection. (Unfortunately the system is restricted to searching over only one database at a time. It does however provide a mechanism for applying a previously executed search to a new database.) See Figure 10.9. The Web-based version of Melvyl also allows retention of context between searches, storing prior results in tables and hyperlinking these results to lists containing the retrieved bibliographic information. Users can also modify any of the previously submitted queries by selecting a checkbox beside the record of the query. The graphical display makes explicit and immediate many of the powerful options of the system that most users would not learn using the command-line version of the interface. Bit-mapped displays are an improvement over command-line interface, but do not solve all the problems. For example, a blank entry form is in some ways II http://www.melvyl.ucop,edu/    QUERY SPECIFICATION 281 : .•. : : : :::::E.~;;~ f;; C;; :: :: ] (C.i.jones. ell) Options 8Ild UmiIs ADDCher Audlar(Wi'""iJ I (e. 8-. non, r) JIIurMI 'OOt (Wi'""iJ I . (e. 8-. daedIIus or jama) R IvI.y words r Ezact begiIlaing r Compltl.e 1iIIe (Wi'""iJ • \--Figure 10.9 A view of query specification in the Web-based version of the Melvyl bibliographic catalog. Copyright © 1998, The Regents of the University of California. not much better than a TTY prompt, because it does not provide the user with clues about what kinds of terms should be entered.  ",
2235,mir,mir-2235,10.5.3 Faceted Queries," 10.5.3 Faceted Queries Yet another problem with Boolean queries is that their strict interpretation tends to yield result sets that are either too large, because the user includes many terms in a disjunct, or are empty, because the user conjoins terms in an effort to reduce the result set. This problem occurs in large part because the user does not know the contents of the collection or the role of terms within the collection. A common strategy for dealing with this problem, employed in systems with command-line-based interfaces like DIALOG's, is to create a series of short queries, view the number of documents returned for each, and combine those queries that produce a reasonable number of results. For example, in DIALOG, each query produces a resulting set of documents that is assigned an identifying name. Rather than returning a list of titles themselves, DIALOG shows the set number with a listing of the number of matched documents. Titles can be shown by specifying the set number and issuing a command to show the titles. Document sets that are not empty can be referred to by a set name and combined with AND operations to produce new sets. If this set in turn is too small, the user can back up and try a different combination of sets, and this process is repeated in pursuit of producing a reasonably sized document set. This kind of query formulation is often called a faceted query, to indicate that the user's query is divided into topics or facets, each of which should be    282 USER INTERFACES AND VISUALIZATION present in the retrieved documents [553, 348]. For example, a query on drugs for the prevention of osteoporosis might consist of three facets, indicated by the disjuncts (osteoporosis OR 'bone loss') (drugs OR pharmaceuticals) (prevention OR cure) This query implies that the user would like to view documents that contain all three topics. A technique to impose an ordering on the results of Boolean queries is what is known as post-coordinate or quorum-level ranking [700, Ch. 8]. In this approach, documents are ranked according to the size of the subset of the query terms they contain. So given a query consisting of 'cats,' 'dogs,' 'fish,' and 'mice,' the system would rank a document with at least one instance of 'cats,' 'dogs,' and 'fish' higher than a document containing 30 occurrences of 'cats' but no occurrences of the other terms. Combining faceted queries with quorum ranking yields a situation intermediate between full Boolean syntax and free-form natural language queries. An interface for specifying this kind of interaction can consist of a list of entry lines. The user enters one topic per entry line, where each topic consists of a list of semantically related terms that are combined in a disjunct. Documents that contain at least one term from each facet are ranked higher than documents containing terms only from one or a few facets. This helps ensure that documents which contain discussions of several of the user's topics are ranked higher than those that contain only one topic. By only requiring that one term from each facet be matched, the user can specify the same concept in several different ways in the hopes of increasing the likelihood of a match. If combined with graphical feedback about which subsets of terms matched the document, the user can see the results of a quorum ranking by topic rather than by word. Section 10.6 describes the TileBars interface which provides this type of feedback. This idea can be extended yet another step by allowing users to weight each facet. More likely to be readily usable, however, is a default weighting in which the facet listed highest is assigned the most weight, the second facet is assigned less weight, and so on, according to some distribution over weights.  ",
2236,mir,mir-2236,10.5.4 Graphical Approaches to Query Specification," 10.5.4 Graphical Approaches to Query Specification Direct manipulation interfaces provide an alternative to command-line syntax. The properties of direct manipulation are [725, p.205]: (1) continuous representation of the object of interest, (2) physical actions or button presses instead of complex syntax, and (3) rapid incremental reversible operations whose impact on the object of interest is immediately visible. Direct manipulation interfaces often evoke enthusiasm from users, and for this reason alone it is worth exploring their use. Although they are not withqut drawbacks, they are easier to use than other methods for many users in many contexts.    QUERY SPECIFICATION 283 \--s.vdtf«."", ~ lr\""MCI NlJiotr.y""~""""Quen _&amp;odoJliI\; O'Gr~, s.arc*w4.,... IJOWWtI; b\A: Mt R~ Gl'O(lhitol ~ •• ""totlon of hol,on (J( ....... on. ill 0 A r'lietwrd Ouer"" 1'''0(""''',,,, '1'1 Q Nltt .... nMIn Aet""levol !Wt. 'otrjcio Sl.,.on On bt.ndlflg tn. u.ttOl"" ~ nodel lOt'""SooleGlt ~ S, l. n. laont. II. 114l'lto, U. Y. ~. ,_ C ""_ ~ .. IH~t I'onlawio\ ion Int ... foe. '01"" 1oo1M"" 1M.... ht. ... G, ~iek, Jefh""\1 0, ~. ~ •. F lyNl, !louilI,. Figure 10.10 The VQuery Venn diagram visualization for Boolean query specification [417]. Several variations of graphical interfaces, both directly manipulable and static, have been developed for simplifying the specification of Boolean syntax. User studies tend to reveal that these graphical interfaces are more effective in terms of accuracy and speed than command-language counterparts. Three such approaches are described below. Graphical depictions of Venn diagrams have been proposed several times as a way to improve Boolean query specification. A query term is associated with a ring or circle and intersection of rings indicates conjunction of terms. Typically the number of documents that satisfy the various conjuncts are displayed within the appropriate segments of the diagram. Several studies have found such interfaces more effective than their command-language-based syntax [417, 368, 558]. Hertzum and Frokjaer found that a simple Venn diagram representation produced faster and more accurate results than a Boolean query syntax. However, a problem with this format is the limitations on the complexity of the expression. For example, a maximum of three query terms can be ANDed together in a standard Venn diagram. Innovations have been designed to get around this problem, as seen in the VQuery system [417] (see Figure 10.10). In VQuery, a direct manipulation interface allows users to assign any number of query terms to ovals. If two or more ovals are placed such that they overlap with one another, and if the user selects the area of their intersection, an AND is implied among those terms. (In Figure 10.10, the term 'Query' is conjoined with 'Boolean'.) If the user selects outside the area of intersection but within the ovals, an OR is implied among the corresponding terms. A NOT operation    2 84 USE R I NT ER FACE S AN D V ISUA L IZA T ION Ii ng oom I II CUIiFiOU I II !lo R I ( opq"" 't&gt; L! H H """""""" ~ _ J "" bkr ... LM&lt; ~ "",,\ u .. "" ,..• ~IWI ~~ . A1 ~ r : Jfflt j • • 1U 1 TUI G wn'\Y i \- u Ii N ert \- . , "" . _hto .lnh. leU- . lor=, c ....... l ot ....."". l :rt l't, (J""' '' 0 .. ! ~ [t-. l ~ :tlI £ .. t 'irl! r ~ _ ; F "" d . Greet G Ii:"""", • Il...i. . H ... .., \- : m.. ~ : 1 .... • • ltrN \- , : tlQI N \- . lolJ • , t """" _ ; I l... . . • • [ I Lo c . n O Ii I liwlA Gll I ii lU A U I I! IITLJ ~ ~ ;=--' Figure 10.11 T he filter- flo w visualization for Bo ol ean q ue ry sp ecifica tion [ 84 1 ]. is associated wi th an y te rm whose ova l a p pears in the activ e ar ea of the displa y bu t whic h r emain s un select ed (in t he figur e , N OT ' Ra n k ing' h as b een s pecified ) . An act ive ar ea indi ca tes t h e cur r e nt qu er y ; all g rou ps of ov al s wit hin t he act ive ar ea ar e co ns i de r ed part of a conjunction. O v al s cont a in ing q uery te rms can be moved out of th e ac tiv e area for later use. Youn g and Shn eid erman [ 84 1] found i mprov em en t s over sta n da rd Boo l e an sy nt ax by providing us ers w ith a dir ec t manipul a ti on filter-flow mod el. Th e user is s hown a sc r olla b le list of attribut e ty pes on t he lef t \- h a nd side a n d se lects at \t r i b utes from anoth er li st of at tri bu t e types shown ac ross t he top of t he sc reen . Clicking on an attribu t e nam e ca uses a li st b o x containing val ues for those att r i b ute s t o be displa yed i n th e main p ortion o f the scree n . The user t h en selects which valu es of th e a ttributes to l e t t he flow go thro ug h . P laci n g two o r mor e of t hese attributes in seq ue n c e cr e at e s th e s emant ics of a co nj unct over the se lecte d v a l ues . Placing two or more of thes e in parall el c reates t he se ma ntics o f a d isjunct. The number of documents that match th e qu e ry a t eac h p oint is indi c at ed by t he width of the ' wa t er' flowing from one att r i bu te t o t he n e x t. (See F igure 10.11.) A conj u n ct ca n r educe the amoun t of flow. The ite ms t h at matc h t he full qu er y a re shown on t he far r ight-hand sid e . A use r st u dy fou nd t hat fewe r e rrors w er e mad e using t h e fil t er flow mod el than a sta nd a rd SQ L database query . How ever , the ex a m p les a n d stud y p er t ai n on ly to database q uer ying r a th er t han information access , sinc e the possible qu er y ter ms f o r in for mat ion access cannot be repres en ted reali s ti call y in a sc ro llable li st . T h is int er f a c e co u ld pe r ha ps be modified to better suit in f o r m at ion acces s applic ati on s by ha vin g t he u ser s u p ply initial qu ery term s , and using the a t t ri b u te se lect i on fa cilit y to s how t hose ter ms    Q UERY SPECIFICATIO N 285 : STAR S:Que r y Refo l1 l1 ul a ti on Worksp a ce F ile t e rms CAO\lJP saveset IS • AP ply Cha ng es I I Disp lay Ti des I I v er si on S I Hel p o Figure 10.12 A bl o ck -or i ented di agram v isua lizat io n f or Bool e an quer y s p eci fica t io n [ 21 ]. t hat are con cep t ua lly r el ated to th e qu er y t erm s . Another alternative is to use t h is displa y as a ca t egor y metadata sel ection interface (see Section 10.4). An ick et al. [ 2 1 ) d escrib e an oth er innov a tiv e dir ect manipula tion int erface for Boolean querie s . Initiall y th e user t ypes a natural language query which is automatic ally conv erted t o a r epr es ent ation in which eac h query term is repr esen ted within a blo ck. Th e blocks a re a r r a n ged into rows . and c o lu m ns ( S ee Figur e 10.12 ) . If two o r m or e blo ck s a p pe a r a lo ng th e sa me row th ey are cons ide red to b e A D ed t oge t he r. Two or mor e block s within the same column ar e OR ed. Thu s th e u ser ca n r epresent a technical term in multiple ways within the same query , providing a kind of fa cet ed quer y interfac e. For ex a m p le, the terms ' ve rsio n 5' , ' ve rs i o n 5.0 ' , and ' v 5' migh t b e s h own in th e sam e colu m n . User s ca n quickl y e x p eri me nt wi th differen t com b i nat ions of t e r ms within Boole an qu eries s im p ly by ac t ivat ing and deactivating block s. This fa cilit y also allows us er s to hav e mul tiple repre s entations of the sam e term in different places throughout the displ ay, thu s allowing rapid fe edback on the consequences of specifying various combination s of qu er y t erm s. Informal e v a l uat ion of th e system found that users were able to l earn to m an ipul a t e the int erfac e quickl y and enjoy ed usin g it . It was not formally c om p a re d to other int eraction te chniques [ 21) . This int erface provides a kind of qu ery pr ev i ew: a low cost , rapid turnaround visualization of th e r esult s of many variations on a query [643] . Another exampl e of qu er y previ ewin g ca n b e found in som e h elp syst ems , which show all the words in t he ind ex whose first l ett er s ma tch th e char act ers t h a t th e user has t yp ed so far , Th e mor e char act er s t yp ed , the f ew er possibl e m atches b ecome available. The HiBrow se syst em d e scribed above [ 646) also provides a kind of preview for viewing cat egory hi erarchies and facets , showing how many documents would be match ed if a ca teg o ry o ne level b elow t he curr ent on e wer e s elected. It p erhaps cou ld be improv ed by s h o wing t he cons equ en ces of more combinations of categories in an a n i mate d m ann er. If based on prior ac tion and interests of th e user , quer y previewing ma y b ecome more generall y applicable for information access interfaces.    • 2 86 USE R INT E RF AC E S AND VI SUAL IZAT ION Average housepri ce ... . . ...... ""' ==-=""J (I &gt; """""",, 'I! ,,~ ~ Average annual pay , 1991 :: ::::::: : : : : : : : : : : 1 ( &gt; _-\--\- A ND '"" OR o Anahe im , CA Lo n g Bea c h , CA o Los A n ge le s, CA o Riv e rsi de , CA • San Di eg o, CA Santa Ana, CA • • Census Figur e 1 0 .1 3 A mag ic l en s i nt er fa ce for query sp ecific ation (co urt e sy o f Ke n Fishkin). A fina l ex a m p le of a gr aphic al a p p roac h to qu e r y s p ecificat ion is th e use of magi c lense s . F ish kin an d S ton e h ave s uggeste d an extens ion to the u sa ge of t h is v isualiz at ion too l fo r the s peci ficat ion of Bo olean q ue r ies [256] . In fo rmat ion is rep resen t ed as li st s or icon s w i thin a 2 D s pace . Len ses act as fil t ers on t he d o cum en t set. (See Figur e 10.13 . ) For exa m p le , a wo r d ca n b e assoc iat ed w it h a t ra n s p a re nt le ns. Whe n th is lens is pla ced over a n iconi c re p r esentat ion of a set of d o cu me n t s , it can ca use all do cum en t s t hat d o not co nta in a given word to d isa pp ear. If a seco n d l e ns r ep r es entin g ano th er wor d is t he n l a i d ov e r the fi rst, t he len ses combi ne t o act as a co nj unct io n of t h e two words wit h t he d o cum en t set , hiding an y d o c u m e n t s t hat do no t co nta in b o t h w o rd s. Add it ional infor m at io n ca n b e a dj us te d d ynamic all y , s u ch as a min imum t hresho ld for how ofte n t he te r m o ccurs in th e d oc u m en t s , or an on -o ff switc h for word stem m ing . For ex a m p le, Figur e 10.13 show s a d i sjun ctiv e qu er y t hat fi nds cit ies with re lat ively low ho usi ng p r ices or hi gh annu al sa la ries . On e lens ' ca ll s o ut ' a cl um p of so ut h \ern C alifo rn ia cit ies, la b e lin g eac h . Ab ove t hat is a len s screening for cit ies wi th average hous e pr ice b elow $194,321 (t h e d a t a is fro m 1 990 ) , and above this one i s a l ens scre e ning for cit ies with av e rag e a n n ua l p a y a bove $28,477 . This app roac h, wh il e p r o mi s in g , h as n ot been eva luat ed in a n inform a ti o n access setti ng. 10 .5.5 Ph rases and Proxim it y In gen era l , p roxim it y inform a ti o n ca n b e qui t e .effect ive at i m proving p re cision of sea r ches . On t he Web , t he d iff er en ce b etw een a s ing le-wo r d q uery and a t w o-w or d  ",
2237,mir,mir-2237,10.5.5 Phrases and Proximity,"  • 2 86 USE R INT E RF AC E S AND VI SUAL IZAT ION Average housepri ce ... . . ...... ""' ==-=""J (I &gt; """""",, 'I! ,,~ ~ Average annual pay , 1991 :: ::::::: : : : : : : : : : : 1 ( &gt; _-\--\- A ND '"" OR o Anahe im , CA Lo n g Bea c h , CA o Los A n ge le s, CA o Riv e rsi de , CA • San Di eg o, CA Santa Ana, CA • • Census Figur e 1 0 .1 3 A mag ic l en s i nt er fa ce for query sp ecific ation (co urt e sy o f Ke n Fishkin). A fina l ex a m p le of a gr aphic al a p p roac h to qu e r y s p ecificat ion is th e use of magi c lense s . F ish kin an d S ton e h ave s uggeste d an extens ion to the u sa ge of t h is v isualiz at ion too l fo r the s peci ficat ion of Bo olean q ue r ies [256] . In fo rmat ion is rep resen t ed as li st s or icon s w i thin a 2 D s pace . Len ses act as fil t ers on t he d o cum en t set. (See Figur e 10.13 . ) For exa m p le , a wo r d ca n b e assoc iat ed w it h a t ra n s p a re nt le ns. Whe n th is lens is pla ced over a n iconi c re p r esentat ion of a set of d o cu me n t s , it can ca use all do cum en t s t hat d o not co nta in a given word to d isa pp ear. If a seco n d l e ns r ep r es entin g ano th er wor d is t he n l a i d ov e r the fi rst, t he len ses combi ne t o act as a co nj unct io n of t h e two words wit h t he d o cum en t set , hiding an y d o c u m e n t s t hat do no t co nta in b o t h w o rd s. Add it ional infor m at io n ca n b e a dj us te d d ynamic all y , s u ch as a min imum t hresho ld for how ofte n t he te r m o ccurs in th e d oc u m en t s , or an on -o ff switc h for word stem m ing . For ex a m p le, Figur e 10.13 show s a d i sjun ctiv e qu er y t hat fi nds cit ies with re lat ively low ho usi ng p r ices or hi gh annu al sa la ries . On e lens ' ca ll s o ut ' a cl um p of so ut h \ern C alifo rn ia cit ies, la b e lin g eac h . Ab ove t hat is a len s screening for cit ies wi th average hous e pr ice b elow $194,321 (t h e d a t a is fro m 1 990 ) , and above this one i s a l ens scre e ning for cit ies with av e rag e a n n ua l p a y a bove $28,477 . This app roac h, wh il e p r o mi s in g , h as n ot been eva luat ed in a n inform a ti o n access setti ng. 10 .5.5 Ph rases and Proxim it y In gen era l , p roxim it y inform a ti o n ca n b e qui t e .effect ive at i m proving p re cision of sea r ches . On t he Web , t he d iff er en ce b etw een a s ing le-wo r d q uery and a t w o-w or d    QUERY SPECIFICATION 287 exact phrase match can mean the difference between an unmanageable mess of retrieved documents and a short list with mainly relevant documents. A large number of methods for specifying phrases have been developed. The syntax in LEXIS-NEXIS requires the proximity range to be specified with an infix operator. For example, 'white w /3 house' means 'white within 3 words of house, independent of order.' Exact proximity of phrases is specified by simply listing one word beside the other, separated by a space. A popular method used by Web search engines is the enclosure of the terms between quotation marks. Shneiderman et al. [726] suggest providing a list of entry labels, as suggested above for specifying facets. The difference is, instead of a disjunction, the terms on each line are treated as a phrase. This is suggested as a way to guide users to more precise query specification. The disadvantage of these methods is that they require exact match of phrases, when it is often the case (in English) that one or a few words comes between the terms of interest. For example, in most cases the user probably wants 'president' and 'lincoln' to be adjacent, but still wants to catch cases of the sort 'President Abraham Lincoln.' Another consideration is whether or not stemming is performed on the terms included in the phrase. The best solution may be to allow users to specify exact phrases but treat them as small proximity ranges, with perhaps an exponential fall-off in weight in terms of distance of the terms. This has been shown to be a successful strategy in non- interactive ranking algorithms [174]. It has also been shown that a combination of quorum ranking of faceted queries with the restriction that the facets occur within a small proximity range can dramatically improve precision of results [356, 566].  ",
2238,mir,mir-2238,10.5.6 Natural Language and Free Text Queries," 10.5.6 Natural Language and Free Text Queries Statistical ranking algorithms have the advantage of allowing users to specify queries naturally, without having to think about Boolean or other operators. But they have the drawback of giving the user less feedback about and control over the results. Usually the result of a statistical ranking is the listing of documents and the association of a score, probability, or percentage beside the title. Users are given little feedback about why the document received the ranking it did and what the roles of the query terms are. This can be especially problematic if the user is particularly interested in one of the query terms being present. One search strategy that can help with this particular problem with statistical ranking algorithms is the specification of 'mandatory' terms within the natural language query. This in effect helps the user control which terms are considered important, rather than relying on the ranking algorithm to correctly weight the query terms. But knowing to include a mandatory specification requires the user to know about a particular command and how it works. The preceding discussion assumes that a natural language query entered by the user is treated as a bag of words, with stopwords removed, for the purposes of document match. However, some systems attempt to parse natural language queries in order to extract concepts to match against concepts in the    288 USER INTERFACES AND VISUALIZATION text collection [399, 552, 748]. Alternatively, the natural language syntax of a question can be used to attempt to answer the question. (Question answering in information access is different than that of database management systems, since the information desired is encoded within the text of documents rather than specified by the database schema.) The Murax system [463] determines from the syntax of a question if the user is asking for a person, place, or date. It then attempts to find sentences within encyclopedia articles that contain noun phrases that appear in the question, since these sentences are likely to contain the answer to the question. For example, given the question 'Who was the Pulitzer Prize-winning novelist that ran for mayor of New York City?,' the system extracts the noun phrases 'Pulitzer Prize,' 'winning novelist,' 'mayor,' and 'New York City.' It then looks for proper nouns representing people's names (since this is a 'who' question) and finds, among others, the following sentences: The Armies of the Night (1968), a personal narrative of the 1967 peace march on the Pentagon, won Mailer the Pulitzer Prize and the National Book Award. In 1969 Mailer ran unsuccessfully as an independent candidate for mayor of New York City. Thus the two sentences link together the relevant noun phrases and the system hypothesizes (correctly) from the title of the article in which the sentences appear that Norman Mailer is the answer. Another approach to automated question answering is the FAQ finder system which matches question-style queries against question-answer pairs on various topics [130]. The system uses a standard IR search to find the most likely FAQ (frequently asked questions) files for the question and then matches the terms in the question against the question portion of the question-answer pairs. A less automated approach to question answering can be found in the Ask Jeeves system [34]. This system makes use of hand-picked Web sites and matches these to a predefined set of question types. A user's query is first matched against the question types. The user selects the most accurate rephrase of their question and this in turn is linked to suggested Web sites. For example, the question 'Who is the leader of Sudan?' is mapped into the question type 'Who is the head of state of X (Sudan)?' where the variable is replaced by a listbox of choices, with Sudan the selected choice in this case. This is linked to a Web page that lists current heads of state. The system also automatically substitutes in the name 'Sudan' in a query against that Web page, thus bringing the answer directly to the user's attention. The question is also sent to standard Web search engines. However, a system is only as good as its question templates. For example a question 'Where can I find reviews of spas in Calistoga?' matches the question 'Where can I find X (reviews) of activities for children aged Y (I)?' and 'Where can I find a concise encyclopedia article on X (hot springs)?'    CONTEXT 289  ",
2239,mir,mir-2239,10.6 Context," 10.6 Context This section discusses interface techniques for placing the current document set in the context of other information types, in order to make the document set more understandable. This includes showing the relationship of the document set to query terms, collection overviews, descriptive metadata, hyperlink structure, document structure, and to other documents within the set. 10.6.1 Docume ",
2240,mir,mir-2240,10.6.1 Document Surrogates," 10.6.1 Document Surrogates The most common way to show results for a query is to list information about documents in order of their computed relevance to the query. Alternatively, for pure Boolean ranking, documents are listed according to a metadata attribute, such as date. Typically the document list consists of the document's title and a subset of important metadata, such as date, source, and length of the article. In systems with statistical ranking, a numerical score or percentage is also often shown alongside the title, where the score indicates a computed degree of match or probability of relevance. This kind of information is sometimes referred to as a document surrogate. See Figure 10.14 from [824]. Some systems provide users with a choice between a short and a detailed view. The detailed view typically contains a summary or abstract. In bibliographic systems, the author-written or service-written abstract is shown. Web search engines automatically generate excerpts, usually extracting the first few lines of non-markup text in the Web page. In most interfaces, clicking on the document's title or an iconic representation of the document shown beside the title will bring up a view of the document itself, either in another window on the screen, or replacing the listing of search results. (In traditional bibliographic systems, the full text was unavailable online, and only bibliographic records could be readily viewed.) 10.6.2 Query Term Hits Within Document Content In systems in which the user can view the full text of a retrieved document, it is often useful to highlight the occurrences of the terms or descriptors that match those of the user's query. It can also be useful for the system to scroll the view of the document to the first passage that contains one or more of the query terms, and highlight the matched t ",
2241,mir,mir-2241,10.6.2 Query Term Hits Within Document Content," 10.6.2 Query Term Hits Within Document Content In systems in which the user can view the full text of a retrieved document, it is often useful to highlight the occurrences of the terms or descriptors that match those of the user's query. It can also be useful for the system to scroll the view of the document to the first passage that contains one or more of the query terms, and highlight the matched terms in a contrasting color or reverse video. This display is thought to help draw the user's attention to the parts of the document most likely to be relevant to the query. Highlighting of query terms has been found time and again to be a useful feature for information access interfaces [481],[542, p.31]. Color highlighting has also recently been found to be useful for scanning lists of bibliographic records [52].    290 USER INTERFACES AND VISU ALI ZATI ON "" .. . 1 C IlUC lI li . T echm cal R ep on CMLIJSEI ·87· TR · ) The Etrec l ofSo l\war e S uppon Need&gt; on the D eportm er~ ofD ef "",. e S oftwar e Acquwtl on Pohcy . Pan I A frwn eworlc for Anal y:lll g l.egall ssues Ame C . MarlIn and KOVlll M Deasy The Eff ect of Sol\\....,.. Supp on Need. on the D epartment ofD efen.e Soft ware A Cquul'J on Pob AN IMPR OVED TREATMENT O f EXTERNAl. WUNDARY f OR THREE ·DIM EN SIONAl. fLOW CO MPIJT ATIONS7 Sem yoo V Tl}'tlk1l vy Yea N V a ll az NAS A ~ey Resear ch C enter . Hamp ton. V A AbstraelWe presenl an mnovalIVe nummcal apprna ch f or 'dlIng lllgllly aC c onl e nonl ocal houn llaty co n d1l1ons al1he extanal c ompulal1Ona1 NatIonal A « onautlC . and S poc e Adrnlna .tnlIonLangI ey Res ear ch C enter'! Hamp lon. V :r .... l3681 ·21~NASA T ed lll1 c:al Pap« 3 631 Mul!1slai e Sch emes WIlh Multignd!or Eulerand N aVl «'Slc kes ~ ons Componems and AnalysuR C n!.anIloy Res earch Cen:tr 7 Hamp1&lt;n, V :r g:maEh TIrtd Tel·A VlV U lll ve n ll A Dutnbuted G artJag e C oll ecb on A1gonlhm T=e CnlthIow UUC S·9 2 ·1 1 Departm c&lt;ll of C orr .pula""Soen ce UIllv emty of ~ Sal! Lak e C IIy.UT 84112USA July 30\. 19 92 AbstraelC onC lJrT c&lt;Il S ch c&lt;ne = ds \he S chc&lt;ne prosrarnmmg language. proVldlng parallel program execulWn on • illstnbul&lt;d nelw otlt The i ln COMPUTER SCIEN TECHNICAL REPOR WlDII_ : S_ JO I ResulU for \he quesy 8_ ( mere than SO documec1s malclled the quesy ) Fig ure 10 . 1 4 An example of a ranked list of titl es and oth er d o cum ent s ur rogate information [8241 . KWIC A facility rela ted to highlighting i s t he ke y word-i n- con t ex t (K W IC) d ocum en t surroga te. Sentence fragmen ts , full sen tences , or groups of se nte nces t h at c onta in query terms are extracted from the full t e xt and pr esen t ed for v ie wing a lon g with other kinds of surrogate information ( s u ch as documen t t it le and abs trac t ). N o te that a KWIC listing is different than an abstract. An abstract s u m ma r izes t he m a i n t opics of the document b u t might not con tain refer ences to th e t erms within the q ue ry. A KW I C ext ract shows sentences that s u m m ar ize t h e wa ys the qu er y terms a re used within the do cument . This di spla y ca n sh ow not only wh ic h subsets of query terms occur in t h e re trieved docum ents , bu t a lso t h e cont ext they appear in with respect t o one ano ther. Tradeoff decisions must be made be tween how man y lines of te xt t o show and which lines to display. It is no t known which c o nt exts a re b est selec t ed for viewing b ut results from text summa rization r es ear ch su ggest t h at the b est fragments to show are those that appear near th e b eginning of t he d o cum ent and that contain the lar ges t subset of query terms [ 4 6 41 . If users hav e specified which    CONTEXT 291 terms are more important than others, then those fragments containing important terms should be shown before those that contain only less important terms. However, to help retain coherence of the excerpts, selected sentences should be shown in order of their occurrence in the original document, independent of how many search terms they contain. The KWIC facility is usually not shown in Web search result display, most likely because the system must have a copy of the original document available from which to extract the sentences containing the search terms. Web search engines typically only retain the index without term position information. Systems that index individual Web sites can show KWIC information in the document list display. TileBars A more compact form of query term hit display is made available through the TileBars interface. The user enters a query in a faceted format, with one topic per line. After the system retrieves documents (using a quorum or statistical ranking algorithm), a graphical bar is displayed next to the title of each document showing the degree of match for each facet. TileBars thus illustrate at a glance which passages in each article eontain which topics \and moreover, how frequently each topic is mentioned (darker squares represent more frequent matches). Each document is represented by a rectangular bar. Figure 10.15 shows an example. The bar is subdivided into rows that correspond to the query facets. The top row of each TileBar corresponds to 'osteoporosis,' the second row to 'prevention,' and the third row to 'research.' The bar is also subdivided into columns, where each colurnn-refers joa-passage within the document. Hits that overlap within the same passage are more likely to indicate a relevant document than hits that are widely dispersed throughout the document [356]. The patterns are meant to indicate whether terms from a facet occur as a main topic throughout the document, as a subtopic, or are just mentioned in passing. The darkness of each square corresponds to the number of times the query occurs in that segment of text; the darker the square the greater the number of hits. White indicates no hits on the query term. Thus, the user can quickly see if some subset of the terms overlap in the same segment of the document. (The segments for this version of the interface are fixed blocks of 100 tokens each.) The first document can be seen to have considerable overlap among the topics of interest towards the middle, but not at the beginning or the end (the actual end is cut off). Thus it most likely discusses topics in addition to research into osteoporosis. The second through fourth documents, which are considerably shorter, also have overlap among all terms of interest, and so are also probably of interest to the user. (The titles help to verify this.) The next three documents are all long, and from the TileBars we can tell they discuss research and prevention, but do not even touch on osteoporosis, and so probably are not of interest. Because the TileBars interface allows the user to specify the query in terms    292 USER INTERFACES AND VISUALIZATION FR88513-o157 A I': Oral"""" Sed&lt; $1 Billiona Yearfor Aging ~h SJMN: WOMEN'S HEALTH LEO ISLA nON PROPOSED AI': Older Alhlet.es Run For Science FR: Committee MeedngJ FR: October AdviseI)' Committees; Meetings FR881:ln-0046 FR: Otronk: Disease Burden and Pr even tion Models; Program AI' Swvey SII)'I Experts Split on Divenion of Ftinds for AIDS FR: Comolidated DcU~tiol1l ,of Aut!l(ltity forPoli,""Y Oevel'GpIt! SJMN: RESEARCH FOR BREAST CANCER IS STUCK IN -QIoIoy I \---I \.... Figure 10.15 An example of the TileBars retrieval results visualization [355]. of facets, where the terms for each facet are listed on an entry line, a color can be assigned to each facet. When the user displays a document with query term hits, the user can quickly ascertain what proportion of search topics appear in a passage based only on how many different highlight colors are visible. Most systems that use highlighting use only a single color to bring attention to all of the search terms. It would be difficult for users to specify in advance which patterns of term hits they are interested in. Instead, TileBars allows users to scan graphic representations and recognize which documents are and are not of interest. It may be the case that TileBars may be most useful for helping users discard misleadingly interesting documents, but only preliminary studies have been conducted to date. Passages can correspond to paragraphs or sections, fixed sized units of arbitrary length, or to automatically determined multiparagraph segments [355]. SeeSoft The SeeSoft visualization [232] represents text in a manner resembling columns of newspaper text, with one 'line' of text on each horizontal line of the strip. (See Figure 10.16.) The representation is compact and aesthetically pleasing. Graphics are used to abstract away the details, providing an overview showing the amount and shape of the text. Color highlighting is used to pick out various attributes, such as where a particular word appears in the text. Details of a smaller portion of the display can be viewed via a pop-up window; the overview    CONTEXT 293 shows mor e of th e te xt b ut in less det ail. Figure 10.16 An ex a m p le of th e SeeSof t visu aliz a t ion for showing locations of char \acters within a t ext [ 2 32]. S eeSof t was or iginally d esigned for softwar e d evelopment , in w h i c h a lin e of te xt is a mean ing fu l u n it of in f o r m a t i o n. ( P r og ra m m er s tend to pl a c e eac h in d ivid ual p rog r a m m ing statem ent on one line of text . ) Thus SeeSoft shows att r i b utes re lev a nt to the programming domain , such as which lines o f code were modified by which programm er , and how often particular lines h ave been modified , and how man y days have ela p sed s in ce the lines were last mod ified. The SeeSoft develop ers then experiment ed with applying thi s idea to the display of te xt , altho ugh thi s has not b een integrat ed into a n information access syste m. Co lor h ighli gh t ing is used to show which chara ct ers appear wher e in a b oo k of fi cti on , a nd whic h passages of the Bibl e co nta in references to particular p eopl e a nd it ems . Note the us e of t he a bstract ion of an ent ire line to sta n d for a s in gle w ord such as a c haracte r 's n a me (even t ho ug h t ho ug h t h is mig ht obsc ure a t igh tly i nter wo ven co nversat ion betwee n two cha racters ). 10 .6 .3 Query Term Hits Between Documents Other vi sualization ideas hav e b een d e v elop ed to s how a diff er en t kind of infor \mation about the r elationship b etw een qu er y te r ms and retrieved documents. Ra th er than showing how query terms appear within ind iv id ual doc ume nts , as is done i n KWI C interfaces and Tile Bars , th ese systems display an overv iew or s u m ma ry of t he retrieved documents according to whic h s ubset of qu er y te r ms t hey co nta in. Th e following s u bsec t io ns d escrib e var iat ions on th is idea.  ",
2242,mir,mir-2242,10.6.3 Query Term Hits Between Documents,"  CONTEXT 293 shows mor e of th e te xt b ut in less det ail. Figure 10.16 An ex a m p le of th e SeeSof t visu aliz a t ion for showing locations of char \acters within a t ext [ 2 32]. S eeSof t was or iginally d esigned for softwar e d evelopment , in w h i c h a lin e of te xt is a mean ing fu l u n it of in f o r m a t i o n. ( P r og ra m m er s tend to pl a c e eac h in d ivid ual p rog r a m m ing statem ent on one line of text . ) Thus SeeSoft shows att r i b utes re lev a nt to the programming domain , such as which lines o f code were modified by which programm er , and how often particular lines h ave been modified , and how man y days have ela p sed s in ce the lines were last mod ified. The SeeSoft develop ers then experiment ed with applying thi s idea to the display of te xt , altho ugh thi s has not b een integrat ed into a n information access syste m. Co lor h ighli gh t ing is used to show which chara ct ers appear wher e in a b oo k of fi cti on , a nd whic h passages of the Bibl e co nta in references to particular p eopl e a nd it ems . Note the us e of t he a bstract ion of an ent ire line to sta n d for a s in gle w ord such as a c haracte r 's n a me (even t ho ug h t ho ug h t h is mig ht obsc ure a t igh tly i nter wo ven co nversat ion betwee n two cha racters ). 10 .6 .3 Query Term Hits Between Documents Other vi sualization ideas hav e b een d e v elop ed to s how a diff er en t kind of infor \mation about the r elationship b etw een qu er y te r ms and retrieved documents. Ra th er than showing how query terms appear within ind iv id ual doc ume nts , as is done i n KWI C interfaces and Tile Bars , th ese systems display an overv iew or s u m ma ry of t he retrieved documents according to whic h s ubset of qu er y te r ms t hey co nta in. Th e following s u bsec t io ns d escrib e var iat ions on th is idea.    294 USER INTERFACES AND VISUALIZATION A B e 36 C D Figure 10.17 A sketch of the InfoCrystal retrieval results display [738]. InfoCrystal The InfoCrystal shows how many documents contain each subset of query terms [738]. This relieves the user from the need to specify Boolean ANDs and ORs in their query, while still showing which combinations of terms actually appear in documents that were ordered by a statistical ranking (although beyond four terms the interface becomes difficult to understand). The InfoCrystal allows visualization of all possible relations among N user-specified 'concepts' (or Boolean keywords). The InfoCrystal displays, in a clever extension of the Venn diagram paradigm, the number of documents retrieved that have each possible subset of the N concepts. Figure 10.17 shows a sketch of what the InfoCrystal might display as the result of a query against four keywords or Boolean phrases, labeled A, B, C, and D. The diamond in the center indicates that one document was discovered that contains all four keywords. The triangle marked with '12' indicates that 12 documents were found containing attributes A, B, and D, and so on. The InfoCrystal does not show proximity among the terms within the documents, nor their relative frequency. So a document that contains dozens of hits on 'volcano' and 'lava' and one hit on 'Mars' will be grouped with documents that contain mainly hits on 'Mars' but just one mention each of 'volcano' and 'lava.'    au1horing,~_==""'-tJ USab~i1y-linkS-and-fictiontJ CONTEXT 295 knowledge representati on Figure 10.18 An example of the VIBE retrieval results display [452]. VIBE and Lyberworld Graphical presentations that operate on similar principles are VIBE [452] and Lyberworld [363]. In these displays, query terms are placed in an abstract graphical space. After the search, icons are created that indicate how many documents contain each subset of query terms. The subset status of each group of documents is indicated by the placement of the icon. For example, in VIBE a set of documents that contain three out of five query terms are shown on an axis connecting these three terms, at a point midway between the representations of the three query terms in question. (See Figure 10.18.) Lyberworld presents a 3D version of this idea. Lattices Several researchers have employed a graphical depiction of a mathematical lattice for the purposes of query formulation, where the query consists of a set of constraints on a hierarchy of categories (actually, semantic attributes in these systems) [631, 147]. This is one solution to the problem of displaying documents in terms of multiple attributes; a document containing terms A, B, C, and D could be placed at a point in the lattice with these four categories as parents. However, if such a representation were to be applied to retrieval results instead of query formulation, the lattice layout would in most cases be too complex to allow for readability. None of the displays discussed in this subsection have been evaluated for effectiveness at improving query specification or understanding of retrieval results, but they are intriguing ideas and perhaps are useful in conjunction with other displays.    296 USER INTERFACES AND VISUALIZATION _.-~t.W:: ~""""""""""(1) r ... ,..J; ............... , ......... :. Figure 10.19 The SuperBook interface for showing retrieval results on a large manual in context [481].  ",
2243,mir,mir-2243,10.6.4 SuperBook: Context via Table of Contents," 10.6.4 SuperBook: Context via Table of Contents The SuperBook system [481, 229, 230] makes use of the structure of a large document to display query term hits in context. The table of contents (TOe) for a book or manual are shown in a hierarchy on the left-hand side of the display, and full text of a page or section is shown on the right-hand side. The user can manipulate the table of contents to expand or contract the view of sections and subsections. A focus-plus-context mechanism is used to expand the viewing area of the sections currently being looked at and compress the remaining sections. When the user moves the cursor to another part of the TOe, the display changes dynamically, making the new focus larger and shrinking down the previously observed sections. After the user specifies a query on the book, the search results are shown in the context of the table of contents hierarchy. (See Figure 10.19.) Those sections that contain search hits are made larger and the others are compressed. The query terms that appear in chapter or section names are highlighted in reverse video. When the user selects a page from the table of contents view, the page itself is displayed on the right-hand side and the query terms within the page are highlighted in reverse video. The SuperBook designers created innovative techniques for evaluating its special features. Subjects were compared using this system against using paper documentation and against a more standard online information access system. Subjects were also compared on different kinds of carefully selected tasks: browsing topics of interest, citation searching, searching to answer questions, and searching and browsing to write summary essays. For most of the tasks    CONTEXT 297 SuperBook subjects were faster and more accurate or equivalent in speed and accuracy to a standard system. When differences arose between Super Book and the standard system, the investigators examined the logs carefully and hypothesized plausible explanations. After the initial studies, they modified SuperBook according to these hypotheses and usually saw improvements as a result [481]. The user studies on the improved system showed that users were faster and more accurate at answering questions in which some of the relevant terms were within the section titles themselves, but they were also faster and more accurate at answering questions in which the query terms fell within the full text of the document only, as compared both to a paper manual and to an interface that did not provide such contextualizing information. SuperBook was not faster than paper when the query terms did not appear in the document text or the table of contents. This and other evidence from the SuperBook studies suggests that query term highlighting is at least partially responsible for improvements seen in the system.  ",
2244,mir,mir-2244,10.6.5 Categories for Results Set Context," 10.6.5 Categories for Results Set Context In section 10.4 we saw the use of category or directory information for providing overviews of text collection content. Category metadata can also be used to place the results of a query in context. For example, the original formulation of SuperBook allowed navigation within a highly structured document, a computer manual. The CORE project extended the main idea to a collection of over 1000 full-text chemistry articles. A study of this representation demonstrated its superiority to a standard search system on a variety of task types [228]. Since a table of contents is not available for this collection, context is provided by placing documents within a category hierarchy containing terms relevant to chemistry. Documents assigned a category are listed when that category is selected for more detailed viewing, and the categories themselves are organized into a hierarchy, thus providing a hierarchical view on the collection. Another approach to using predefined categories to provide context for retrieval results is demonstrated by the DynaCat system [650]. The DynaCat system organizes retrieved documents according to which types of categories, selected from the large MeSH taxonomy, are known in advance to be important for a given query type. DynaCat begins with a set of query types known to be useful for a given user population and collection. One query type can encompass many different queries. For example, the query type 'Treatment-Adverse Effects' covers queries such as 'What are the complications of a mastectomy?' as well as 'What are the side-effects of aspirin?' Documents are organized according to a set of criteria associated with each query type. These criteria specify which types of categories that are acceptable to use for organizing the documents and consequently, which categories should be omitted from the display. Once categories have been assigned to the retrieved documents, a hierarchy is formed based on where the categories exist within MeSH. The algorithm selects only a subset of the category    298 USER INTERFACES AND VISUALIZATION • .. Behavior and Behavior Mechanisms • Attitude • Attitude to Health • Por La Vida rnterventron model for cancer I:!reventlOn In Latlnas • Breast c""mer preventIOn educallon at a shOP-Ring center In Israel a stlJdent nurse comrnurliN health Qrojecl • Future challenges ;11 second""IJ~ prevention of breast cancer for wwnen at high riSk • ft, studv of diet and tlre""st c""mer prevention If) ':;anad3 'Nn'! t""lealth'( \i,/c'rnen QartlciRate rn (ontrol!ed tn""I'?, • Knowledge. Attitudes. Practice • F'ol L"" \/l&lt;i3 mterventlon mO':h?1 for c""r,,:er Rreventlol""' In Latln::lS • .... BehaYior and BehaYior Mechanism.s (14 refs) • Attitude (9 Wfs) • BehaVior (8 refs) • PS}'IChology, Social (3 refs) Biocbllmical PbellO_na, Metabolism, and Nllttltion (S rets) • Diet(~) Cbemicals an4 Drugs (52 refs) • AmiIIO Acids, Peptides, ud Proteins (?~f;l) • Antineopla.:Jtic ud 1 mm1lllOsuppressive ~~llenlts, 18,relf~3~~b~.!!!!1I!IIIIIIIIJI!!~~1 Figure 10.20 The DynaCat interface for viewing category labels that correspond to query types [650]. labels that might be assigned to the document to be used in the organization. Figure 10.20 shows the results for a query on breast cancer prevention. The interface is tiled into three windows. The top window displays the user's query and the number of documents found. The left window shows the categories in the first two levels of the hierarchy, providing a table of contents view of the organization of search results. The right pane displays all the categories in the hierarchy and the titles of the documents that belong in those categories. An obstacle to using category labels to organize retrieval results is the requirement of precompiled knowledge about which categories are of interest for a particular user or a particular query type. The SONIA system [692] circumvents this problem by using a combination of unsupervised and supervised methods to organize a set of documents. The unsupervised method (document clustering similar to Scatter/Gather) imposes an initial organization on a user's personal information collection or on a set of documents retrieved as the result of a query. The user, can then invoke a direct manipulation interface to make adjustments to this initial clustering, causing it to align more closely with their preferences (because unsupervised methods do not usually produce an organization that corresponds to a human-derived category structure [357]). The resulting organization is then used to train a supervised text categorization algorithm which automatically classifies any new documents that are added to the collection. As the collection grows it can be periodically reorganized by rerunning the clustering algorithm and redoing the manual adjustments.    CONTEXT 299  ",
2245,mir,mir-2245,10.6.6 Using Hyperlinks to Organize Retrieval Results," 10.6.6 Using Hyperlinks to Organize Retrieval Results Although the SuperBook authors describe it as a hypertext system, it is actually better thought of as a means of showing search results in the context of a structure that users can understand and view all at once. The hypertext component was not analyzed separately to assess its importance, but it usually is not mentioned by the authors when describing what is successful about their design. In fact, it seems to be responsible for one of the main problems seen with the revised version of the system \that users tend to wander off (often unintentionally) from the pages they are reading, thus causing the time spent on a given topic to be longer for SuperBook in some cases. (Using completion time to evaluate users on browsing tasks can be problematic, however, since by definition browsing is a casual, unhurried process [804].) This wandering may occur in part because SuperBook uses a non-standard kind of hypertext, in which any word is automatically linked to occurrences of the same word in other parts of the document. This has not turned out to be how hypertext links are created in practice. Today, hyperlinked help systems and hyperlinks on the Web make much more discriminating use of hyperlink connections (in part since they are usually generated by an author rather than automatically). These links tend to be labeled i na somewhat meaningful manner by their surrounding context. Back-of-the-book indexes often do not contain listings of every occurrence of a word, but rather to the more important uses or the beginnings of series of uses. Automated hypertext linking should perhaps be based on similar principles. Additionally, at least one study showed that users formed better mental models of a small hypertext system that was organized hierarchically than one that allowed more flexible access [226]. Problems relating to navigation of hypertext structure have long been suspected and investigated in the hypertext literature [181, 551, 440, 334]. More recent work has made better use of hyperlink information for providing context for retrieval results. Some of this work is described below. Cha-Cha: Super Book on the Web The Cha-Cha intranet search system [164] extends the SuperBook idea to a large heterogeneous Web site such as might be found in an organization's intranet. Figure 10.21 shows an example. This system differs from SuperBook in several ways. On most Web sites there is no existing real table of contents or category structure, and an intranet like those found at large universities or large corporations is usually not organized by one central unit. Cha-Cha uses link structure present within the site to create what is intended to be a meaningful organization on top of the underlying chaos. After the user issues a query, the shortest paths from the root page to each of the search hits are recorded and a subset of these are selected to be shown as a hierarchy, so that each hit is shown only once. (Users can begin with a query, rather than with a table of contents view.) If a user does not know to use the term 'health center' but instead queries on 'medical center,' if 'medical' appears as a term in a document within    300 USER INTERFACES AND VISUALIZATION University Health SerVices .... Health Services for F acultv and Staff y Oth, Programs Ava,lable to Faculty aod Staff , Health Net Colleges and SChools. .. School of Social Weltere: Home Page "" proarams, Curricula, and Coyrses y MSW PR.QGfIAM • [""illd wort Aaenges y The Letters &amp;, Scence WWW Home paae "" Departments !ls Dlylslons y Toyeod Center for the Humanities, UC Berkeley SeDtember Townse~ Center Newsletter .~ y UC Berkeley Libranes y tle1tSciences Information Seryice HSIS Medical Informatcs .... cenir for Southeast As,a Studies (SEAS NewslPtter.Upcom,na Events, Spong 1996 Policies and Guidelines for web publishing at ."" ., IrnaielMultimedj. Database Resources MediCI' Imaae Databases Page Summary iJ:i HuM Net Health Net HealthNet Health Care."". ""university Health Services (VHS) at the Un,versitY of Caldomla at Berkeley offers general medical office VISits, physical therapy, and laboratory semC9S to faculty and staff who are HealthNet members and have selected a Personal Care PhYSician (PCP) at the Tang Center"" "".""HospltalizatlOn: If you , need to be hospitalized, in most cases you will be cared for at Alta Bates Medical Center by a physician affihated with Alta l!ates"""""".Tittle is ad:Jve In Quality assurance actvities at UniverSIty Health Services where he has been a phYSICIan since 1977, He received his medical degree from y Stanfond University ,n 1973 and q&lt; speaalized in Internal Medione during f hrs reSidences at Pacific Medical ; Center and VCS ... Figure 10.21 The Cha-Cha interface for showing Web intranet search results in context displaying results on the query 'medical centre'[164]. the health center part of the Web, the home page (or starting point) of this center will be presented as well as the more specific hits. Users can then either query or navigate within a subset of sites if they wish. The organization produced by this simple method is surprisingly comprehensible on the UC Berkeley site. It seems especially useful for providing the information about the sources (the Web server) associated with the search hits, whose titles are often cryptic. The AMIT system [826] also applies the basic ideas behind SuperBook to the Web, but focuses on a single-topic Web site, which is likely to have a more reasonable topic structure than a complex intranet. The link structure of the Web site is used as contextualizing information but all of the paths to a given document are shown and focus-plus-context is used to emphasize subsets of the document space. The WebTOC system [585] is similar to AMIT but focuses on showing the structure and number of documents within each Web subhierarchy, and is not tightly coupled with search.    CONTEXT 301 Figure 10.22 Example of a Web subset visualized by Mapuccino (courtesy of M. Jacovi, B. Shaul and Y. Maarek). Mapuccino: Graphical Depiction of Link Structure The Mapuccino system (formerly WebCutter) [527] allows the user to issue a query on a particular Web site. The system crawls the site in real-time, checking each encountered page for relevance to the query. When a relevant page is found; the weights on that page's outlinks are increased. Thus, the search is based partly on an assumption that relevant pages will occur near one another in the Web site. The subset of the Web site that has been crawled is depicted graphically in a nodes-and-links view (see Figure 10.22). This kind of display does not provide the user with information about what the contents of the pages are, but rather only shows their link structure. Other researchers have also investigated spreading activation among hypertext links as a way to guide an information retrieval system, e.g., [278, 555].  ",
2246,mir,mir-2246,10.6.7 Tables," 10.6.7 Tables Tabular display is another approach for showing relationships among retrieval documents. The Envision system [273] allows the user to organize results according to metadata such as author or date along the X and Y-axes, and uses graphics to show values for attributes associated with retrieved documents within each cell (see Figure 10.23). Color, shape, and size of an iconic representation of a document are used to show the computed relevance, the type of document, or    302 USER INTERFACES AND VISUALTZATTON ...... ~ .. ,.-.. . \-. \--1lI Figure 10.23 The Envision tabular display for graphically organizing retrieved documents [270]. other attributes. Clicking on an icon brings up more information about the document in another window. Like the WebCutter system, this view provides few cues about how the documents are related to one another in terms of their content or meaning. The SenseMaker system also allows users to group documents into different views via a table-like display [51], including a Scatter/Gather [203J style view. Although tables are appealing, they cannot show the intersections of many different attributes; rather they are better for pairwise comparisons. Another problem with tables for display of textual information is that very little information can be fitted on a screen at a time, making comparisons difficult. The Table Lens [666] is an innovative interface for viewing and interactively reorganizing very large tables of information (see Figure 10.24). It uses focus-plus-context to fit hundreds of rows of information in a space occupied by at most two dozen rows in standard spreadsheets. And because it allows for rapid reorganization via sorting of columns, users can quickly switch from a view focused around one kind of metadata to another. For example, first sorting documents by rank and then by author name can show the relative ranks of different articles by the same author. A re-sort by date can show patterns in relevance scores with respect to date of publication. This rapid re-sorting capability helps circumvent the problems associated with the fact that tables cannot show many simultaneous intersections. Another variation on the table theme is that seen in the Perspective Wall [5301 in which a focus-plus-context display is used to center information currently    USING RELEVANCE JUDGEMENTS 303 Figure 10.24 The TableLens visualization [666]. of interest in the middle of the display, compressing less important information into the periphery on the sides of the wall. The idea is to show in detail the currently most important information while at the same time retaining the context of the rest of the information. For example, if viewing documents in chronological order, the user can easily tell if they are currently looking at documents in the beginning, middle, or end of the time range. These interfaces have not been applied to information access tasks. The problem with such displays when applied to text is that they require an attribute that can be shown according to an underlying order, such as date. Unfortunately, information useful for organizing text content, such as topic labels, does not have an inherent meaningful order. Alphabetical order is useful for looking up individual items, but not for seeing patterns across items according to adjacency, as in the case for ordered data types like dates and size.  ",
2247,mir,mir-2247,10.7 Using Relevance Judgements," 10.7 Using Relevance Judgements An important part of the information access process is query reformulation, and a proven effective technique for query reformulation is relevance feedback. In its original form, relevance feedback refers to an interaction cycle in which the user selects a small set of documents that appear to be relevant to the query, and the system then uses features derived from these selected relevant documents to revise the original query. This revised query is then executed and a new set of documents is returned. Documents from the original set can appear in the new results    304 USER INTERFACES AND VISUALIZATION list, althotighthey are likely to appear in a different rank order. Relevance feedback in its original form has been shown to be an effective mechanism for improving retrieval results in a variety of studies and settings [702, 343, 127J. In recent years the scope of ideas that can be classified under this term has widened greatly. Relevance feedback introduces important design choices, including which operations should be performed automatically by the system and which should be user initiated and controlled. Bates discusses this issue in detail [66], asserting that despite the emphasis in modern systems to try to automate the entire process, an intermediate approach in which the system helps automate search at a stmtegic level is preferable. Bates suggests an analogy of an automatic camera versus one with adjustable lenses and .shutter speeds. On many occasions, a quick, easy method that requires little training or thought is appropriate. At other times the user needs more control over the operation of the machinery, while still not wanting to know about the low level details of its operation. A related idea is that, for any interface, control should be described in terms of the task being done, not in terms of how the machine can be made to accomplish the task [607J. Continuing the camera analogy, the user should be able to control the mood created by the photograph, rather than the adjustment of the lens. In information access systems, control should be over the kind of information returned, not over which terms are used to modify the query. Unfortunately it is often quite difficult to build interfaces to complex systems that behave in this manner.  ",
2248,mir,mir-2248,10.7.1 Interfaces for Standard Relevance Feedback," 10.7.1 Interfaces for Standard Relevance Feedback A standard interface for relevance feedback consists of a list of titles with checkboxes beside the titles that allow the user to mark relevant documents. This can imply either that unmarked documents are not relevant or that no opinion has been made about unmarked documents, depending on the system. Another option is to provide a choice among several checkboxes indicating relevant or not relevant (with no selection implying no opinion). In some cases users are allowed to indicate a value on a relevance scale [73J. Standard relevance feedback algorithms usually do not perform better given negative relevance judgement evidence [225], but machine learning algorithms can take advantage of negative feedback [629, 460]. After the user has made a set of relevance judgements and issued a search command, the system can either automatically reweight the query and re-execute the search, or generate a list of terms for the user to select from in order to augment the original query. (See Figure 10.25, taken from [448].) Systems usuallydo not suggest terms to remove from the query. After the query is re-executed, a new list of titles is shown: It can be helpful to retain an indicator such as a marked checkbox beside the documents that the user has already judged. A difficult design decision concerns whether or not to show documents that the user has already viewed towards the top of the ranked list [1 J. Repeatedly showing the same set of documents at the top may inconvenience a User Who is trying to create a large set of relevant documents,    USING RELEVANCE JUDGEMENTS 305 ~ P!""lOC ~.5"" 'l.lJti Qi..:IRVI &amp; Sftrttl 'fock 'ftxt! f.""... ''''c.. ,;! ,jdt It\\.! \tr«:)l)IRVJ Enttr ("".;(1') MIlo' ~.rr t&gt;,:ow'M ""it &lt;llf1""VIUi~ fl&lt;;\' ,G'I I""H~.IYOl.i~tll;,fd C 4\X;.i""'l'lf!'!t$ • i I 1 QMPI'm .. _U AlQQQ""M'UQ"" MO .. ""' ........... ~ C""t·,""tQ;lttV ~.~ . ~(l""lI')) 2 C~'~d~ull v~I&lt;;I~tQ __ if ~tiv.P.rts ~--~N •• I ';,i~'l·r'I.M'tT'"",·· J &gt; i~Mf&gt;tc"",H&lt;lnd&amp; C~.CII'ltf«ltt \---AW""li'Stt'Mf car' . P4~latldGU""'HSlr~ Of~dt .. rnJeIU.C~ Mft'tt"" 'iKai· • ~.l UQton Corp ~n~ '96,oooC'fl fO' o.feetlW ""111;*1 rot"" cf t1"""" dOC~h rettl""""""':; NmC re r.* t=::J .. Oe,~t., ce 574"" (;M1""am.toRe&lt;,U e~_oo:o lHIt-t9Clin W't'"" QU.4. !1'I9ltofl I 1M! Qiwfr\l I '6~~~;t2i~: s-eeer f():A~t;.l UJ rACE '2 oe'!R:CI'! \-~'a; Mo~o'~ COt~ ~""4 rt» t~'.Hi!'0-9 2.000 t~8'-69 ~i C""l t&lt;w&lt;~ witl'\ it!I'l;9h-ttCl'\ (NM"" iL~.~ ~ ~t~~t ~~;ii~ d:~~d~,';I:J:,;~t~CH~' ~~!t""~ 1\I;&lt;k Skylarir; _~'t~ ~~ ,witt! tN '!&gt;-v""'"" '-::ill~r Qwl;,l4 e""'jf,,~!'1#W M' Il~~ 1""'a:t ~&lt;X.&gt;l"" C'.&lt;:;,. ,-,'aw·&lt;ltef'''""'tl'lf~~tAi~~CU!'li.1r''(:tw~ reNrn of 2\. btl '''\\.ll~ t&gt;v iUkj A~t!iM.olf tc ee ""\jitytut' I'.~~,,,~J-...~~)i~""""~ lv'loloil'$~l'\O !""!I;I1'n !fll.l'tmgfr~meo l~l4tn11 WSOI'!l.beoUt )12000 ':.!~~iXl('d .... jtl'; Qu-ld ol enQi&lt;'\C~ l"" ~ 1~e.t-e~,~' Yt!.r~ r&gt; ""fI¢~l act,&lt;)j'""~ CM ~.,c ,t "" ''t&lt;..{iiA9 .~out 3,2""""-.10 01 i~ 1 sso OI~ilt'ClitUI% Calai •• ~ !4l,{;:1i. S!\\)il.r~ """"~h tc eo #.,,;tl-:;~ 1ffe1:~, ~'"" &amp;,~ ~,""&gt;t~ ~ Quad. ~ a-nte,,-~ """"&lt;I::' S~litf'r t,)u, (;:rW~ .. :kr CM ij;,&lt;-t .,"",.,~:y ~:r fl'tS '/&gt;"" ,!'lIUN!} rt'IH~(f rc (t,. tut: HfIot tJtI:X:&gt;Il!l'N If&gt; ;~ 9r~ (!f at"" ~ $""'"""",m,,"" ~a;d A~' 'o!O'''lr~ WIl!tJot a':&gt;flff'«&lt; ,,1':hM9t ""c o .... ""tr. ~ omw~sailj. $.t'Ca!,,~I;.'_~\jS: s .. 1~~ ar~ &lt;:;1 Vv1bwa&lt;}r' At's A~, ~j~:""'i aid It'~ .~,alli~ \,6/)(: 'V90--~~i AWl eo. ,.,;? !'l'dC~Quatt'~iUl(u'I;&lt;:atj tc r~i .. ce a-ck~tiVf'Mjt;"" ~ass~lyr!""~t!(X:h~;t~,!r''''9''&lt;'''t!!''~~a'lsrGrl&lt;~1 rl'lt'W~Ct.'veM!'t~C&gt;\l;&lt;lI.&gt;'!!'a~ (al,;~ir&gt;g I'""'&gt;""~""""""""""'-j~: t,) Figure 10.25 An example of an interface for relevance feedback [448]. but at the same time, this can serve as feedback indicating that the revised query does not downgrade the ranking of those documents that have been found especially important. One solution is to retain a separate window that shows the rankings of only the documents that have not been retrieved or ranked highly previously. Another solution is to use smaller fonts or gray-out color for the titles of documents already seen. Creating multiple relevance judgements is an effortful task, and the notion of relevance feedback is unfamiliar to most users. To circumvent these problems, Web-based search engines have adopted the terminology of 'more like this' as a simpler way to indicate that the user is requesting documents similar to the selected one. This 'one-click' interaction method is simpler than standard relevance feedback dialog which requires users to rate a small number of documents and then request a reranking. Unfortunately, in most cases relevance feedback requires many relevance judgements in order to work well. To partly alleviate this problem, Aalbersberg [1] proposes incremental relevance feedback which works well given only one relevant document at a time and thus can be used to hide the two-step procedure from the user.  ",
2249,mir,mir-2249,10.7.2 Studies of User Interaction with Relevance Feedback Systems," 10.7.2 Studies of User Interaction with Relevance Feedback Systems Standard relevance feedback assumes the user is involved in the interaction by specifying the relevant documents. In some interfaces users are also able to    306 USER INTERFACES AND VISUALIZATION select which terms to add to the query. However, most ranking and reweighting algorithms are difficult to understand or predict (even for the creators of the algorithms!) and so it might be the case that users have difficulties controlling a relevance feedback system explicitly. A recent study was conducted to investigate directly to what degree user control of the feedback process is beneficial. Koenemann and Belkin [448] measured the benefits of letting users 'under the hood' during relevance feedback. They tested four cases using the Inquery system [772]: • Control No relevance feedback; the subjects could only reformulate the query by hand. • Opaque The subjects simply selected relevant documents and saw the revised rankings. • Transparent The subjects could see how the system reformulated the queries (that is, see which terms were added \- the system did not reweight the subjects' query terms) and the revised rankings. • Penetrable The system is stopped midway through the reranking process. The subjects are shown the terms that the system would have used for opaque and transparent query reformulation. The subjects then select which, if any, of the new terms to add to the query. The system then presents the revised rankings. The 64 subjects were much more effective (measuring precision at a cutoff of top 5, top 10, top 30, and top 100 documents) with relevance feedback than without it. The penetrable group performed significantly better than the control, with the opaque and transparent performances falling between the two in effectiveness. Search times did not differ significantly among the conditions, but there were significant differences in the number of feedback iterations. The subjects in the penetrable group required significantly fewer iterations to achieve better queries (an average of 5.8 cycles in the penetrable group, 8.2 cycles in the control group, 7.7 cycles in the opaque group, and surprisingly, the transparent group required more cycles, 8.8 on average). The average number of documents marked relevant ranged between 11 and 14 for the three conditions. All subjects preferred relevance feedback over the baseline system, and several remarked that they preferred the 'lazy' approach of selecting suggested terms over having to think up their own. An observational study on a TTY-based version of an online catalog system [338] also found that users performed better using a relevance feedback mechanism that allowed manual selection of terms. However, a later observational study did not find overall success with this form of relevance feedback [337]. The authors attribute these results to a poor design of a new graphical interface. These results may also be due to the fact that users often selected only one relevant document before performing the feedback operation, although they were using a system optimized from multiple document selection.    USING RELEVANCE JUDGEMENTS 307  ",
2250,mir,mir-2250,10.7.3 Fetching Relevant Information in the Background," 10.7.3 Fetching Relevant Information in the Background Standard relevance feedback is predicated on the goal of improving an ad hoc query or building a profile for a routing query. More recently researchers have begun developing systems that monitor users' progress and behavior over long interaction periods in an attempt to predict which documents or actions the user is likely to want in future. These systems are called semi-automated assistants or recommender 'agents,' and often make use of machine learning techniques [565]. Some of these systems require explicit user input in the form of a goal statement [406] or relevance judgements [629], while others quietly record users' actions and try to make inferences based on these actions. A system developed by Kozierok and Maes [460, 536] makes predictions about how users will handle email messages (what order to read them in, where to file them) and how users will schedule meetings in a calendar manager application. The system 'looks over the shoulder' of the users, recording every relevant action into a database. After enough data has been accumulated, the system uses a nearest-neighbors method [743] to predict a user's action based on the similarity of the current situation to situations already encountered. For example, if the user almost always saves email messages from a particular person into a particular file, the system can offer to automate this action the next time a message from that person arrives [536]. This system integrates learning from both implicit and explicit user feedback. If a user ignores the system's suggestion, the system treats this as negative feedback, and accordingly adds the overriding action to the action database. After certain types of incorrect predictions, the system asks the user questions that allow it to adjust the weight of the feature that caused the error. Finally, the user can explicitly train the system by presenting it with hypothetical examples of input-action pairs. Another system, Syskill and Webert [629], attempts to learn a user profile based on explicit relevance judgements of pages explored while browsing the Web. In a sense this is akin to standard relevance feedback, except the user judgements are retained across sessions and the interaction model differs: as the user browses a new Web page, the links on the page are automatically annotated as to whether or not they should be relevant to the user's interest. A related system is Letizia [518], whose goal is to bring to the user's attention a percentage of the available next moves that are most likely to be of interest, given the user's earlier actions. Upon request, Letizia provides recommendations for further action on the user's part, usually in the form of suggestions of links to follow when the user is unsure what to do next. The system monitors the user's behavior while navigating and reading Web pages, and concurrently evaluates the links reachable from the current page. The system uses only implicit feedback. Thus, saving a page as a bookmark is taken as strong positive evidence for the terms in the corresponding Web page. Links skipped are taken as negative support for the information reachable from the link. Selected links can indicate positive or negative evidence, depending on how much time the user spends on the resulting page and whether or not the decision to leave a page quickly is later reversed. Additionally, the evidence for user interest remains persistent across    308 USER INTERFACES AND VISUALIZATION browsing sessions. Thus, a user who often reads kayaking pages is at another time reading the home page of a professional contact and may be alerted to the fact that the colleague's personal interests page contains a link to a shared hobby. The system uses a best-first search strategy and heuristics to determine which pages to recommend most strongly. A more user-directed approach to prefetching potentially relevant information is seen in the Butterfly system [531]. This interface helps the user follow a series of citation links from a given reference, an important information seeking strategy [66]. The system automatically examines the document the user is currently reading and prefetches the bibliographic citations it refers to. It also retrieves lists of articles that cite the focus document. The underlying assumption is that the services from which the citations are requested do not respond immediately. Rather than making the user wait during the delay associated with each request, the system handles many requests in parallel and the interface uses graphics and animations to show the incrementally growing list of available citations. The system does not try to be clever about which cites to bring first; rather the user can watch the 'organically' growing visualization of the document and its citations, and based on what looks relevant, direct the system as to which parts of the citation space to spend more time on.  ",
2251,mir,mir-2251,10.7.4 Group Relevance Judgements," 10.7.4 Group Relevance Judgements Recently there has been much interest in using relevance judgements from a large number of different users to rate or rank information of general interest [672]. Some variations of this social recommendation approach use only similarity among relevance judgements by people with similar tastes, ignoring the representation of the information being judged altogether. This has been found highly effective for rating information in which taste plays a major role, such as movie and music recommendations [720]. More recent work has combined group relevance judgements with content information [64]. 10.7.5 Pseudo-Relevance Feedback At the far end of the system versus user feedback spectrum is what is informally known as pseudo-relevance feedback. In this method, rather than relying on the user to choose the top k relevant documents, the system simply assumes that its top-ranked documents are relevant, and uses these documents to augment the query with a relevance feedback ranking algorithm. This procedure has been found to be highly effective in some settings [760, 465, 12], most likely those in which the original query statement is long and precise .. An intriguing extension to this idea is to use the output of clustering of retrieval results as the input to a relevance feedback mechanism, either by having the user or the system select the cluster to be used [359], but this idea has not yet been evaluated.  ",
2252,mir,mir-2252,10.7.5 Pseudo-Relevance Feedback," 10.7.5 Pseudo-Relevance Feedback At the far end of the system versus user feedback spectrum is what is informally known as pseudo-relevance feedback. In this method, rather than relying on the user to choose the top k relevant documents, the system simply assumes that its top-ranked documents are relevant, and uses these documents to augment the query with a relevance feedback ranking algorithm. This procedure has been found to be highly effective in some settings [760, 465, 12], most likely those in which the original query statement is long and precise .. An intriguing extension to this idea is to use the output of clustering of retrieval results as the input to a relevance feedback mechanism, either by having the user or the system select the cluster to be used [359], but this idea has not yet been evaluated.    IN TERFA CE S UPPOR T FOR THE SEAR CH PROCESS 30 9 1 ,~.8 Interface Support for the Search Process The user inte r face des igner mus t make decisions abou t how to arra nge vario us kinds of in fo rmat ion on t he com p uter scree n a nd how to st ructure the p ossible se que nces of inte ractions . This des ign problem is especia lly daunti ng for a complex activity like information access . In this section w e discuss design cho ices s ur ro u nd ing t he layo ut of in format ion within comp lex in for matio n syste ms , a nd ill us t r a t e t he id eas wi th exa m p les of ex isti ng inter faces . We beg in with a discussion of very si m ple search interfaces , those used for string search in ' fi n d' operations , and then progress to multiwindow interfaces and sophisti \cated works paces . This is followed by a discuss ion of the in t egr a t i on of scanning , selecti ng , and q ueryi ng wit h in in f or m a t i on access i nter faces a n d conclu des wi th interface su pport for r et a in in g t h e his tory of the search process. 10.8 .1 Interfaces for String Match ing A common simple search need is tha t of th e ' fi n d' operation , typ ically run over the contents of a doc u ment t hat is currently being viewed . Usually this funct i o n does not prod uce r a nk ed o ut p ut , n or a llow Bo ole an com b i nat ions of ter ms; t he main operation is a simple string match (wit ho u t regular expression capabilities ). T ypicall y a special purpos e search window is cre a t e d , containing a f ew simple controls (e.g. , case-sensitivity , sea rch forwa rd or backward ) . T h e use r ty pes the query stri ng into an ent ry f or m an d string matches a re high lig hte d i n t he ta rget docu ment (see Figur e 10.26) . The nex t degree of comple xity is th e ' fi n d ' function for searching ac ross small co llect ions , such as the files on a personal comp uter 's hard disk , or the history li st o f a We b b rowse r . T h is type of f un ct ion is also us ua lly im ple me nte d as a simple string match . Again , the controls and parameter se ttings are sho wn at the top of a special pur pose search window and the va rious opt ions are set via check b oxe s an d e nt ry fo r ms . Th e d ifference f rom the p re vio us exa m p le is t hat a r esul t s list is shown w it h in t he sea rc h in t erf a ce it s elf (see F igure 10.27) . A common pr o b lem a rises even in these very simple inte r faces. An ambig u \ous state occurs in which the results f o r an earlier search are sho wn while the u ser i s e nte ring a new q ue ry or mo di fying t he previous one . If t he us er ty pes in Figure 1 0\. 26 An example of a simple interface for string matching , from Netsca pe Communicator 4 .05.  ",
2253,mir,mir-2253,10.8 Interface Support for the Search Process,"  IN TERFA CE S UPPOR T FOR THE SEAR CH PROCESS 30 9 1 ,~.8 Interface Support for the Search Process The user inte r face des igner mus t make decisions abou t how to arra nge vario us kinds of in fo rmat ion on t he com p uter scree n a nd how to st ructure the p ossible se que nces of inte ractions . This des ign problem is especia lly daunti ng for a complex activity like information access . In this section w e discuss design cho ices s ur ro u nd ing t he layo ut of in format ion within comp lex in for matio n syste ms , a nd ill us t r a t e t he id eas wi th exa m p les of ex isti ng inter faces . We beg in with a discussion of very si m ple search interfaces , those used for string search in ' fi n d' operations , and then progress to multiwindow interfaces and sophisti \cated works paces . This is followed by a discuss ion of the in t egr a t i on of scanning , selecti ng , and q ueryi ng wit h in in f or m a t i on access i nter faces a n d conclu des wi th interface su pport for r et a in in g t h e his tory of the search process. 10.8 .1 Interfaces for String Match ing A common simple search need is tha t of th e ' fi n d' operation , typ ically run over the contents of a doc u ment t hat is currently being viewed . Usually this funct i o n does not prod uce r a nk ed o ut p ut , n or a llow Bo ole an com b i nat ions of ter ms; t he main operation is a simple string match (wit ho u t regular expression capabilities ). T ypicall y a special purpos e search window is cre a t e d , containing a f ew simple controls (e.g. , case-sensitivity , sea rch forwa rd or backward ) . T h e use r ty pes the query stri ng into an ent ry f or m an d string matches a re high lig hte d i n t he ta rget docu ment (see Figur e 10.26) . The nex t degree of comple xity is th e ' fi n d ' function for searching ac ross small co llect ions , such as the files on a personal comp uter 's hard disk , or the history li st o f a We b b rowse r . T h is type of f un ct ion is also us ua lly im ple me nte d as a simple string match . Again , the controls and parameter se ttings are sho wn at the top of a special pur pose search window and the va rious opt ions are set via check b oxe s an d e nt ry fo r ms . Th e d ifference f rom the p re vio us exa m p le is t hat a r esul t s list is shown w it h in t he sea rc h in t erf a ce it s elf (see F igure 10.27) . A common pr o b lem a rises even in these very simple inte r faces. An ambig u \ous state occurs in which the results f o r an earlier search are sho wn while the u ser i s e nte ring a new q ue ry or mo di fying t he previous one . If t he us er ty pes in Figure 1 0\. 26 An example of a simple interface for string matching , from Netsca pe Communicator 4 .05.  ",
2254,mir,mir-2254,10.8.1 Interfaces for String Matching,"  IN TERFA CE S UPPOR T FOR THE SEAR CH PROCESS 30 9 1 ,~.8 Interface Support for the Search Process The user inte r face des igner mus t make decisions abou t how to arra nge vario us kinds of in fo rmat ion on t he com p uter scree n a nd how to st ructure the p ossible se que nces of inte ractions . This des ign problem is especia lly daunti ng for a complex activity like information access . In this section w e discuss design cho ices s ur ro u nd ing t he layo ut of in format ion within comp lex in for matio n syste ms , a nd ill us t r a t e t he id eas wi th exa m p les of ex isti ng inter faces . We beg in with a discussion of very si m ple search interfaces , those used for string search in ' fi n d' operations , and then progress to multiwindow interfaces and sophisti \cated works paces . This is followed by a discuss ion of the in t egr a t i on of scanning , selecti ng , and q ueryi ng wit h in in f or m a t i on access i nter faces a n d conclu des wi th interface su pport for r et a in in g t h e his tory of the search process. 10.8 .1 Interfaces for String Match ing A common simple search need is tha t of th e ' fi n d' operation , typ ically run over the contents of a doc u ment t hat is currently being viewed . Usually this funct i o n does not prod uce r a nk ed o ut p ut , n or a llow Bo ole an com b i nat ions of ter ms; t he main operation is a simple string match (wit ho u t regular expression capabilities ). T ypicall y a special purpos e search window is cre a t e d , containing a f ew simple controls (e.g. , case-sensitivity , sea rch forwa rd or backward ) . T h e use r ty pes the query stri ng into an ent ry f or m an d string matches a re high lig hte d i n t he ta rget docu ment (see Figur e 10.26) . The nex t degree of comple xity is th e ' fi n d ' function for searching ac ross small co llect ions , such as the files on a personal comp uter 's hard disk , or the history li st o f a We b b rowse r . T h is type of f un ct ion is also us ua lly im ple me nte d as a simple string match . Again , the controls and parameter se ttings are sho wn at the top of a special pur pose search window and the va rious opt ions are set via check b oxe s an d e nt ry fo r ms . Th e d ifference f rom the p re vio us exa m p le is t hat a r esul t s list is shown w it h in t he sea rc h in t erf a ce it s elf (see F igure 10.27) . A common pr o b lem a rises even in these very simple inte r faces. An ambig u \ous state occurs in which the results f o r an earlier search are sho wn while the u ser i s e nte ring a new q ue ry or mo di fying t he previous one . If t he us er ty pes in Figure 1 0\. 26 An example of a simple interface for string matching , from Netsca pe Communicator 4 .05.    310 USER INTERFACES AND VISUALIZATION l!i Searching UC ... h!tp:IIwww-resOlSce... 717/1998... 1 hours ago 8/271199.. .ElQJ"" !if ...... T't;·UC·B·;kei·:·· .. htiP:;iiibia;y:·t;;k.:··:···1··h;;'~s·ag;;··f~s .. a; · s / 2 i h · ii9 : : · : · .. ····· 3 !if Befkeley Pledge http/lwww.urel.berk .. 1 hours ago 1 hours ago 8127/199... 1 !if 1998 Berkeleya .. htlp/lwww.urel.berk 1 hours ago 1 hours ago 8/27/199... 3 !if Berkeleyan Arc... http://www.urel.berk 1 hours ago 1 hours ago 8/27/199... 1 !if Berkeleyan / PI. .. http://www.urel.berk 1 hours ago 1 hours ago 8/27/199. 3 !if Berkeleyan/PL http://_urel.berL 1 hours ago 1 hourSC!lgo 8/27/199... 1 !if 02·25-98 Berkel http/lwww.urel.beJk. 2 hours ago 1 hoUIS ago 8127/199. 7 !if UCBerkeleyOir http//www·resource 7/22/199. 1 hours ago 8/27/199.. 55 Pal UC Berkeley Oil... http//wwwberkeley 1hourSaqo. 1 hours aQO B/27/199.. ' .. 4&gt;.r.i Il;k ....... ~IItIIIPA!III~ •• .,4It ...... ,. Figure 10.27 An example of an string matching over a list, in this case, a history of recently viewed Web pages, from Netscape Communicator 4.05. new terms and but then does not activate the search, the interface takes on a potentially misleading state, since a user could erroneously assume that the old search hits shown correspond to the newly typed-in query. One solution for this problem is to clear the results list as soon as the user begins to type in a new query. However, the user may want to refer to terms shown in the search results to help reformulate the query, or may decide not to issue the new query and instead continue with the previous results. These goals would be hampered by erasing the current result set as soon as the new query is typed. Another solution is to bring up a new window for every new query. However, this requires the user to execute an additional command and can lead to a proliferation of windows. A third, probably more workable solution, is to automatically 'stack' the queries and results lists in a compact format and allow the user to move back and forth among the stacked up prior searches. Simple interfaces like these can be augmented with functionality that can greatly aid initial query formulation. Spelling errors are a major cause of void result sets. A spell-checking function that suggests alternatives for query terms that have low frequency in the collection might be useful at this stage. Another option is to suggest thesaurus terms associated with the query terms at the time the query terms are entered. Usually these kinds of information are shown after the query is entered and documents have been retrieved, but an alternative is to provide this information as the user enters the query, in a form of query preview.    INTERFACE SUPPORT FOR THE SEARCH PROCESS 311  ",
2255,mir,mir-2255,10.8.2 Window Management," 10.8.2 Window Management For search tasks more complex than the simple string matching find operations described above, the interface designer must decide how to layout the various choices and information displays within the interface. As discussed above, traditional bibliographic search systems use TTYbased command-line interfaces or menus. When the system responds to a command, the new results screen obliterates the contents of the one before it, requiring the user to remember the context. For example, the user can usually see only one level of a subject hierarchy at a time, and must leave the subject view in order to see query view or the document view. The main design choices in such a system are in the command or menu structure, and the order of presentation of the available options. In modern graphical interfaces, the windowing system can be used to divide functionality into different, simultaneously displayed views [582]. In information access systems, it is often useful to link the information from one window to the information in another, for example, linking documents to their position in a table of contents, as seen in SuperBook. Users can also use the selection to cut and paste information from one window into another, for example, copy a word from a display of thesaurus terms and paste the word into the query specification form. When arranging information within windows, the designer must choose between a monolithic display, in which all the windows are laid out in predefined positions and are all simultaneously viewable, tiled windows, and overlapping windows. User studies have been conducted comparing these options when applied to various tasks [725, 96]. Usually the results of these studies depend on the domain in which the interface is used, and no clear guidelines have yet emerged for information access interfaces. The monolithic interface has several advantages. It allows the designer to control the organization of the various options, makes all the information simultaneously viewable, and places the features in familiar positions, making them easier to find. But monolithic interfaces have disadvantages as well. They often work best if occupying the full viewing screen, and the number of views is inherently limited by the amount ofroom available on the screen (as opposed to overlapping windows which allow display of more information than can fit on the screen at once). Many modern work-intensive applications adopt a monolithic design, but this can hamper the integration of information access with other work processes such as text editing and data analysis. Plaisant et al. [644] discuss issues relating to coordinating information across different windows to providing overview plus details. A problem for any information access interface is an inherent limit in how many kinds of information can be shown at once. Information access systems must always reserve room for a text display area, and this must take up a significant proportion of screen space in order for the text to be legible. A tool within a paint program, for example, can be made quite small while nevertheless remaining recognizable and usable. For legibility reasons, it is difficult to compress many of the information displays needed for an information access system (such    312 USER INTERFACES AND VISUALIZATION as lists of thesaurus terms, query specifications, and lists of saved titles) in this manner. Good layout, graphics, and font design can improve the situation; for example, Web search results can look radically different depending on spacing, font, and other small touches [580]. Overlapping windows provide flexibility in arrangement, but can quickly lead to a crowded, disorganized display. Researchers have observed that much user activity is characterized by movement from one set of functionally related windows to another. Bannon et at. [54] define the notion of a workspace \the grouping together of sets of windows known to be functionally related to some activity or goal - arguing that this kind of organization more closely matches users' goal structure than individual windows [96]. Card et al: [140] also found that window usage could be categorized according to a 'working set' model. They looked at the relationship between the demands of the task and the number of windows in use, and found the largest number of individual windows were in use when users transitioned from one task to another. Based on these and other observations, Henderson and Card [420] built a system intended to make it easier for users to move between 'multiple virtual workspaces' [96]. The system uses a 3D spatial metaphor, where each workspace is a 'room,' and users transition between workspaces by 'moving' through virtual doors. By 'traveling' from one room to the next, users can change from one work context to another. In each work context, the application programs and data files that are associated with that work context are visible and readily available for reopening and perusal. The workspace notion as developed by Card et al. also emphasizes the importance of having sessions persist across time. The user should be able to leave a room dedicated to some task, work on another task, and three days later return to the first room and see all of the applications still in the same state as before. This notion of bundling applications and data together for each task has since been widely adopted by window manager software in workstation operating system interfaces. Elastic windows [428] is an extension to the workspace or rooms notion to the organization of 2D tiled windows. The main idea is to make the transition easier from one role or task to another, by adjusting how much of the screen real estate is consumed by the current role. The user can enlarge an entire group of windows with a simple gesture, and this resizing automatically causes the rest of the workspaces to reduce in size so they all still fit on the screen without overlap.  ",
2256,mir,mir-2256,10.8.3 Example Systems," 10.8.3 Example Systems The following sections describe the information layout and management approaches taken by several modern information access interfaces. The InfoGrid Layout The InfoGrid system [667] is a typical example of a monolithic layout for an information access interface. The layout assumes a large display is available    INTERFACE SUPPORT FOR THE SEARCH PROCESS 313 Search Parameters Property Sheet I Thumbnail Document ] Images Text c 0 U Holding Area Search Paths Control Panel TOC Subset Table of Contents Document Text Search Parameters Figure 10.28 Diagrams of monolithic layouts for information access interfaces. and is divided into a left-hand and right-hand side (see Figure 10.28). The lefthand side is further subdivided into an area at the top that contains structured entry forms for specifying the properties of a query, a column of iconic controls lining the left side, and an area for retaining documents of interest along the bottom. The main central area is used for the viewing of retrieval results, either as thumbnail representations of the original documents, or derived organizations of the documents, such as Scatter/Gather-style cluster results. Users can select documents from this area and store them in the holding area below or view them in the right-hand side. Most of the right-hand side of the display is used for viewing selected documents, with the upper portion showing metadata associated with the selected document. The area below the document display is intended to show a graphical history of earlier interactions. Designers must make decisions about which kinds of information to show in the primary view(s). If InfoGrid were used on a smaller display, either the document viewing area or the retrieval results viewing area would probably have to be shown via a pop-up overlapping window; otherwise the user would have to toggle between the two views. If the system were to suggest terms for relevance feedback, one of the existing views would have to be supplanted with this information or a pop-up window would have to be used to display the candidate terms. The system does not provide detailed information for source selection, although this could be achieved in a very simple way with a pop-up menu of choices from the control panel. The SuperBook Layout The layout of the InfoGrid is quite similar to that of SuperBook (see section 10.6). The main difference is that SuperBook retains the table of contents-like display in the main left-hand pane, along with indicators of how many documents containing search hits occur in each level of the outline. Like InfoGrid, the main pane of the right-hand side is used to display selected documents. Query    314 USER INTERFACES AND VISUALIZATION formulation is done just below the table of contents view (although in earlier versions this appeared in a separate window). Terms related to the user's query are shown in this window as well. Large images appear in pop-up overlapping windows. . The SuperBook layout is the result of several cycles of iterative design [481]. Earlier versions used overlapping windows instead of a monolithic layout, allowing users to sweep out a rectangular area on the screen in order to create a new text box. This new text box had its own set of buttons that allowed users to jump to occurrences of highlighted words in other documents or to the table of contents. SuperBook was redesigned after noting results of experimental studies [350, 532] showing that users can be more efficient if given fewer, well chosen interaction paths, rather than allowing wide latitude (A recent study of auditory interfaces found that although users were more efficient with a more flexible interface, they nevertheless preferred the more rigid, predictable interface [801]). The designers also took careful note of log files of user interactions. Before the redesign, users had to choose to view the overall frequency of a hit, move the mouse to the table of contents window, click the button and wait for the results to be updated. Since this pattern was observed to occur quite frequently, in the next version of the interface, the system was redesigned to automatically perform this sequence of actions immediately after a search was run. The SuperBook designers also attempted a redesign to allow the interface to fit into smaller displays. The redesign made use of small, overlapping windows. Some of the interaction sequences that were found useful in this more constrained environment were integrated into later designs for large monolithic displays. The CLiTE Interface The DLITE system [193, 192] makes a number of interesting design choices. It splits functionality into two parts: control of the search process and display of results . The control portion is a graphical direct manipulation display with animation (see FigurelO.29). Queries, sources, documents, and groups of retrieved documents are represented as graphical objects. The user creates a query by filling out the editable fields within a query constructor object. The system manufactures a query object, which is represented by a small icon which can be dragged and dropped onto iconic representations of collections or search services. If a service is active, it responds by creating an empty results set object and attaching the query to this. A set of retrieval results is represented as a circular pool, and documents within the result set are represented as icons distributed along the perimeter of the pool. Documents can be dragged out of the results set pool and dropped into other services, such as a document summarizer or a language translator. Meanwhile, the user can make a copy of the query icon and drop it onto another search service. Placing the mouse over the iconic representation of the query causes a 'tool-tips' window to pop up to show the contents of the underlying query. Queries can be stored and reused at a later time, thus facilitating retention of previously successful search strategies.    INTERFACE SUPPORT FOR THE SEARCH PROCESS 315 Figure 10.29 The DLITE interface [193]. A flexible interface architecture frees the user from the restriction of a rigid order of commands. On the other hand, as seen in the Super Book discussion, such an architecture must provide guidelines, to help get the user started, give hints about valid ways to proceed, and prevent the user from making errors. The graphical portion of the DLITE interface makes liberal use of animation to help guide the user. For example, if the user attempts to drop a query in the document summarizer icon - an illegal operation \rather than failing and giving the user an accusatory error message [185], the system takes control of the object being dropped, refusing to let it be placed on the representation for the target application, and moves the object left, right, and left again, mimicking a 'shake-thehead-no' gesture. Animation is also used to help the user understand the state of the system, for example, in showing the progress of the retrieval of search results by moving the result set object away from the service from which it was invoked. DLITE uses a separate Web browser window for the display of detailed information about the retrieved documents, such as their bibliographic citations and their full text. The browser window is also used to show Scatter/Gatherstyle cluster results and to allow users to select documents for relevance feedback. Earlier designs of the system attempted to incorporate text display into the direct manipulation portion, but this was found to be infeasible because of the space required [192]. Thus, DLITE separates the control portion of the information access process from the scanning and reading portion. This separation allows for reusable query construction and service selection, while at the same time allowing for a legible view of documents and relationships among retrieved documents. The selection in the display view is linked to the graphical control portion, so a document viewed in the display could be used as part of a query in a query constructor.    316 USER INTERFACES AND VISUALIZATION DLITE also incorporates the notion of a workspace, or 'workcenter,' as it is known in this system. Different workspaces are created for different kinds of tasks. For example, a workspace for buying computer software can be equipped with source icons representing good sources of reviews of computer software, good Web sites to search for price information and link to the user's online credit service. The SketchTrieve Interface The guiding principle behind the Sketch'Irieve interface (365) is the depiction of information access as an informal process, in which half-finished ideas and partly explored paths can be retained for later use, saved and brought back to compare to later interactions, and the results can be combined via operations on graphical objects and connectors between them. It has been observed [584, 722] that users use the physical layout of information within a spreadsheet to organize information. This idea motivates the design of SketchTrieve, which allows users to arrange retrieval results in a side-by-side manner to facilitate comparison and recombination (see Figure 10.30). The notion of a canvas or workspace for the retention of the previous context should be adopted more widely in future. Many issues are not easily solved, such as how to show the results of a set of interrelated queries, with minor modifications based on query expansion, relevance feedback, and other forms of modification. One idea is to show sets of related retrieval results as a stack of JOhn a SIftdh Dena Kay Sl'WIh EUI,n i&lt;w lQOl; ~ .. r lil!lliiitfAl.4i .... "" .... 3 W_Zotl..v ...Figure 10.30 The SketchTrieve interface [365].    INTERFACE SUPPORT FOR THE SEARCH PROCESS 317 cards within a folder and allow the user to extract subsets of the cards and view them side by side, as is done in SketchTrieve, or compare them via a difference operation.  ",
2257,mir,mir-2257,10.8.4 Examples of Poor Use of Overlapping Windows," 10.8.4 Examples of Poor Use of Overlapping Windows Sometimes conversion from a command-line-based interface to a graphical display can cause problems. Hancock-Beaulieu et ol. [337] describe poor design decisions made in an overlapping windows display for a bibliographic system. (An improvement was found with a later redesign of the system that used a monolithic interface [336].) Problems can also occur when designers make a 'literal' transformation from a TTY interface to a graphical interface. The consequences can be seen in the current LEXIS-NEXIS interface, which does not make use of the fact that window systems allow the user to view different kinds of information simultaneously. Instead, despite the fact that it occupies the entire screen, the interface does not retain window context when the user switches from one function to another. For example, viewing a small amount of metadata about a list of retrieved titles causes the list of results to disappear, rather than overlaying the information with a pop-up window or rearranging the available space with resizable tiles. Furthermore, this metadata is rendered in poorly-formatted ASCII instead of using the bit-map capabilities of a graphical interface. When a user opts to see the full text view of a document, it is shown in a small space, a few paragraphs at a time, instead of expanding to fill the entire available space. 10.8.5 Retaining Search History Section 10.3 discusses information seeking strategies and behaviors that have been observed by researchers in the field. This discussion suggests that the user interface should show what  ",
2258,mir,mir-2258,10.8.5 Retaining Search History," 10.8.5 Retaining Search History Section 10.3 discusses information seeking strategies and behaviors that have been observed by researchers in the field. This discussion suggests that the user interface should show what the available choices are at any given point, as well as what moves have been made in the past, short-term tactics as well as longerterm strategies, and allow the user to annotate the choices made and information found along the way. Users should be able to bundle search sessions as well as save individual portions of a given search session, and flexibly access and modify each. There is also increasing interest in incorporating personal preference and usage information both into formulation of queries and use of the results of search [277]. For the most part these strategies are not supported well in current user interfaces; however some mechanisms have been introduced that begin to address these needs. In particular, mechanisms to retain prior history of the search are useful for these tasks. Some kind of history mechanism has been made available in most search systems in the past. Usually these consist of a list of the commands executed earlier. More recently, graphical history has been introduced, that allows tracking of commands and results as well. Kim and Hirtle    318 USER INTERFACES AND VISUALIZATION Figure 10.31 The VISAGE interaction history visualization [685]. [440] present a summary of graphical history presentation mechanisms. Recently, a graphical interface that displays Web page access history in a hierarchical structure was found to require fewer page accesses and require less time when returning to pages already visited [370]. An innovation of particular interest for information access interfaces is exemplified by the saving of state in miniature form in a 'slide sorter' view as exercised by the VISAGE system for information visualization [685] (see Figure 10.31). The VISAGE application has the added advantage of being visual in nature and so individual states are easier to recognize. Although intended to be used as a presentation creation facility, this interface should also be useful for retaining search action history. 10.8.6 Integrating Scanning. Selection, and Querying User interfaces for information access in general do not do a good job of supporting strategies, or even of sequences of movements from one operation to the next. Even something as simple as taking the output of retrieval results from one query and using them as input to another query executed in a later search session is not well supported in most interfaces. Hertzum and Frokjaer [368] found that users preferred an integration of scanning and query specification in their user interfaces. They did not, however, observe better results with such interactions. They hypothesized that if interactions are too unrestricted this can lead to erroneous or wasteful behavior, and interaction between two different modes requires more guidance. This suggests that more flexibility is needed, but within constraints (this argument was also made in the discussion of the SuperBook system in section 10.6). There are exceptions. The new Web version of the Melyvl system provides ways to take the output of one query and modify it later for re-execution (see Figure 10.32). The workspace-based systems such as DLITE and Rooms allow storage and reuse of previous state. However, these systems do not integrate the general search process well with scanning and selection of information from auxiliary structures. Scanning, selection, and querying needs to be better integrated in general. This discussion will conclude with an example of an interface that does attempt to tightly couple querying and browsing.  ",
2259,mir,mir-2259,"10.8.6 Integrating Scanning, Selection, and Querying","  318 USER INTERFACES AND VISUALIZATION Figure 10.31 The VISAGE interaction history visualization [685]. [440] present a summary of graphical history presentation mechanisms. Recently, a graphical interface that displays Web page access history in a hierarchical structure was found to require fewer page accesses and require less time when returning to pages already visited [370]. An innovation of particular interest for information access interfaces is exemplified by the saving of state in miniature form in a 'slide sorter' view as exercised by the VISAGE system for information visualization [685] (see Figure 10.31). The VISAGE application has the added advantage of being visual in nature and so individual states are easier to recognize. Although intended to be used as a presentation creation facility, this interface should also be useful for retaining search action history. 10.8.6 Integrating Scanning. Selection, and Querying User interfaces for information access in general do not do a good job of supporting strategies, or even of sequences of movements from one operation to the next. Even something as simple as taking the output of retrieval results from one query and using them as input to another query executed in a later search session is not well supported in most interfaces. Hertzum and Frokjaer [368] found that users preferred an integration of scanning and query specification in their user interfaces. They did not, however, observe better results with such interactions. They hypothesized that if interactions are too unrestricted this can lead to erroneous or wasteful behavior, and interaction between two different modes requires more guidance. This suggests that more flexibility is needed, but within constraints (this argument was also made in the discussion of the SuperBook system in section 10.6). There are exceptions. The new Web version of the Melyvl system provides ways to take the output of one query and modify it later for re-execution (see Figure 10.32). The workspace-based systems such as DLITE and Rooms allow storage and reuse of previous state. However, these systems do not integrate the general search process well with scanning and selection of information from auxiliary structures. Scanning, selection, and querying needs to be better integrated in general. This discussion will conclude with an example of an interface that does attempt to tightly couple querying and browsing.    INTERFACE SUPPORT FOR THE SEARCH PROCESS 319 ·~j·n'prMTmrna~J 17 ""nzg II Figure 10.32 A view of query history revision in the Web-based version of the Melvyl bibliographic catalog. Copyright @, The Regents of the University of California. The Cat-a-Cone interface integrates querying and browsing of very large category hierarchies with their associated text collections. The prototype system uses 3D+animation interface components from the Information Visualizer [144J, applied in a novel way, to support browsing and search of text collections and their category hierarchies. See Figure 10.33. A key component of the interface is the separation of the graphical representation of the category hierarchy from the graphical representation of the documents. This separation allows for a fluid, flexible interaction between browsing and search, and between categories and documents. It also provides a mechanism by which a set of categories associated with a document can be viewed along with their hierarchical context. Another key component of the design is assignment of first-class status to the representation of text content. The retrieved documents are stored in a 3D+animation book representation [144J that allows for compact display of moderate numbers of documents. Associated with each retrieved document is a page of links to the category hierarchy and a page of text showing the document contents. The user can 'ruffle' the pages of the book of retrieval results and see corresponding changes in the category hierarchy, which is also represented in 3D+animation. All and only those parts of the category space that reflect the semantics of the retrieved document are shown with the document. The system allows for several different kinds of starting points. Users can start by typing in a name of a category and seeing which parts of the category hierarchy match it. For example, Figure 10.34 shows the results of searching on    32 0 USE R I N T E RF A C E S AND VISUALIZATION Figur e 10.33 The Cat-a-Cone interface for integrating category and text scanning and search [358 J. 'Radiation ' over the MeSH terms in this subcollection. Th e word appears unde r four main headings (Physical Sciences , D is ease s , Diagnostics , and B i o lo gi cal Sciences) . The hie rarchy immediately shows why ' Ra d ia t ion' appears under Diseases \- as part of a subtree on occupational hazard s . Now the user can select one or more of these category labels as input to a qu er y specification. Another way the user can start is by simply typing in a free text query into an entry label. Th is query is matched against the collection. Relevant documents are retrieved and placed in the book format . When the user ' o p e ns ' the book to a retrieved document , the parts of the category hierarchy that correspond to the retrieved documents are shown in the hierarchical representation . Thus , multiple intersecting categories can be shown simultaneously , in their hierarchical context. T h us, t his interface fluidly combines large, complex metadata , starting points, sca nni ng , and querying into one inte rface. The interface allows for a kind of relevance f eed b a c k , by suggesting additional categories that are related to the docume nts that have been retrieved . This interaction model is similar to that pr oposed by [5 ] . R ecall t he eval uat ion of t he Kohonen feature map representation disc ussed in se ct i on 10.4. Th e experimenters found that some users expressed a desi re fo r a v isible hie rarchical organization , others wanted an ability to zoom in on a subarea to get more detail , and some users disliked having to look t h r o ug h t he e ntire map to find a theme , desiring an alphabetical ordering instead. The s u b jects liked the ease of being able to jum p from one a rea to another without    TRENDS AND RESEARCH ISSUES 321 Fig u re 1 0\. 34 An int erf ace for a st ar t ing point for searching over catego ry la b els [358 J . having to back up (as is required in Yahoo! ) and lik ed the fact that the maps hav e varying levels of granularit y. These results all s u p p o r t the d esign decision s m ade in t he Cat -a-Con e. Hi erarchical repre sentation of term meanings is suppor ted , so users can choose which level of des crip tion is meaningful t o t h e m . Fur th ermor e , different levels of descrip tion can be viewed s im u lt a n eo us ly, so more fam iliar concep ts can be viewed in more detail , and less familiar a t a more general level. An alphabetical ordering of th e categories coupled with a regular ex p r ess ion s earch mechanism allows for st r a ight for w a r d loca tion of ca te go ry lab els. Retrieved do cumen t s ar e represented as first-class objects , so full text is visible , but in a compact form. Catego ry labels ar e dis ambiguated by the ir an ces t or /descendant /sibling repr esen t a t io n . Users can jump easily from one categor y t o another and can in addition quer y on multiple categories simultaneousl y (so me t h in g that is not a natural featur e of t h e maps ). The Cat-a-Cone has several addi tional advantages as well, such as allowing a document to be placed a t the intersection of several categories , and explicitly linking document contents with the category representation.  ",
2260,mir,mir-2260,10.9 Trends and Research Issues," 10.9 Trends and Research Issues The importance of human computer interaction is receiving increasing recognition with in the field of computer science [ 58 7]. As should be evident from the    322 USER INTERFACES AND VISUALIZATION contents of this chapter, the role of the user interface in the information access process has only recently begun to receive the attention it deserves. Research in this area can be expected to increase rapidly, primarily because of the rise of the Web. The Web has suddenly made vast quantities of information available globally, leading to an increase in interest in the problem of information access. This has lead to the creation of new information access paradigms, such as the innovative use of relevance feedback seen in the Amazon.com interface. Because the Web provides a platform-independent user interface, investment in better user interface design can have an impact on a larger user population than before. Another trend that can be anticipated is an amplified interest in organization and search over personal information collections. Many researchers are proposing that in future a person's entire life will be recorded using various media, from birth to death. One motivation for this scenario is to enable searching over everything a person has ever read or written. Another motivation is to allow for searching using contextual clues, such as 'find the article I was reading in the meeting I had on May 1st with Pam and Hal'. If this idea is pursued, it will require new, more sophisticated interfaces for searching and organizing a huge collection of personal information. There is also increasing interest in leveraging the behavior of individuals and groups, both for rating and assessing the quality of information items, and for suggesting starting points for search within information spaces. Recommender systems can be expected to increase in prevalence and diversity. User interfaces will be needed to guide users to appropriate recommended items based on their information needs. The field of information visualization needs some new ideas about how to display large, abstract information spaces intuitively. Until this happens, the role of visualization in information access will probably be primarily confined to providing thematic overviews of topic collections and displaying large category hierarchies dynamically. Breakthroughs in information visualization can be expected to have a strong impact on information access systems.  ",
2177,iir,iir-2177,12 Language models for information retrieval," 12 Language models for information retrieval A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. This approach thus provides a different realization of some of the basic ideas for document ranking which we saw in Section 6.2 (page 117). Instead of overtly modeling the probability P(R=1|q,d) of relevance of a document dto a query q, as in the traditional probabilistic approach to IR (Chapter 11), the basic language modeling approach instead builds a probabilistic language model Mdfrom each document d, and ranks documents based on the probability of the model generating the query: P(q|Md). In this chapter, we first introduce the concept of language models (Section 12.1) and then describe the basic and most commonly used language modeling approach to IR, the Query Likelihood Model (Section 12.2). After some comparisons between the language modeling approach and other approaches to IR (Section 12.3), we finish by briefly describing various extensions to the language modeling approach (Section 12.4). 12.1 Language models 12.1.1 Finite automata and language models What do we mean by a document model generatin ",12.1
2178,iir,iir-2178,12.1 Language models," 12.1 Language models 12.1.1 Finite automata and language models What do we mean by a document model generating a query? A traditional generative model of a language, of the kind familiar from formal language theory, can be used either to recognize or to generate strings. For example, the finite automaton shown in Figure 12.1 can generate strings that include the examples shown. The full set of strings that can be generated is called the language of the automaton.1 LANGUAGE  ",12.1
2179,iir,iir-2179,12.1.1 Finite automata and language models," 12.1.1 Finite automata and language models What do we mean by a document model generating a query? A traditional generative model of a language, of the kind familiar from formal language theory, can be used either to recognize or to generate strings. For example, the finite automaton shown in Figure 12.1 can generate strings that include the examples shown. The full set of strings that can be generated is called the language of the automaton.1 LANGUAGE      238 12 Language models for information retrieval  A simple finite automaton and some of the strings in the language it generates. →shows the start state of the automaton and a double circle indicates a (possible) finishing state. q1 P(STOP|q1) = 0.2 the 0.2 a 0.1 frog 0.01 toad 0.01 said 0.03 likes 0.02 that 0.04 ... ... ◮Figure 12.2 A one-state finite automaton that acts as a unigram language model. We show a partial specification of the state emission probabilities. If instead each node has a probability distribution over generating different terms, we have a language model. The notion of a language model is inherently probabilistic. A language model is a function that puts a probability measure over strings drawn from some vocabulary. That is, for a language model Mover an alphabet Σ: ∑ s∈Σ∗ P(s) = 1 (12.1) One simple kind of language model is equivalent to a probabilistic finite automaton consisting of just a single node with a single probability distribution over producing different terms, so that ∑t∈VP(t) = 1, as shown in Figure 12.2. After generating each word, we decide whether to stop or to loop around and then produce another word, and so the model also requires a probability of stopping in the finishing state. Such a model places a probability distribution over any sequence of words. By construction, it also provides a model for generating text according to its distribution. 1. Finite automata can have outputs attached to either their states or their arcs; we use states here, because that maps directly on to the way probabilistic automata are usually formalized.      12.1 Language models 239 Model M1Model M2 the 0.2 the 0.15 a 0.1 a 0.12 frog 0.01 frog 0.0002 toad 0.01 toad 0.0001 said 0.03 said 0.03 likes 0.02 likes 0.04 that 0.04 that 0.04 dog 0.005 dog 0.01 cat 0.003 cat 0.015 monkey 0.001 monkey 0.002 ... ... ... ... ◮Figure 12.3 Partial specification of two unigram language models. ✎Example 12.1: To find the probability of a word sequence, we just multiply the probabilities which the model gives to each word in the sequence, together with the probability of continuing or stopping after producing each word. For example, P(frog said that toad likes frog) = (0.01 ×0.03 ×0.04 ×0.01 ×0.02 ×0.01) (12.2) ×(0.8 ×0.8 ×0.8 ×0.8 ×0.8 ×0.8 ×0.2) ≈0.000000000001573 As you can see, the probability of a particular string/document, is usually a very small number! Here we stopped after generating frog the second time. The first line of numbers are the term emission probabilities, and the second line gives the probability of continuing or stopping after generating each word. An explicit stop probability is needed for a finite automaton to be a well-formed language model according to Equation (12.1). Nevertheless, most of the time, we will omit to include STOP and (1−STOP)probabilities (as do most other authors). To compare two models for a data set, we can calculate their likelihood ratio, which results from simply dividing the probability of the data according to one model by the probability of the data according to the other model. Providing that the stop probability is fixed, its inclusion will not alter the likelihood ratio that results from comparing the likelihood of two language models generating a string. Hence, it will not alter the ranking of documents.2 Nevertheless, formally, the numbers will no longer truly be probabilities, but only proportional to probabilities. See Exercise 12.4. ✎Example 12.2: Suppose, now, that we have two language models M1and M2, shown partially in Figure 12.3. Each gives a probability estimate to a sequence of 2. In the IR context that we are leading up to, taking the stop probability to be fixed across models seems reasonable. This is because we are generating queries, and the length distribution of queries is fixed and independent of the document from which we are generating the language model.     240 12 Language models for information retrieval terms, as already illustrated in Example 12.1. The language model that gives the higher probability to the sequence of terms is more likely to have generated the term sequence. This time, we will omit STOP probabilities from our calculations. For the sequence shown, we get: (12.3) sfrog said that toad likes that dog M10.01 0.03 0.04 0.01 0.02 0.04 0.005 M20.0002 0.03 0.04 0.0001 0.04 0.04 0.01 P(s|M1) = 0.00000000000048 P(s|M2) = 0.000000000000000384 and we see that P(s|M1)&gt;P(s|M2). We present the formulas here in terms of products of probabilities, but, as is common in probabilistic applications, in practice it is usually best to work with sums of log probabilities (cf. page 258).  ",12.1
2180,iir,iir-2180,12.1.2 Types of language models," 12.1.2 Types of language models How do we build probabilities over sequences of terms? We can always use the chain rule from Equation (11.1) to decompose the probability of a sequence of events into the probability of each successive event conditioned on earlier events: P(t1t2t3t4) = P(t1)P(t2|t1)P(t3|t1t2)P(t4|t1t2t3) (12.4) The simplest form of language model simply throws away all conditioning context, and estimates each term independently. Such a model is called a unigram language model : Puni(t1t2t3t4) = P(t1)P(t2)P(t3)P(t4) (12.5) There are many more complex kinds of language models, such as bigram language models, which condition on the previous term, Pbi(t1t2t3t4) = P(t1)P(t2|t1)P(t3|t2)P(t4|t3) (12.6) and even more complex grammar-based language models such as probabilistic context-free grammars. Such models are vital for tasks like speech recognition, spelling correction, and machine translation, where you need the probability of a term conditioned on surrounding context. However, most language-modeling work in IR has used unigram language models. IR is not the place where you most immediately need complex language models, since IR does not directly depend on the structure of sentences to the extent that other tasks like speech recognition do. Unigram models are often sufficient to judge the topic of a text. Moreover, as we shall see, IR language models are frequently estimated from a single document and so it is      12.1 Language models 241 questionable whether there is enough training data to do more. Losses from data sparseness (see the discussion on page 260) tend to outweigh any gains from richer models. This is an example of the bias-variance tradeoff (cf. Section 14.6, page 308): With limited training data, a more constrained model tends to perform better. In addition, unigram models are more efficient to estimate and apply than higher-order models. Nevertheless, the importance of phrase and proximity queries in IR in general suggests that future work should make use of more sophisticated language models, and some has begun to (see Section 12.5, page 252). Indeed, making this move parallels the model of van Rijsbergen in Chapter 11 (page 231).  ",12.1
2181,iir,iir-2181,12.1.3 Multinomial distributions over words," 12.1.3 Multinomial distributions over words Under the unigram language model the order of words is irrelevant, and so such models are often called “bag of words” models, as discussed in Chapter 6(page 117). Even though there is no conditioning on preceding context, this model nevertheless still gives the probability of a particular ordering of terms. However, any other ordering of this bag of terms will have the same probability. So, really, we have a multinomial distribution over words. So long as we stick to unigram models, the language model name and motivation could be viewed as historical rather than necessary. We could instead just refer to the model as a multinomial model. From this perspective, the equations presented above do not present the multinomial probability of a bag of words, since they do not sum over all possible orderings of those words, as is done by the multinomial coefficient (the first term on the right-hand side) in the standard presentation of a multinomial model: P(d) = Ld! tft1,d!tft2,d!···tftM,d!P(t1)tft1,dP(t2)tft2,d···P(tM)tftM,d (12.7) Here, Ld=∑1≤i≤Mtfti,dis the length of document d,Mis the size of the term vocabulary, and the products are now over the terms in the vocabulary, not the positions in the document. However, just as with STOP probabilities, in practice we can also leave out the multinomial coefficient in our calculations, since, for a particular bag of words, it will be a constant, and so it has no effect on the likelihood ratio of two different models generating a particular bag of words. Multinomial distributions also appear in Section 13.2 (page 258). The fundamental problem in designing language models is that we do not know what exactly we should use as the model Md. However, we do generally have a sample of text that is representative of that model. This problem makes a lot of sense in the original, primary uses of language models. For example, in speech recognition, we have a training sample of (spoken) text. But we have to expect that, in the future, users will use different words and in     242 12 Language models for information retrieval different sequences, which we have never observed before, and so the model has to generalize beyond the observed data to allow unknown words and sequences. This interpretation is not so clear in the IR case, where a document is finite and usually fixed. The strategy we adopt in IR is as follows. We pretend that the document dis only a representative sample of text drawn from a model distribution, treating it like a fine-grained topic. We then estimate a language model from this sample, and use that model to calculate the probability of observing any word sequence, and, finally, we rank documents according to their probability of generating the query. ?Exercise 12.1 [⋆] Including stop probabilities in the calculation, what will the sum of the probability estimates of all strings in the language of length 1 be? Assume that you generate a word and then decide whether to stop or not (i.e., the null string is not part of the language). Exercise 12.2 [⋆] If the stop probability is omitted from calculations, what will the sum of the scores assigned to strings in the language of length 1 be? Exercise 12.3 [⋆] What is the likelihood ratio of the document according to M1and M2in Example 12.2? Exercise 12.4 [⋆] No explicit STOP probability appeared in Example 12.2. Assuming that the STOP probability of each model is 0.1, does this change the likelihood ratio of a document according to the two models? Exercise 12.5 [⋆⋆] How might a language model be used in a spelling correction system? In particular, consider the case of context-sensitive spelling correction, and correcting incorrect usages of words, such as their in Are you their? (See Section 3.5 (page 65) for pointers to some literature on this topic.)  ",12.1
2182,iir,iir-2182,12.2 The query likelihood model," 12.2 The query likelihood model 12.2.1 Using query likelihood language models in IR Language modeling is a quite general formal approach to IR, with many variant realizations. The original and basic method for using language models in IR is the query likelihood model. In it, we construct from each document  in the collection a language model Md. Our goal is to rank documents by P(d|q), where the probability of a document is interpreted as the likelihood that it is relevant to the query. Using Bayes rule (as introduced in Section 11.1, page 220), we have: P(d|q) = P(q|d)P(d)/P(q)  ",12.2
2183,iir,iir-2183,12.2.1 Using query likelihood language models in IR," 12.2.1 Using query likelihood language models in IR Language modeling is a quite general formal approach to IR, with many variant realizations. The original and basic method for using language models in IR is the query likelihood model. In it, we construct from each document  in the collection a language model Md. Our goal is to rank documents by P(d|q), where the probability of a document is interpreted as the likelihood that it is relevant to the query. Using Bayes rule (as introduced in Section 11.1, page 220), we have: P(d|q) = P(q|d)P(d)/P(q)      12.2 The query likelihood model 243 P(q)is the same for all documents, and so can be ignored. The prior probability of a document P(d)is often treated as uniform across all dand so it can also be ignored, but we could implement a genuine prior which could include criteria like authority, length, genre, newness, and number of previous people who have read the document. But, given these simplifications, we return results ranked by simply P(q|d), the probability of the query qunder the language model derived from d. The Language Modeling approach thus attempts to model the query generation process: Documents are ranked by the probability that a query would be observed as a random sample from the respective document model. The most common way to do this is using the multinomial unigram language model, which is equivalent to a multinomial Naive Bayes model (page 263), where the documents are the classes, each treated in the estimation as a separate “language”. Under this model, we have that: P(q|Md) = Kq∏ t∈V P(t|Md)tft,d (12.8) where, again Kq=Ld!/(tft1,d!tft2,d!···tftM,d!)is the multinomial coefficient for the query q, which we will henceforth ignore, since it is a constant for a particular query. For retrieval based on a language model (henceforth LM), we treat the generation of queries as a random process. The approach is to 1\. Infer a LM for each document. 2\. Estimate P(q|Mdi), the probability of generating the query according to each of these document models. 3\. Rank the documents according to these probabilities. The intuition of the basic model is that the user has a prototype document in mind, and generates a query based on words that appear in this document. Often, users have a reasonable idea of terms that are likely to occur in documents of interest and they will choose query terms that distinguish these documents from others in the collection.3Collection statistics are an integral part of the language model, rather than being used heuristically as in many other approaches.  ",12.2
2184,iir,iir-2184,12.2.2 Estimating the query generation probability," 12.2.2 Estimating the query generation probability In this section we describe how to estimate P(q|Md). The probability of producing the query given the LM Mdof document dusing maximum likelihood 3. Of course, in other cases, they do not. The answer to this within the language modeling approach is translation language models, as briefly discussed in Section 12.4.      244 12 Language models for information retrieval estimation (MLE) and the unigram assumption is: ˆ P(q|Md) = ∏ t∈q ˆ Pmle(t|Md) = ∏ t∈q tft,d Ld (12.9) where Mdis the language model of document d, tft,dis the (raw) term frequency of term tin document d, and Ldis the number of tokens in document d. That is, we just count up how often each word occurred, and divide through by the total number of words in the document d. This is the same method of calculating an MLE as we saw in Section 11.3.2 (page 226), but now using a multinomial over word counts. The classic problem with using language models is one of estimation (the ˆsymbol on the P’s is used above to stress that the model is estimated): terms appear very sparsely in documents. In particular, some words will not have appeared in the document at all, but are possible words for the information need, which the user may have used in the query. If we estimate ˆ P(t|Md) = 0 for a term missing from a document d, then we get a strict conjunctive semantics: documents will only give a query non-zero probability if all of the query terms appear in the document. Zero probabilities are clearly a problem in other uses of language models, such as when predicting the next word in a speech recognition application, because many words will be sparsely represented in the training data. It may seem rather less clear whether this is problematic in an IR application. This could be thought of as a human-computer interface issue: vector space systems have generally preferred more lenient matching, though recent web search developments have tended more in the direction of doing searches with such conjunctive semantics. Regardless of the approach here, there is a more general problem of estimation: occurring words are also badly estimated; in particular, the probability of words occurring once in the document is normally overestimated, since their one occurrence was partly by chance. The answer to this (as we saw in Section 11.3.2, page 226) is smoothing. But as people have come to understand the LM approach better, it has become apparent that the role of smoothing in this model is not only to avoid zero probabilities. The smoothing of terms actually implements major parts of the term weighting component (Exercise 12.8). It is not just that an unsmoothed model has conjunctive semantics; an unsmoothed model works badly because it lacks parts of the term weighting component. Thus, we need to smooth probabilities in our document language models: to discount non-zero probabilities and to give some probability mass to unseen words. There’s a wide space of approaches to smoothing probability distributions to deal with this problem. In Section 11.3.2 (page 226), we already discussed adding a number (1, 1/2, or a small α) to the observed      12.2 The query likelihood model 245 counts and renormalizing to give a probability distribution.4In this section we will mention a couple of other smoothing methods, which involve combining observed counts with a more general reference probability distribution. The general approach is that a non-occurring term should be possible in a query, but its probability should be somewhat close to but no more likely than would be expected by chance from the whole collection. That is, if tft,d=0 then ˆ P(t|Md)≤cft/T where cftis the raw count of the term in the collection, and Tis the raw size (number of tokens) of the entire collection. A simple idea that works well in practice is to use a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire collection: ˆ P(t|d) = λˆ Pmle(t|Md) + (1−λ)ˆ Pmle(t|Mc) (12.10) where 0 &lt;λ&lt;1 and Mcis a language model built from the entire document collection. This mixes the probability from the document with the general collection frequency of the word. Such a model is referred to as a linear interpolation language model.5Correctly setting λis important to the good performance of this model. An alternative is to use a language model built from the whole collection as a prior distribution in a Bayesian updating process (rather than a uniform distribution, as we saw in Section 11.3.2). We then get the following equation: ˆ P(t|d) = tft,d+αˆ P(t|Mc) Ld+α (12.11) Both of these smoothing methods have been shown to perform well in IR experiments; we will stick with the linear interpolation smoothing method for the rest of this section. While different in detail, they are both conceptually similar: in both cases the probability estimate for a word present in the document combines a discounted MLE and a fraction of the estimate of its prevalence in the whole collection, while for words not present in a document, the estimate is just a fraction of the estimate of the prevalence of the word in the whole collection. The role of smoothing in LMs for IR is not simply or principally to avoid estimation problems. This was not clear when the models were first proposed, but it is now understood that smoothing is essential to the good properties 4. In the context of probability theory, (re)normalization refers to summing numbers that cover an event space and dividing them through by their sum, so that the result is a probability distribution which sums to 1. This is distinct from both the concept of term normalization in Chapter 2 and the concept of length normalization in Chapter 6, which is done with a L2norm. 5. It is also referred to as Jelinek-Mercer smoothing.     246 12 Language models for information retrieval of the models. The reason for this is explored in Exercise 12.8. The extent of smoothing in these two models is controlled by the λand αparameters: a small value of λor a large value of αmeans more smoothing. This parameter can be tuned to optimize performance using a line search (or, for the linear interpolation model, by other methods, such as the expectation maximimization algorithm; see Section 16.5, page 368). The value need not be a constant. One approach is to make the value a function of the query size. This is useful because a small amount of smoothing (a “conjunctive-like” search) is more suitable for short queries, while a lot of smoothing is more suitable for long queries. To summarize, the retrieval ranking for a query qunder the basic LM for IR we have been considering is given by: P(d|q)∝P(d)∏ t∈q ((1−λ)P(t|Mc) + λP(t|Md)) (12.12) This equation captures the probability that the document that the user had in mind was in fact d. ✎Example 12.3: Suppose the document collection contains two documents: •d1: Xyzzy reports a profit but revenue is down •d2: Quorus narrows quarter loss but revenue decreases further The model will be MLE unigram models from the documents and collection, mixed with λ=1/2. Suppose the query is revenue down. Then: P(q|d1) = [(1/8 +2/16)/2]×[(1/8 +1/16)/2] (12.13) =1/8 ×3/32 =3/256 P(q|d2) = [(1/8 +2/16)/2]×[(0/8 +1/16)/2] =1/8 ×1/32 =1/256 So, the ranking is d1&gt;d2.  ",12.2
2185,iir,iir-2185,12.2.3 Ponte and Croft’s Experiments," 12.2.3 Ponte and Croft’s Experiments Ponte and Croft (1998) present the first experiments on the language modeling approach to information retrieval. Their basic approach is the model that we have presented until now. However, we have presented an approach where the language model is a mixture of two multinomials, much as in (Miller et al. 1999,Hiemstra 2000) rather than Ponte and Croft’s multivariate Bernoulli model. The use of multinomials has been standard in most subsequent work in the LM approach and experimental results in IR, as well as evidence from text classification which we consider in Section 13.3      12.2 The query likelihood model 247 Precision Rec. tf-idf LM %chg 0.0 0.7439 0.7590 +2.0 0.1 0.4521 0.4910 +8.6 0.2 0.3514 0.4045 +15.1 * 0.3 0.2761 0.3342 +21.0 * 0.4 0.2093 0.2572 +22.9 * 0.5 0.1558 0.2061 +32.3 * 0.6 0.1024 0.1405 +37.1 * 0.7 0.0451 0.0760 +68.7 * 0.8 0.0160 0.0432 +169.6 * 0.9 0.0033 0.0063 +89.3 1.0 0.0028 0.0050 +76.9 Ave 0.1868 0.2233 +19.55 * ◮Figure 12.4 Results of a comparison of tf-idf with language modeling (LM) term weighting by Ponte and Croft (1998). The version of tf-idf from the INQUERY IR system includes length normalization of tf. The table gives an evaluation according to 11-point average precision with significance marked with a * according to a Wilcoxon signed rank test. The language modeling approach always does better in these experiments, but note that where the approach shows significant gains is at higher levels of recall. (page 263), suggests that it is superior. Ponte and Croft argued strongly for the effectiveness of the term weights that come from the language modeling approach over traditional tf-idf weights. We present a subset of their results in Figure 12.4 where they compare tf-idf to language modeling by evaluating TREC topics 202–250 over TREC disks 2 and 3. The queries are sentencelength natural language queries. The language modeling approach yields significantly better results than their baseline tf-idf based term weighting approach. And indeed the gains shown here have been extended in subsequent work. ?Exercise 12.6 [⋆] Consider making a language model from the following training text: the martian has landed on the latin pop sensation ricky martin a. Under a MLE-estimated unigram probability model, what are P(the)and P(martian)? b. Under a MLE-estimated bigram model, what are P(sensation|pop)and P(pop|the)?      248 12 Language models for information retrieval Exercise 12.7 [⋆⋆] Suppose we have a collection that consists of the 4 documents given in the below table. docID Document text 1 click go the shears boys click click click 2 click click 3 metal here 4 metal shears click here Build a query likelihood language model for this document collection. Assume a mixture model between the documents and the collection, with both weighted at 0.5. Maximum likelihood estimation (mle) is used to estimate both as unigram models. Work out the model probabilities of the queries click,shears, and hence click shears for each document, and use those probabilities to rank the documents returned by each query. Fill in these probabilities in the below table: Query Doc 1 Doc 2 Doc 3 Doc 4 click shears click shears What is the final ranking of the documents for the query click shears? Exercise 12.8 [⋆⋆] Using the calculations in Exercise 12.7 as inspiration or as examples where appropriate, write one sentence each describing the treatment that the model in Equation (12.10) gives to each of the following quantities. Include whether it is present in the model or not and whether the effect is raw or scaled. a. Term frequency in a document b. Collection frequency of a term c. Document frequency of a term d. Length normalization of a term Exercise 12.9 [⋆⋆] In the mixture model approach to the query likelihood model (Equation (12.12)), the probability estimate of a term is based on the term frequency of a word in a document, and the collection frequency of the word. Doing this certainly guarantees that each term of a query (in the vocabulary) has a non-zero chance of being generated by each document. But it has a more subtle but important effect of implementing a form of term weighting, related to what we saw in Chapter 6. Explain how this works. In particular, include in your answer a concrete numeric example showing this term weighting at work.  ",12.2
2186,iir,iir-2186,12.3 Language modeling versus other approaches in IR," 12.3 Language modeling versus other approaches in IR The language modeling approach provides a novel way of looking at the problem of text retrieval, which links it with a lot of recent work in speech   12.3 Language modeling versus other approaches in IR 249 and language processing. As Ponte and Croft (1998) emphasize, the language modeling approach to IR provides a different approach to scoring matches between queries and documents, and the hope is that the probabilistic language modeling foundation improves the weights that are used, and hence the performance of the model. The major issue is estimation of the document model, such as choices of how to smooth it effectively. The model has achieved very good retrieval results. Compared to other probabilistic approaches, such as the BIM from Chapter 11, the main difference initially appears to be that the LM approach does away with explicitly modeling relevance (whereas this is the central variable evaluated in the BIM approach). But this may not be the correct way to think about things, as some of the papers in Section 12.5 further discuss. The LM approach assumes that documents and expressions of information needs are objects of the same type, and assesses their match by importing the tools and methods of language modeling from speech and natural language processing. The resulting model is mathematically precise, conceptually simple, computationally tractable, and intuitively appealing. This seems similar to the situation with XML retrieval (Chapter 10): there the approaches that assume queries and documents are objects of the same type are also among the most successful. On the other hand, like all IR models, you can also raise objections to the model. The assumption of equivalence between document and information need representation is unrealistic. Current LM approaches use very simple models of language, usually unigram models. Without an explicit notion of relevance, relevance feedback is difficult to integrate into the model, as are user preferences. It also seems necessary to move beyond a unigram model to accommodate notions of phrase or passage matching or Boolean retrieval operators. Subsequent work in the LM approach has looked at addressing some of these concerns, including putting relevance back into the model and allowing a language mismatch between the query language and the document language. The model has significant relations to traditional tf-idf models. Term frequency is directly represented in tf-idf models, and much recent work has recognized the importance of document length normalization. The effect of doing a mixture of document generation probability with collection generation probability is a little like idf: terms rare in the general collection but common in some documents will have a greater influence on the ranking of documents. In most concrete realizations, the models share treating terms as if they were independent. On the other hand, the intuitions are probabilistic rather than geometric, the mathematical models are more principled rather than heuristic, and the details of how statistics like term frequency and document length are used differ. If you are concerned mainly with performance numbers, recent work has shown the LM approach to be very effective in retrieval experiments, beating tf-idf and BM25 weights. Nevertheless, there is      250 12 Language models for information retrieval Query Query model P(t|Query) Document Doc. model P(t|Document) (a) (b) (c) ◮Figure 12.5 Three ways of developing the language modeling approach: (a) query likelihood, (b) document likelihood, and (c) model comparison. perhaps still insufficient evidence that its performance so greatly exceeds that of a well-tuned traditional vector space retrieval system as to justify changing an existing implementation.  ",12.3
2187,iir,iir-2187,12.4 Extended language modeling approaches," 12.4 Extended language modeling approaches In this section we briefly mention some of the work that extends the basic language modeling approach. There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work. Rather than looking at the probability of a document language model Mdgenerating the query, you can look at the probability of a query language model Mqgenerating the document. The main reason that doing things in this direction and creating a document likelihood model is less appealing is that there is much less   text available to estimate a language model based on the query text, and so the model will be worse estimated, and will have to depend more on being smoothed with some other language model. On the other hand, it is easy to see how to incorporate relevance feedback into such a model: you can expand the query with terms taken from relevant documents in the usual way and hence update the language model Mq(Zhai and Lafferty 2001a). Indeed, with appropriate modeling choices, this approach leads to the BIM model of Chapter 11. The relevance model of Lavrenko and Croft (2001) is an instance of a document likelihood model, which incorporates pseudo-relevance feedback into a language modeling approach. It achieves very strong empirical results. Rather than directly generating in either direction, we can make a language model from both the document and query, and then ask how different these two language models are from each other. Lafferty and Zhai (2001) lay   12.4 Extended language modeling approaches 251 out these three ways of thinking about the problem, which we show in Figure 12.5, and develop a general risk minimization approach for document retrieval. For instance, one way to model the risk of returning a document d as relevant to a query qis to use the Kullback-Leibler (KL) divergence between their respective language models: R(d;q) = KL(MdkMq) = ∑ t∈V P(t|Mq)log P(t|Mq) P(t|Md) (12.14) KL divergence is an asymmetric divergence measure originating in information theory, which measures how bad the probability distribution Mqis at modeling Md(Cover and Thomas 1991,Manning and Schütze 1999). Lafferty and Zhai (2001) present results suggesting that a model comparison approach outperforms both query-likelihood and document-likelihood approaches. One disadvantage of using KL divergence as a ranking function is that scores are not comparable across queries. This does not matter for ad hoc retrieval, but is important in other applications such as topic tracking. Kraaij and Spitters (2003) suggest an alternative proposal which models similarity as a normalized log-likelihood ratio (or, equivalently, as a difference between cross-entropies). Basic LMs do not address issues of alternate expression, that is, synonymy, or any deviation in use of language between queries and documents. Berger and Lafferty (1999) introduce translation models to bridge this query-document gap. A translation model lets you generate query words not in a document by translation to alternate terms with similar meaning. This also provides a basis for performing cross-language IR. We assume that the translation model can be represented by a conditional probability distribution T(·|·)between vocabulary terms. The form of the translation query generation model is then: P(q|Md) = ∏ t∈q ∑ v∈V P(v|Md)T(t|v) (12.15) The term P(v|Md)is the basic document language model, and the term T(t|v) performs translation. This model is clearly more computationally intensive and we need to build a translation model. The translation model is usually built using separate resources (such as a traditional thesaurus or bilingual dictionary or a statistical machine translation system’s translation dictionary), but can be built using the document collection if there are pieces of text that naturally paraphrase or summarize other pieces of text. Candidate examples are documents and their titles or abstracts, or documents and anchor-text pointing to them in a hypertext environment. Building extended LM approaches remains an active area of research. In general, translation models, relevance feedback models, and model compar     252 12 Language models for information retrieval ison approaches have all been demonstrated to improve performance over the basic query likelihood LM.  ",12.4
2188,iir,iir-2188,12.5 References and further reading," 12.5 References and further reading For more details on the basic concepts of probabilistic language models and techniques for smoothing, see either Manning and Schütze (1999, Chapter 6) or Jurafsky and Martin (2008, Chapter 4). The important initial papers that originated the language modeling approach to IR are: (Ponte and Croft 1998,Hiemstra 1998,Berger and Lafferty 1999,Miller et al. 1999). Other relevant papers can be found in the next several years of SIGIR proceedings. (Croft and Lafferty 2003) contains a collection of papers from a workshop on language modeling approaches and Hiemstra and Kraaij (2005) review one prominent thread of work on using language modeling approaches for TREC tasks. Zhai and Lafferty (2001b) clarify the role of smoothing in LMs for IR and present detailed empirical comparisons of different smoothing methods. Zaragoza et al. (2003) advocate using full Bayesian predictive distributions rather than MAP point estimates, but while they outperform Bayesian smoothing, they fail to outperform a linear interpolation. Zhai and Lafferty (2002) argue that a two-stage smoothing model with first Bayesian smoothing followed by linear interpolation gives a good model of the task, and performs better and more stably than a single form of smoothing. A nice feature of the LM approach is that it provides a convenient and principled way to put various kinds of prior information into the model; Kraaij et al. (2002) demonstrate this by showing the value of link information as a prior in improving web entry page retrieval performance. As briefly discussed in Chapter 16 (page 353), Liu and Croft (2004) show some gains by smoothing a document LM with estimates from a cluster of similar documents; Tao et al. (2006) report larger gains by doing document-similarity based smoothing. Hiemstra and Kraaij (2005) present TREC results showing a LM approach beating use of BM25 weights. Recent work has achieved some gains by going beyond the unigram model, providing the higher order models are smoothed with lower order models (Gao et al. 2004,Cao et al. 2005), though the gains to date remain modest. Spärck Jones (2004) presents a critical viewpoint on the rationale for the language modeling approach, but Lafferty and Zhai (2003) argue that a unified account can be given of the probabilistic semantics underlying both the language modeling approach presented in this chapter and the classical probabilistic information retrieval approach of Chapter 11. The Lemur Toolkit (http://www.lemurproject.org/) provides a flexible open source framework for investigating language modeling approaches to IR.  ",
2283,mir,mir-2283,13 Searching the Web, 13 Searching the Web 13.1 Intr ,
2310,iir,iir-2310,13 Text classification and Naive Bayes," 13 Text classification and Naive Bayes Thus far, this book has mainly discussed the process of ad hoc retrieval, where users have transient information needs that they try to address by posing one or more queries to a search engine. However, many users have ongoing information needs. For example, you might need to track developments in multicore computer chips. One way of doing this is to issue the query multicore AND computer AND chip against an index of recent newswire articles each morning. In this and the following two chapters we examine the question: How can this repetitive task be automated? To this end, many systems support standing queries. A standing query is like any other query except that it is periodically executed on a collection to which new documents are incrementally added over time. If your standing query is just multicore AND computer AND chip, you will tend to miss many relevant new articles which use other terms such as multicore processors. To achieve good recall, standing queries thus have to be refined over time and can gradually become quite complex. In this example, using a Boolean search engine with stemming, you might end up with a query like (multicore OR multi-core) AND (chip OR processor OR microprocessor). To capture the generality and scope of the problem space to which standing queries belong, we now introduce the general notion of a classificationCLASSIFICATION problem. Given a set of classes, we seek to determine which class(es) a given object belongs to. In the example, the standing query serves to divide new newswire articles into the two classes: documents about multicore computer chips and documents not about multicore computer chips. We refer to this as two- class classification. Classification using standing queries is also called routing orROUTING filteringand will be discussed further in Section 15.3.1 (page 335).FILTERING A class need not be as narrowly focused as the standing query multicore computer chips. Often, a class is a more general subject area like China or coffee. Such more general classes are usually referred to as topics, and the classification task is then called text classification,text categorization,topic classification,TEXT CLASSIFICATION or topic spotting. An example for China appears in Figure 13.1. Standing queries and topics differ in their degree of specificity, but the methods for   13 Text classification and Naive Bayes solving routing, filtering, and text classification are essentially the same. We therefore include routing and filtering under the rubric of text classification in this and the following chapters. The notion of classification is very general and has many applications within and beyond information retrieval (IR). For instance, in computer vision, a classifier may be used to divide images into classes such as landscape,portrait, and neither. We focus here on examples from information retrieval such as: •Several of the preprocessing steps necessary for indexing as discussed in Chapter 2: detecting a document’s encoding (ASCII, Unicode UTF-8 etc; page 20); word segmentation (Is the white space between two letters a word boundary or not? page 24 ) ; truecasing (page 30); and identifying the language of a document (page 46). •The automatic detection of spam pages (which then are not included in the search engine index). •The automatic detection of sexually explicit content (which is included in search results only if the user turns an option such as SafeSearch off). •Sentiment detection or the automatic classification of a movie or product review as positive or negative. An example application is a user searching for negative reviews before buying a camera to make sure it has no undesirable features or quality problems. •Personal email sorting. A user may have folders like talk announcements,EMAIL SORTING electronic bills,email from family and friends, and so on, and may want a classifier to classify each incoming email and automatically move it to the appropriate folder. It is easier to find messages in sorted folders than in a very large inbox. The most common case of this application is a spam folder that holds all suspected spam messages. •Topic-specific or vertical search. Vertical search engines restrict searches to a particular topic. For example, the query computer science on a vertical search engine for the topic China will return a list of Chinese computer science departments with higher precision and recall than the query computer science China on a general purpose search engine. This is because the vertical search engine does not include web pages in its index that contain the term china in a different sense (e.g., referring to a hard white ceramic), but does include relevant pages even if they do not explicitly mention the term China. •Finally, the ranking function in ad hoc information retrieval can also be based on a document classifier as we will explain in Section 15.4 (page 341).     255 This list shows the general importance of classification in IR. Most retrieval systems today contain multiple components that use some form of classifier. The classification task we will use as an example in this book is text classification. A computer is not essential for classification. Many classification tasks have traditionally been solved manually. Books in a library are assigned Library of Congress categories by a librarian. But manual classification is expensive to scale. The multicore computer chips example illustrates one alternative approach: classification by the use of standing queries – which can be thought of as rules – most commonly written by hand. As in our exam-RULES IN TEXT CLASSIFICATION ple (multicore OR multi-core) AND (chip OR processor OR microprocessor), rules are sometimes equivalent to Boolean expressions. A rule captures a certain combination of keywords that indicates a class. Hand-coded rules have good scaling properties, but creating and maintaining them over time is labor intensive. A technically skilled person (e.g., a domain expert who is good at writing regular expressions) can create rule sets that will rival or exceed the accuracy of the automatically generated classifiers we will discuss shortly; however, it can be hard to find someone with this specialized skill. Apart from manual classification and hand-crafted rules, there is a third approach to text classification, namely, machine learning-based text classification. It is the approach that we focus on in the next several chapters. In machine learning, the set of rules or, more generally, the decision criterion of the text classifier, is learned automatically from training data. This approach is also called statistical text classification if the learning method is statistical.STATISTICAL TEXT CLASSIFICATION In statistical text classification, we require a number of good example documents (or training documents) for each class. The need for manual classification is not eliminated because the training documents come from a person who has labeled them – where labeling refers to the process of annotatingLABELING each document with its class. But labeling is arguably an easier task than writing rules. Almost anybody can look at a document and decide whether or not it is related to China. Sometimes such labeling is already implicitly part of an existing workflow. For instance, you may go through the news articles returned by a standing query each morning and give relevance feedback (cf. Chapter 9) by moving the relevant articles to a special folder like multicore-processors. We begin this chapter with a general introduction to the text classification problem including a formal definition (Section 13.1); we then cover Naive Bayes, a particularly simple and effective classification method (Sections 13.2– 13.4). All of the classification algorithms we study represent documents in high-dimensional spaces. To improve the efficiency of these algorithms, it is generally desirable to reduce the dimensionality of these spaces; to this end, a technique known as feature selection is commonly applied in text clas   13 Text classification and Naive Bayes sification as discussed in Section 13.5. Section 13.6 covers evaluation of text classification. In the following chapters, Chapters 14 and 15, we look at two other families of classification methods, vector space classifiers and support vector machines. 13.1 The text classification problem In t ",13.1
2284,mir,mir-2284,13.1 Introduction," 13.1 Introduction The World Wide Web dates from the end of the 1980s [85] and no one could have imagined its current impact. The boom in the use of the Web and its exponential growth are now well known. Just the amount of textual data available is estimated to be in the order of one terabyte. In addition, other media, such as images, audio, and video, are also available. Thus, the Web can be seen as a very large, unstructured but ubiquitous database. This triggers the need for efficient tools to manage, retrieve, and filter information from this database. This problem is also becoming important in large intranets, where we want to extract or infer new information to support a decision process, a task called data mining. Ai3 mentioned in Chapter 1, we make the important distinction between data and information retrieval. We are interested in the latter case, in which the user searches for data that fulfills his information need. We focus on text, because although there are techniques to search for images and other non-textual data, they cannot be applied (yet) on a large scale. We also emphasize syntactic search. That is, we search for Web documents that have user-specified words or patterns in their text. Ai3 discussed in Chapter 2, such words or patterns mayor may not reflect the intrinsic semantics of the text. An alternative approach to syntactic search is to do a natural language analysis of the text. Although the techniques to preprocess natural language and extract the text semantics are not new, they are not yet very effective and they are also too costly for large amounts of data. In addition, in most cases they are only effective with well structured text, a thesaurus, and other contextual information. There are basically three different forms of searching the Web. Two of them are well known and are frequently used. The first is to use search engines that index a portion of the Web documents as a full-text database. The second is to use Web directories, which classify selected Web documents by subject. The third and not yet fully available, is to search the Web exploiting its hyperlink] t We will use hyperlink or link to denote a pointer (anchor) from a Web page to another Web page. 367    368 SEARCHING THE WEB structure. We cover all three forms of Web search here. We first discuss the challenges of searching the Web, followed by some Web statistics and models which can be used to understand the complexity of the problem. Next, we discuss in detail the main tools used today to search the Web. The discussion includes search engines, Web directories, hybrid systems, user interfaces, and searching examples. We continue with new query languages that exploit the graphical structure of the Web. Finally, we survey current trends and research issues. As Web research is a very dynamic field, we may have missed some important work, for which we apologize in advance.  ",
2311,iir,iir-2311,13.1 The text classification problem," 13.1 The text classification problem In text classification, we are given a description d∈Xof a document, where Xis the document space; and a fixed set of classes C={c1,c2, . . . , cJ}. Classes CLASS are also called categories or labels. Typically, the document space Xis some type of high-dimensional space, and the classes are human defined for the needs of an application, as in the examples China and documents that talk about multicore computer chips above. We are given a training set Dof labeled documents hd,ci,where hd,ci ∈ X×C. For example: hd,ci=hBeijing joins the World Trade Organization, Chinai for the one-sentence document Beijing joins the World Trade Organization and the class (or label) China. Using a learning method or learning algorithm, we then wish to learn a clas- LEARNING METHOD sifier or classification function γthat maps documents to classes:CLASSIFIER γ:X→C (13.1) This type of learning is called supervised learning because a supervisor (the human who defines the classes and labels training documents) serves as a teacher directing the learning process. We denote the supervised learning method by Γand write Γ(D) = γ. The learning method Γtakes the training set Das input and returns the learned classification function γ. Most names for learning methods Γare also used for classifiers γ. We talk about the Naive Bayes (NB) learning method Γwhen we say that “Naive Bayes is robust,” meaning that it can be applied to many different learning problems and is unlikely to produce classifiers that fail catastrophically. But when we say that “Naive Bayes had an error rate of 20%,” we are describing an experiment in which a particular NB classifier γ(which was produced by the NB learning method) had a 20% error rate in an application. Figure 13.1 shows an example of text classification from the Reuters-RCV1 collection, introduced in Section 4.2, page 69. There are six classes (UK,China, ..., sports), each with three training documents. We show a few mnemonic words for each document’s content. The training set provides some typical examples for each class, so that we can learn the classification function γ. Once we have learned γ, we can apply it to the test set (or test data), for ex-TEST SET ample, the new document first private Chinese airline whose class is unknown.   13.1 The text classification problem 257 classes: training set: test set: regions industries subject areas γ(d′) =China first private Chinese airline UK China poultry coffee elections sports London congestion Big Ben Parliament the Queen Windsor Beijing Olympics Great Wall tourism communist Mao chicken feed ducks pate turkey bird flu beans roasting robusta arabica harvest Kenya votes recount run-off seat campaign TV ads baseball diamond soccer forward captain team d′ ◮Figure 13.1 Classes, training set, and test set in text classification . In Figure 13.1, the classification function assigns the new document to class γ(d) = China, which is the correct assignment. The classes in text classification often have some interesting structure such as the hierarchy in Figure 13.1. There are two instances each of region categories, industry categories, and subject area categories. A hierarchy can be an important aid in solving a classification problem; see Section 15.3.2 for further discussion. Until then, we will make the assumption in the text classification chapters that the classes form a set with no subset relationships between them. Definition (13.1) stipulates that a document is a member of exactly one class. This is not the most appropriate model for the hierarchy in Figure 13.1. For instance, a document about the 2008 Olympics should be a member of two classes: the China class and the sports class. This type of classification problem is referred to as an any-of problem and we will return to it in Section 14.5 (page 306). For the time being, we only consider one-of problems where a document is a member of exactly one class. Our goal in text classification is high accuracy on test data or new data – for example, the newswire articles that we will encounter tomorrow morning in the multicore chip example. It is easy to achieve high accuracy on the training set (e.g., we can simply memorize the labels). But high accuracy on the training set in general does not mean that the classifier will work well on      258 13 Text classification and Naive Bayes new data in an application. When we use the training set to learn a classifier for test data, we make the assumption that training data and test data are similar or from the same distribution. We defer a precise definition of this notion to Section 14.6 (page 308).  ",13.1
2309,mir,mir-2309,13.10 Bibliographic Discussion," 13.10 Bibliographic Discussion There are hundreds of books about the Web. Many of them include some information about searching the Web and tips for users. A recent book edited by Abrams includes a chapter on searching the Web [3]. Other sources are [682], the special numbers of Scientific American on the Internet (March 1997) and IEEE's Internet Computing on Search Technologies (July/August 1998). For details about crawlers and other software agents see [166, 817]. In addition, the best source for references to the Web is the Web itself. To start with, there are many Web sites devoted to inform and rate search engines and Web directories. Among them we can distinguish Search Engine Watch [749] and Search Engine Showdown [609]. A survey about Web characterizations is given by Pitkow [641] and a good directory to Web characteristics is [217]. Other Web pages provide pointers and references related to searching the Web, in particular the World Wide Web Consortium (www.w3.org), the World Wide Web journal (w3j. com) and WWW conferences. These and other pointers are available in the Web page of this book (see Chapter 1). Acknowledgements We would like to thank the following for their helpful comments: Omar Alonso, Eric Brown, Pablo de la Fuente, Monika Henzinger and Gonzalo Navarro.  ",
2285,mir,mir-2285,13.2 Challenges," 13.2 Challenges We now mention the main problems posed by the Web. We can divide them in two classes: problems with the data itself and problems regarding the user and his interaction with the retrieval system. The problems related to the data are: • Distributed data: due to the intrinsic nature of the Web, data spans over many computers and platforms. These computers are interconnected with no predefined topology and the available bandwidth and reliability on the network interconnections varies widely. • High percentage of volatile data: due to Internet dynamics, new computers and data can be added or removed easily (it is estimated that 40% of the Web changes every month [424]). We also have dangling links and relocation problems when domain or file names change or disappear. • Large volume: the exponential growth of the Web poses scaling issues that are difficult to cope with. • Unstructured and redundant data: most people say that the Web is a distributed hypertext. However, this is not exactly so. Any hypertext has a conceptual model behind it, which organizes and adds consistency to the data and the hyper links. That is hardly true in the Web, even for individual documents. In addition, each HTML page is not well structured and some people use the term semi-structured data. Moreover, much Web data is repeated (mirrored or copied) or very similar. Approximately 30% of Web pages are (near) duplicates [120, 723]. Semantic redundancy can be even larger. • Quality of data: the Web can be considered as a new publishing medium. However, there is, in most cases, no editorial process. So, data can be false, invalid (for example, because it is too old), poorly written or, typically, with many errors from different sources (typos, grammatical mistakes, OCR errors, etc.). Preliminary studies show that the number of words with typos can range from 1 in 200 for common words to 1 in 3 for foreign surnames [588].    CHARACTERIZING THE WEB 369 • Heterogeneous data: in addition to having to deal with multiple media types and hence with multiple formats, we also have different languages and, what is worse, different alphabets, some of them very large (for example, Chinese or Japanese Kanji). Most of these problems (such as the variety of data types and poor data quality) are not solvable simply by software improvements. In fact, many of them will not change (and they should not, as in the case of language diversity!) because they are problems (also features) intrinsic to human nature. The second class of problems are those faced by the user during the interaction with the retrieval system. There are basically two problems: (1) how to specify a query and (2) how to interpret the answer provided by the system. Without taking into account the semantic content of a document, it is not easy to precisely specify a query, unless it is very simple ... Further, even if the user is able to pose the query, the answer might be a thousand Web pages. How do we handle a large answer? How do we rank the documents? How do we select the documents that really are of interest to the user? In addition, a single document could be large. How do we browse efficiently in large documents? So, the overall challenge, in spite of the intrinsic problems posed by the Web, is to submit a good query to the search system, and obtain a manageable and relevant answer. Moreover, in practice we should try to achieve the latter goal even for poorly formulated queries. In the rest of this chapter, we use the term Web pages for HTML documents (HTML is described in Chapter 6). To denote all possible data types available on the Web, we use the term Web documents.  ",
2312,iir,iir-2312,13.2 Naive Bayes text classification," 13.2 Naive Bayes text classification The first supervised learning method we introduce is the multinomial Naive Bayes or multinomial NB model, a probabilistic learning method. The probability of a document dbeing in class cis computed as P(c|d)∝P(c)∏ 1≤k≤nd P(tk|c) (13.2) where P(tk|c)is the conditional probability of term tkoccurring in a document of class c.1We interpret P(tk|c)as a measure of how much evidence tkcontributes that cis the correct class. P(c)is the prior probability of a document occurring in class c. If a document’s terms do not provide clear evidence for one class versus another, we choose the one that has a higher prior probability. ht1,t2, . . ., tndiare the tokens in dthat are part of the vocabulary we use for classification and ndis the number of such tokens in d. For example, ht1,t2, . . . , tndifor the one-sentence document Beijing and Taipei join the WTO might be hBeijing,Taipei,join,WTOi, with nd=4, if we treat the terms and and the as stop words. In text classification, our goal is to find the best class for the document. The best class in NB classification is the most likely or maximum a posteriori (MAP)MAXIMUM A POSTERIORI CLASS class cmap: cmap =arg max c∈C ˆ P(c|d) = arg max c∈C ˆ P(c)∏ 1≤k≤nd ˆ P(tk|c). (13.3) We write ˆ Pfor Pbecause we do not know the true values of the parameters P(c)and P(tk|c), but estimate them from the training set as we will see in a moment. In Equation (13.3), many conditional probabilities are multiplied, one for each position 1 ≤k≤nd. This can result in a floating point underflow. It is therefore better to perform the computation by adding logarithms of probabilities instead of multiplying probabilities. The class with the highest log probability score is still the most probable; log(xy) = log(x) + log(y) and the logarithm function is monotonic. Hence, the maximization that is 1. We will explain in the next section why P(c|d)is proportional to (∝), not equal to the quantity on the right.   13.2 Naive Bayes text classification 259 actually done in most implementations of NB is: cmap =arg max c∈C [log ˆ P(c) + ∑ 1≤k≤nd log ˆ P(tk|c)]. (13.4) Equation (13.4) has a simple interpretation. Each conditional parameter log ˆ P(tk|c)is a weight that indicates how good an indicator tkis for c. Similarly, the prior log ˆ P(c)is a weight that indicates the relative frequency of c. More frequent classes are more likely to be the correct class than infrequent classes. The sum of log prior and term weights is then a measure of how much evidence there is for the document being in the class, and Equation (13.4) selects the class for which we have the most evidence. We will initially work with this intuitive interpretation of the multinomial NB model and defer a formal derivation to Section 13.4. How do we estimate the parameters ˆ P(c)and ˆ P(tk|c)? We first try the maximum likelihood estimate (MLE; Section 11.3.2, page 226), which is simply the relative frequency and corresponds to the most likely value of each parameter given the training data. For the priors this estimate is: ˆ P(c) = Nc N, (13.5) where Ncis the number of documents in class cand Nis the total number of documents. We estimate the conditional probability ˆ P(t|c)as the relative frequency of term tin documents belonging to class c: ˆ P(t|c) = Tct ∑t′∈VTct′, (13.6) where Tct is the number of occurrences of tin training documents from class c, including multiple occurrences of a term in a document. We have made the positional independence assumption here, which we will discuss in more detail in the next section: Tct is a count of occurrences in all positions kin the documents in the training set. Thus, we do not compute different estimates for different positions and, for example, if a word occurs twice in a document, in positions k1and k2, then ˆ P(tk1|c) = ˆ P(tk2|c). The problem with the MLE estimate is that it is zero for a term–class combination that did not occur in the training data. If the term WTO in the training data only occurred in China documents, then the MLE estimates for the other classes, for example UK, will be zero: ˆ P(WTO|UK) = 0. Now, the one-sentence document Britain is a member of the WTO will get a conditional probability of zero for UK because we are multiplying the conditional probabilities for all terms in Equation (13.2). Clearly, the model should      260 13 Text classification and Naive Bayes TRAINMULTINOMIALNB(C,D) 1V←EXTRACTVOCABULARY(D) 2N←COUNTDOCS(D) 3for each c∈C 4do Nc←COUNTDOCSINCLASS(D,c) 5prior[c]←Nc/N 6textc←CONCATENATETEXTOFALLDOCSINCLASS(D,c) 7for each t∈V 8do Tct ←COUNTTOKENSOFTERM(textc,t) 9for each t∈V 10 do condprob[t][c]←Tct+1 ∑t′(Tct′+1) 11 return V,prior,condprob APPLYMULTINOMIALNB(C,V,prior,condprob,d) 1W←EXTRACTTOKENSFROMDOC(V,d) 2for each c∈C 3do score[c]←log prior[c] 4for each t∈W 5do score[c] += log condprob[t][c] 6return arg maxc∈Cscore[c] ◮Figure 13.2 Naive Bayes algorithm (multinomial model): Training and testing. assign a high probability to the UK class because the term Britain occurs. The problem is that the zero probability for WTO cannot be “conditioned away,” no matter how strong the evidence for the class UK from other features. The estimate is 0 because of sparseness: The training data are never large enough to represent the frequency of rare events adequately, for example, the frequency of WTO occurring in UK documents. To eliminate zeros, we use add-one or Laplace smoothing, which simply adds  one to each count (cf. Section 11.3.2): ˆ P(t|c) = Tct +1 ∑t′∈V(Tct′+1)=Tct +1 (∑t′∈VTct′) + B, (13.7) where B=|V|is the number of terms in the vocabulary. Add-one smoothing can be interpreted as a uniform prior (each term occurs once for each class) that is then updated as evidence from the training data comes in. Note that this is a prior probability for the occurrence of a term as opposed to the prior probability of a class which we estimate in Equation (13.5) on the document level.   13.2 Naive Bayes text classification 261 ◮Table 13.1 Data for parameter estimation examples. docID words in document in c=China? training set 1 Chinese Beijing Chinese yes 2 Chinese Chinese Shanghai yes 3 Chinese Macao yes 4 Tokyo Japan Chinese no test set 5 Chinese Chinese Chinese Tokyo Japan ? ◮Table 13.2 Training and test times for NB. mode time complexity training Θ(|D|Lave +|C||V|) testing Θ(La+|C|Ma) = Θ(|C|Ma) We have now introduced all the elements we need for training and applying an NB classifier. The complete algorithm is described in Figure 13.2. ✎Example 13.1: For the example in Table 13.1, the multinomial parameters we need to classify the test document are the priors ˆ P(c) = 3/4 and ˆ P(c) = 1/4 and the following conditional probabilities: ˆ P(Chinese|c) = (5+1)/(8+6) = 6/14 =3/7 ˆ P(Tokyo|c) = ˆ P(Japan|c) = (0+1)/(8+6) = 1/14 ˆ P(Chinese|c) = (1+1)/(3+6) = 2/9 ˆ P(Tokyo|c) = ˆ P(Japan|c) = (1+1)/(3+6) = 2/9 The denominators are (8+6)and (3+6)because the lengths of textcand textcare 8 and 3, respectively, and because the constant Bin Equation (13.7) is 6 as the vocabulary consists of six terms. We then get: ˆ P(c|d5)∝3/4 ·(3/7)3·1/14 ·1/14 ≈0.0003. ˆ P(c|d5)∝1/4 ·(2/9)3·2/9 ·2/9 ≈0.0001. Thus, the classifier assigns the test document to c=China. The reason for this classification decision is that the three occurrences of the positive indicator Chinese in d5 outweigh the occurrences of the two negative indicators Japan and Tokyo. What is the time complexity of NB? The complexity of computing the parameters is Θ(|C||V|)because the set of parameters consists of |C||V|conditional probabilities and |C|priors. The preprocessing necessary for computing the parameters (extracting the vocabulary, counting terms, etc.) can be done in one pass through the training data. The time complexity of this      262 13 Text classification and Naive Bayes component is therefore Θ(|D|Lave), where |D|is the number of documents and Lave is the average length of a document. We use Θ(|D|Lave)as a notation for Θ(T)here, where Tis the length of the training collection. This is nonstandard; Θ(.)is not defined for an average. We prefer expressing the time complexity in terms of Dand Lave because these are the primary statistics used to characterize training collections. The time complexity of APPLYMULTINOMIALNB in Figure 13.2 is Θ(|C|La). Laand Maare the numbers of tokens and types, respectively, in the test document. APPLYMULTINOMIALNB can be modified to be Θ(La+|C|Ma)(Exercise 13.8). Finally, assuming that the length of test documents is bounded, Θ(La+|C|Ma) = Θ(|C|Ma)because La&lt;b|C|Mafor a fixed constant b.2 Table 13.2 summarizes the time complexities. In general, we have |C||V|&lt; |D|Lave, so both training and testing complexity are linear in the time it takes to scan the data. Because we have to look at the data at least once, NB can be said to have optimal time complexity. Its efficiency is one reason why NB is a popular text classification method.  ",13.2
2313,iir,iir-2313,13.2.1 Relation to multinomial unigram language model," 13.2.1 Relation to multinomial unigram language model The multinomial NB model is formally identical to the multinomial unigram language model (Section 12.2.1, page 242). In particular, Equation (13.2) is a special case of Equation (12.12) from page 243, which we repeat here for λ=1: P(d|q)∝P(d)∏ t∈q P(t|Md). (13.8) The document din text classification (Equation (13.2)) takes the role of the query in language modeling (Equation (13.8)) and the classes cin text classification take the role of the documents din language modeling. We used Equation (13.8) to rank documents according to the probability that they are relevant to the query q. In NB classification, we are usually only interested in the top-ranked class. We also used MLE estimates in Section 12.2.2 (page 243) and encountered the problem of zero estimates owing to sparse data (page 244); but instead of add-one smoothing, we used a mixture of two distributions to address the problem there. Add-one smoothing is closely related to add-1 2smoothing in Section 11.3.4 (page 228). ?Exercise 13.1 Why is |C||V|&lt;|D|Lave in Table 13.2 expected to hold for most text collections? 2. Our assumption here is that the length of test documents is bounded. Lawould exceed b|C|Mafor extremely long test documents.       ",13.2
2286,mir,mir-2286,13.3 Characterizing the Web," 13.3 Characterizing the Web 13.3.1 Measuring the Web Measuring the Internet and in particular the Web, is a difficult task due to its highly dynamic nature. Nowadays, there are more than 40 million computers in more than 200 countries connected to the Internet, many of them hosting Web servers. The estimated number of Web servers ranges from 2.4 million according to NetSizer [597] (November 1998) to over three million according to the Netcraft Web survey [596] (October 1998). This wide range might be explained when we consider that there are many Web sites that share the same Web server using virtual hosts, that not all of them are fully accessible, that many of them are provisional, etc. Other estimations were made by sampling 0.1 % of all Internet numeric addresses obtaining about 2 million unique Web sites [619] or by counting domain names starting with www which in July 1998 were 780,000 according to the Internet Domain survey [599]. However, since not all Web servers have this prefix, the real number is even higher. Considering that in July 1998 the number of Internet hosts was estimated at 36.7 million [599], there is about one Web server per every ten computers connected to the  ",
2314,iir,iir-2314,13.3 The Bernoulli model," 13.3 The Bernoulli model 263 TRAINBERNOULLINB(C,D) 1V←EXTRACTVOCABULARY(D) 2N←COUNTDOCS(D) 3for each c∈C 4do Nc←COUNTDOCSINCLASS(D,c) 5prior[c]←Nc/N 6for each t∈V 7do Nct ←COUNTDOCSINCLASSCONTAININGTERM(D,c,t) 8condprob[t][c]←(Nct +1)/(Nc+2) 9return V,prior,condprob APPLYBERNOULLINB(C,V,prior,condprob,d) 1Vd←EXTRACTTERMSFROMDOC(V,d) 2for each c∈C 3do score[c]←log prior[c] 4for each t∈V 5do if t∈Vd 6then score[c] += log condprob[t][c] 7else score[c] += log(1−condprob[t][c]) 8return arg maxc∈Cscore[c] ◮Figure 13.3 NB algorithm (Bernoulli model): Training and testing. The add-one smoothing in Line 8 (top) is in analogy to Equation (13.7) with B=2. 13.3 The Bernoulli model There are two different ways we can set up an NB classifier. The model we introduced in the previous section is the multinomial model. It generates one term from the vocabulary in each position of the document, where we assume a generative model that will be discussed in more detail in Section 13.4 (see also page 237). An alternative to the multinomial model is the multivariate Bernoulli model or Bernoulli model. It is equivalent to the binary independence model of Sec- BERNOULLI MODEL tion 11.3 (page 222), which generates an indicator for each term of the vocabulary, either 1 indicating presence of the term in the document or 0 indicating absence. Figure 13.3 presents training and testing algorithms for the Bernoulli model. The Bernoulli model has the same time complexity as the multinomial model. The different generation models imply different estimation strategies and different classification rules. The Bernoulli model estimates ˆ P(t|c)as the fraction of documents of class cthat contain term t(Figure 13.3, TRAINBERNOULLI      264 13 Text classification and Naive Bayes NB, line 8). In contrast, the multinomial model estimates ˆ P(t|c)as the fraction of tokens or fraction of positions in documents of class cthat contain term t(Equation (13.7)). When classifying a test document, the Bernoulli model uses binary occurrence information, ignoring the number of occurrences, whereas the multinomial model keeps track of multiple occurrences. As a result, the Bernoulli model typically makes many mistakes when classifying long documents. For example, it may assign an entire book to the class China because of a single occurrence of the term China. The models also differ in how nonoccurring terms are used in classification. They do not affect the classification decision in the multinomial model; but in the Bernoulli model the probability of nonoccurrence is factored in when computing P(c|d)(Figure 13.3, APPLYBERNOULLINB, Line 7). This is because only the Bernoulli NB model models absence of terms explicitly. ✎Example 13.2: Applying the Bernoulli model to the example in Table 13.1, we have the same estimates for the priors as before: ˆ P(c) = 3/4, ˆ P(c) = 1/4\. The conditional probabilities are: ˆ P(Chinese|c) = (3+1)/(3+2) = 4/5 ˆ P(Japan|c) = ˆ P(Tokyo|c) = (0+1)/(3+2) = 1/5 ˆ P(Beijing|c) = ˆ P(Macao|c) = ˆ P(Shanghai|c) = (1+1)/(3+2) = 2/5 ˆ P(Chinese|c) = (1+1)/(1+2) = 2/3 ˆ P(Japan|c) = ˆ P(Tokyo|c) = (1+1)/(1+2) = 2/3 ˆ P(Beijing|c) = ˆ P(Macao|c) = ˆ P(Shanghai|c) = (0+1)/(1+2) = 1/3 The denominators are (3+2)and (1+2)because there are three documents in c and one document in cand because the constant Bin Equation (13.7) is 2 – there are two cases to consider for each term, occurrence and nonoccurrence. The scores of the test document for the two classes are ˆ P(c|d5)∝ˆ P(c)·ˆ P(Chinese|c)·ˆ P(Japan|c)·ˆ P(Tokyo|c) ·(1−ˆ P(Beijing|c)) ·(1−ˆ P(Shanghai|c)) ·(1−ˆ P(Macao|c)) =3/4 ·4/5 ·1/5 ·1/5 ·(1−2/5)·(1−2/5)·(1−2/5) ≈0.005 and, analogously, ˆ P(c|d5)∝1/4 ·2/3 ·2/3 ·2/3 ·(1−1/3)·(1−1/3)·(1−1/3) ≈0.022 Thus, the classifier assigns the test document to c=not-China. When looking only at binary occurrence and not at term frequency, Japan and Tokyo are indicators for c (2/3 &gt;1/5) and the conditional probabilities of Chinese for cand care not different enough (4/5 vs. 2/3) to affect the classification decision.       ",13.3
2287,mir,mir-2287,13.3.1 Measuring the Web," 13.3.1 Measuring the Web Measuring the Internet and in particular the Web, is a difficult task due to its highly dynamic nature. Nowadays, there are more than 40 million computers in more than 200 countries connected to the Internet, many of them hosting Web servers. The estimated number of Web servers ranges from 2.4 million according to NetSizer [597] (November 1998) to over three million according to the Netcraft Web survey [596] (October 1998). This wide range might be explained when we consider that there are many Web sites that share the same Web server using virtual hosts, that not all of them are fully accessible, that many of them are provisional, etc. Other estimations were made by sampling 0.1 % of all Internet numeric addresses obtaining about 2 million unique Web sites [619] or by counting domain names starting with www which in July 1998 were 780,000 according to the Internet Domain survey [599]. However, since not all Web servers have this prefix, the real number is even higher. Considering that in July 1998 the number of Internet hosts was estimated at 36.7 million [599], there is about one Web server per every ten computers connected to the    370 SEARCHING THE WEB Internet. The characterization of the Web isa new task of the Web Consortium [797]. In two interesting articles, already (sadly) outdated, Bray [114] and Woodruff et al. [834] studied different statistical measures of the Web. The first study uses 11 million pages while the second uses 2.6 million pages, with both sets gathered in November 1995. Their characterization of Web pages is partially reproduced in the following paragraphs. A first question is how many different institutions (not Web servers) maintain Web data. This number is smaller than the number of servers, because many places have multiple servers. The exact number is unknown, but should be more than 40% of the number of Web servers (this percentage was the value back in 1995). The exact number of Web pages is also not known. Estimates at the beginning of 1998 ranged from 200 to 320 million, with 350 million as the best current estimate (July 1998 [91]). The latter study used 20,000 random queries based on a lexicon of 400,000 words extracted from Yahoo!. Those queries were submitted to four search engines and the union of all the answers covered about 70% of the Web. Figure 13.1 gives an approximation of how the number of Web servers and the number of pages have changed in recent years. Between 1997 and 1998, the size of the Web doubled in nine months and is currently growing at a rate of 20 million pages per month. On the other hand, it is estimated that the 30,000 largest Web sites (about 1% of the Web) account for approximately 50% of all Web pages [619]. The most popular formats for Web documents are HTML, followed by GIF and JPG (both for images), ASCII text, and Postscript, in that order. The most popular compression tools used are GNU zip, Zip, and Compress. What is a typical HTML page? First, most HTML pages are not standard, meaning that they do not comply with all the HTML specifications. In adNumberof Web pages 300 (millions) _1:1. 200 100 Number of Web sites (millions) ...Q.. 3 2 0'\-__ \--+ -+\--' 0 1996 1997 1998 Figure 13.1 Approximate growth of the Web.    CHARACTERIZING THE WEB 371 dition, although HTML is an instance of SGML, HTML documents seldom start with a formal document type definition. Second, they are small (around 5 Kbs on average with a median of 2 Kbs) and usually contain few images (between one and two on average with an average size of 14 Kb). The pages that have images use them for presentation issues such as colored bullets and lines. An average page has between five and 15 hyperlinks (more than eight links on average) and most of them are local (that is, they point to pages in their own Web server hierarchy). On average, no external server points to any given page (typically, there are only local links pointing to a given page). This is true even for home pages of Web sites. In fact, in 1995, around 80% of these home pages had fewer than ten external links pointing to each of them. The top ten most referenced sites are Microsoft, Netscape, Yahoo!, and top US universities. In these cases we are talking about sites which are referenced by at least 100,000 places. On the other hand, the site with most links to outside sites is Yahoo!. In some sense, Yahoo! and other directories are the glue of the Web. Without them we would have many isolated portions (which is the case with many personal Web pages). If we assume that the average HTML page has 5 Kb and that there are 300 million Web pages, we have at least 1.5 terabytes of text. This is consistent with other measures obtained from search engines. Note that this volume does not include non-textual documents. Regarding the languages used in Web pages, there have been three studies made. The first study was done by Funredes [637] from 1996 to 1998\. It uses the AltaVista search engine and is based on searching different words in different languages. This technique might not be significant statistically, but the results are consistent with the second study which was carried out by Alis Technology [11] and is based on automatic software that can detect the language used. One of the goals of the study was to test such software (done in 8000 Web servers). The last study was done by OCLC in June of 1998 [619] by sampling Internet numeric addresses and using the SILC language identification software. Table 13.1 gives the percentages of Web pages written in each language (with the exception of the OCLC data that counts Web sites), as well as the number of people (millions) who speak the language. The variations for Japanese might be due to an inability to detect pages written in Kanji. Some languages, in particular Spanish and Portuguese, are growing fast and will surpass French in the near future. The total number of languages exceeds 100.  ",
2288,mir,mir-2288,13.3.2 Modeling the Web," 13.3.2 Modeling the Web Can we model the document characteristics of the whole Web? Yes, as has already been discussed partially in Chapter 6\. The Heaps' and Zipf's laws are also valid in the Web. In particular, the vocabulary grows faster (larger (3) and the word distribution should be more biased (larger 0). However, there are no experiments on large Web collections to measure these parameters.    372 SEARCHING THE WEB Language English Japanese German French Spanish Italian Portuguese Funredes (1998, %) 76.4 4.8 4.4 2.9 2.6 1.5 0.8 Alis Tech. (June 1997, %) 82.3 1.6 4.0 1.5 1.1 0.8 0.7 OCLC (June 1998, %) 71 4 7 3 3 1 2 Spoken by (millions) 450 126 118 122 266 63 175 Table 13.1 Languages of the Web. \---,-::::.......... , , "" , , \ .... _----- .... \ \ \'.' "" '\ . ................... \\\\\ ..... AIIFiles \'\ : Image Files \- \- - "" ~ Audio Files \- - - - \ "" ', Video Files . TextFiles _._. 2345678 log (File Size in Bytes) Figure 13.2 Left: Distribution for all file sizes (courtesy of M. Crovella, 1998). Right: Right tail distribution for different file types (from Crovella and Bestavros, 1996). All logarithms are in base 10\. An additional model is related to the distribution of document sizes. According to this model, the document sizes are self-similar [201], that is, they have a large variance (a similar behavior appears in Web traffic). This can be modeled by two different distributions. The main body of the distribution follows a logarithmic normal distribution, such that the probability of finding a document of size x bytes is given by where the average (1-£) and standard deviation (0') are 9.357 and 1.318, respectively [59]. Figure 13.2 (left) shows the size distribution of the experimental data.    SEARCH ENGINES 373 The right tail of the distribution is 'heavy-tailed.' That is, the majority of documents are small, but there is a non-trivial number of large documents. This is intuitive for image or video files, but it is also true for HTML pages. A good fit is obtained with the Pareto distribution ok"" p(x) = x1+ o where x is measured in bytes and k and a are parameters of the distribution [59] (see Figure 13.2 (right)). For text files, a is about 1.36, being smaller for images and other binary formats [201, 819]. Taking all Web documents into account, we get a = 1.1 and k = 9.3 Kb [58]. That is, 9.3 Kb is the cut point between both distributions, and 93% of all the files have a size below this value. In fact, for less than 50 Kb, images are the typical files, from 50 to 300 Kb we have an increasing number of audio files, and over that to several megabytes, video files are more frequent. The parameters of these distributions were obtained from a sample of more than 54,000 Web pages requested by several users in a period of two months of 1995. Recent data collected in 1998 show that the size distributions have the same form, but parameters change [58]. Related information can be found on Web benchmarks such as WebSpec96 and the Sun/Inktomi Inkbench [395].  ",
2315,iir,iir-2315,13.4 Properties of Naive Bayes," 13.4 Properties of Naive Bayes 265 13.4 Properties of Naive Bayes To gain a better understanding of the two models and the assumptions they make, let us go back and examine how we derived their classification rules in Chapters 11 and 12. We decide class membership of a document by assigning it to the class with the maximum a posteriori probability (cf. Section 11.3.2, page 226), which we compute as follows: cmap =arg max c∈C P(c|d) =arg max c∈C P(d|c)P(c) P(d) (13.9) =arg max c∈C P(d|c)P(c),(13.10) where Bayes’ rule (Equation (11.4), page 220) is applied in (13.9) and we drop the denominator in the last step because P(d)is the same for all classes and does not affect the argmax. We can interpret Equation (13.10) as a description of the generative process we assume in Bayesian text classification. To generate a document, we first choose class cwith probability P(c)(top nodes in Figures 13.4 and 13.5). The two models differ in the formalization of the second step, the generation of the document given the class, corresponding to the conditional distribution P(d|c): Multinomial P(d|c) = P(ht1, . . . , tk, . . ., tndi|c)(13.11) Bernoulli P(d|c) = P(he1, . . . , ei, . . . , eMi|c),(13.12) where ht1, . . . , tndiis the sequence of terms as it occurs in d(minus terms that were excluded from the vocabulary) and he1, . . . , ei, . . . , eMiis a binary vector of dimensionality Mthat indicates for each term whether it occurs in dor not. It should now be clearer why we introduced the document space Xin Equation (13.1) when we defined the classification problem. A critical step in solving a text classification problem is to choose the document representation. ht1, . . . , tndiand he1, . . . , eMiare two different document representations. In the first case, Xis the set of all term sequences (or, more precisely, sequences of term tokens). In the second case, Xis {0, 1}M. We cannot use Equations (13.11) and (13.12) for text classification directly. For the Bernoulli model, we would have to estimate 2M|C|different parameters, one for each possible combination of Mvalues eiand a class. The number of parameters in the multinomial case has the same order of magni      266 13 Text classification and Naive Bayes C=China X1=Beijing X2=and X3=Taipei X4=join X5=WTO ◮Figure 13.4 The multinomial NB model. tude.3This being a very large quantity, estimating these parameters reliably is infeasible. To reduce the number of parameters, we make the Naive Bayes conditional independence assumption. We assume that attribute values are independent of each other given the class: Multinomial P(d|c) = P(ht1, . . . , tndi|c) = ∏ 1≤k≤nd P(Xk=tk|c) (13.13) Bernoulli P(d|c) = P(he1, . . . , eMi|c) = ∏ 1≤i≤M P(Ui=ei|c).(13.14) We have introduced two random variables here to make the two different generative models explicit. Xkis the random variable for position kin the X document and takes as values terms from the vocabulary. P(Xk=t|c)is the probability that in a document of class cthe term twill occur in position k.Ui RANDOM VARIABLE U is the random variable for vocabulary term iand takes as values 0 (absence) and 1 (presence). ˆ P(Ui=1|c)is the probability that in a document of class c the term tiwill occur – in any position and possibly multiple times. We illustrate the conditional independence assumption in Figures 13.4 and 13.5. The class China generates values for each of the five term attributes (multinomial) or six binary attributes (Bernoulli) with a certain probability, independent of the values of the other attributes. The fact that a document in the class China contains the term Taipei does not make it more likely or less likely that it also contains Beijing. In reality, the conditional independence assumption does not hold for text data. Terms are conditionally dependent on each other. But as we will discuss shortly, NB models perform well despite the conditional independence assumption. 3. In fact, if the length of documents is not bounded, the number of parameters in the multinomial case is infinite.   13.4 Properties of Naive Bayes 267 UAlaska=0 UBeijing=1 UIndia=0 Ujoin=1 UTaipei=1 UWTO=1 C=China ◮Figure 13.5 The Bernoulli NB model. Even when assuming conditional independence, we still have too many parameters for the multinomial model if we assume a different probability distribution for each position kin the document. The position of a term in a document by itself does not carry information about the class. Although there is a difference between China sues France and France sues China, the occurrence of China in position 1 versus position 3 of the document is not useful in NB classification because we look at each term separately. The conditional independence assumption commits us to this way of processing the evidence. Also, if we assumed different term distributions for each position k, we would have to estimate a different set of parameters for each k. The probability of bean appearing as the first term of a coffee document could be different from it appearing as the second term, and so on. This again causes problems in estimation owing to data sparseness. For these reasons, we make a second independence assumption for the multinomial model, positional independence: The conditional probabilities for a term are the same independent of position in the document. P(Xk1=t|c) = P(Xk2=t|c) for all positions k1,k2, terms tand classes c. Thus, we have a single distribution of terms that is valid for all positions kiand we can use Xas its symbol.4Positional independence is equivalent to adopting the bag of words model, which we introduced in the context of ad hoc retrieval in Chapter 6 (page 117). With conditional and positional independence assumptions, we only need to estimate Θ(M|C|)parameters P(tk|c)(multinomial model) or P(ei|c)(Bernoulli 4. Our terminology is nonstandard. The random variable Xis a categorical variable, not a multinomial variable, and the corresponding NB model should perhaps be called a sequence model. We have chosen to present this sequence model and the multinomial model in Section 13.4.1 as the same model because they are computationally identical.      268 13 Text classification and Naive Bayes ◮Table 13.3 Multinomial versus Bernoulli model. multinomial model Bernoulli model event model generation of token generation of document random variable(s) X=tiff toccurs at given pos Ut=1 iff toccurs in doc document representation d=ht1, . . . , tk, . . ., tndi,tk∈V d =he1, . . . , ei, . . . , eMi, ei∈ {0, 1} parameter estimation ˆ P(X=t|c)ˆ P(Ui=e|c) decision rule: maximize ˆ P(c)∏1≤k≤ndˆ P(X=tk|c)ˆ P(c)∏ti∈Vˆ P(Ui=ei|c) multiple occurrences taken into account ignored length of docs can handle longer docs works best for short docs # features can handle more works best with fewer estimate for term the ˆ P(X=the|c)≈0.05 ˆ P(Uthe =1|c)≈1.0 model), one for each term–class combination, rather than a number that is at least exponential in M, the size of the vocabulary. The independence assumptions reduce the number of parameters to be estimated by several orders of magnitude. To summarize, we generate a document in the multinomial model (Figure 13.4) by first picking a class C=cwith P(c)where Cis a random variable C taking values from Cas values. Next we generate term tkin position kwith P(Xk=tk|c)for each of the ndpositions of the document. The Xkall have the same distribution over terms for a given c. In the example in Figure 13.4, we show the generation of ht1,t2,t3,t4,t5i=hBeijing,and,Taipei,join,WTOi, corresponding to the one-sentence document Beijing and Taipei join WTO. For a completely specified document generation model, we would also have to define a distribution P(nd|c)over lengths. Without it, the multinomial model is a token generation model rather than a document generation model. We generate a document in the Bernoulli model (Figure 13.5) by first picking a class C=cwith P(c)and then generating a binary indicator eifor each term tiof the vocabulary (1 ≤i≤M). In the example in Figure 13.5, we show the generation of he1,e2,e3,e4,e5,e6i=h0, 1, 0, 1, 1, 1i, corresponding, again, to the one-sentence document Beijing and Taipei join WTO where we have assumed that and is a stop word. We compare the two models in Table 13.3, including estimation equations and decision rules. Naive Bayes is so called because the independence assumptions we have just made are indeed very naive for a model of natural language. The conditional independence assumption states that features are independent of each other given the class. This is hardly ever true for terms in documents. In many cases, the opposite is true. The pairs hong and kong or london and en   13.4 Properties of Naive Bayes 269 ◮Table 13.4 Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. c1c2class selected true probability P(c|d)0.6 0.4 c1 ˆ P(c)∏1≤k≤ndˆ P(tk|c)(Equation (13.13)) 0.00099 0.00001 NB estimate ˆ P(c|d)0.99 0.01 c1 glish in Figure 13.7 are examples of highly dependent terms. In addition, the multinomial model makes an assumption of positional independence. The Bernoulli model ignores positions in documents altogether because it only cares about absence or presence. This bag-of-words model discards all information that is communicated by the order of words in natural language sentences. How can NB be a good text classifier when its model of natural language is so oversimplified? The answer is that even though the probability estimates of NB are of low quality, its classification decisions are surprisingly good. Consider a document dwith true probabilities P(c1|d) = 0.6 and P(c2|d) = 0.4 as shown in Table 13.4. Assume that dcontains many terms that are positive indicators for c1and many terms that are negative indicators for c2. Thus, when using the multinomial model in Equation (13.13), ˆ P(c1)∏1≤k≤ndˆ P(tk|c1)will be much larger than ˆ P(c2)∏1≤k≤ndˆ P(tk|c2)(0.00099 vs. 0.00001 in the table). After division by 0.001 to get well-formed probabilities for P(c|d), we end up with one estimate that is close to 1.0 and one that is close to 0.0. This is common: The winning class in NB classification usually has a much larger probability than the other classes and the estimates diverge very significantly from the true probabilities. But the classification decision is based on which class gets the highest score. It does not matter how accurate the estimates are. Despite the bad estimates, NB estimates a higher probability for c1and therefore assigns dto the correct class in Table 13.4.Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well. Even if it is not the method with the highest accuracy for text, NB has many virtues that make it a strong contender for text classification. It excels if there are many equally important features that jointly contribute to the classification decision. It is also somewhat robust to noise features (as defined in the next section) and concept drift – the gradual change over time of the con- CONCEPT DRIFT cept underlying a class like US president from Bill Clinton to George W. Bush (see Section 13.7). Classifiers like kNN (Section 14.3, page 297) can be carefully tuned to idiosyncratic properties of a particular time period. This will then hurt them when documents in the following time period have slightly      270 13 Text classification and Naive Bayes ◮Table 13.5 A set of documents for which the NB independence assumptions are problematic. (1) He moved from London, Ontario, to London, England. (2) He moved from London, England, to London, Ontario. (3) He moved from England to London, Ontario. different properties. The Bernoulli model is particularly robust with respect to concept drift. We will see in Figure 13.8 that it can have decent performance when using fewer than a dozen terms. The most important indicators for a class are less likely to change. Thus, a model that only relies on these features is more likely to maintain a certain level of accuracy in concept drift. NB’s main strength is its efficiency: Training and classification can be accomplished with one pass over the data. Because it combines efficiency with good accuracy it is often used as a baseline in text classification research. It is often the method of choice if (i) squeezing out a few extra percentage points of accuracy is not worth the trouble in a text classification application, (ii) a very large amount of training data is available and there is more to be gained from training on a lot of data than using a better classifier on a smaller training set, or (iii) if its robustness to concept drift can be exploited. In this book, we discuss NB as a classifier for text. The independence assumptions do not hold for text. However, it can be shown that NB is an optimal classifier (in the sense of minimal error rate on new data) for data where the independence assumptions do hold.  ",13.4
2289,mir,mir-2289,13.4 Search Engines," 13.4 Search Engines In this section we cover different architectures of retrieval systems that model the Web as a full-text database. One main difference between standard IR systems and the Web is that, in the Web, all queries must be answered without accessing the text (that is, only the indices are available). Otherwise, that would require either storing locally a copy of the Web pages (too expensive) or accessing remote pages through the network at query time (too slow). This difference has an impact on the indexing and searching algorithms, as well as on the query languages made available. 13.4.1 Centralized Architecture Most search engines use a centralized crawler-indexer architecture. Crawlers are programs (software agents) that traverse the Web sending new or updated pages to a main server where they are indexed. Crawlers are also called robots, spiders, wanderers, walkers, and knowbots. In spite of their name, a crawler does not actually move to and run on remote machines, rather the crawler runs on a local system and sends requests to remote Web servers. The index is used in a centralized fashion to answer queries submitted from different places in the Web. Figure 13.3 shows the software architecture of a search engine based on the AltaVista architecture [17]. It has two parts: one that deals with the users,  ",
2316,iir,iir-2316,13.4.1 A variant of the multinomial model," 13.4.1 A variant of the multinomial model An alternative formalization of the multinomial model represents each document das an M-dimensional vector of counts htft1,d, . . . , tftM,diwhere tfti,d is the term frequency of tiin d.P(d|c)is then computed as follows (cf. Equation (12.8), page 243); P(d|c) = P(htft1,d, . . . , tftM,di|c)∝∏ 1≤i≤M P(X=ti|c)tfti,d (13.15) Note that we have omitted the multinomial factor. See Equation (12.8) (page 243). Equation (13.15) is equivalent to the sequence model in Equation (13.2) as P(X=ti|c)tfti,d=1 for terms that do not occur in d(tfti,d=0) and a term that occurs tfti,d≥1 times will contribute tfti,dfactors both in Equation (13.2) and in Equation (13.15).       ",13.4
2290,mir,mir-2290,13.4.1 Centralized Architecture," 13.4.1 Centralized Architecture Most search engines use a centralized crawler-indexer architecture. Crawlers are programs (software agents) that traverse the Web sending new or updated pages to a main server where they are indexed. Crawlers are also called robots, spiders, wanderers, walkers, and knowbots. In spite of their name, a crawler does not actually move to and run on remote machines, rather the crawler runs on a local system and sends requests to remote Web servers. The index is used in a centralized fashion to answer queries submitted from different places in the Web. Figure 13.3 shows the software architecture of a search engine based on the AltaVista architecture [17]. It has two parts: one that deals with the users,    374 SEARCHING THE WEB Users Web Figure 13.3 Typical crawler-indexer architecture. consisting of the user interface and the query engine and another that consists of the crawler and indexer modules. In 1998, the overall Alta Vista system was running on 20 multi-processor machines, all of them having more than 130 Gb of RAM and over 500 Gb of disk space. Only the query engine uses more than 75% of these resources. The main problem faced by this architecture is the gathering of the data, because of the highly dynamic nature of the Web, the saturated communication links, and the high load at Web servers. Another important problem is the volume of the data. In fact, the crawler-indexer architecture may not be able to cope with Web growth in the near future. Particularly important is good load balancing between the different activities of a search engine, internally (answering queries and indexing) and externally (crawling). The largest search engines, considering Web coverage in June 1998, were I AltaVista [17], HotBot [380], Northern Light [608], and Excite [240], in that order. According to recent studies, these engines cover 28-55% [749] or 14-34% [490] of all Web pages, whose number was estimated at over 300 million in 1998\. Table 13.2 lists the most important search engines and their estimated sizes along with their corresponding URLs. Beware that some search engines are powered by the same internal engine. For example, HotBot, GoTo, and Microsoft are powered by Inktomi [395] and Magellan by Excite's internal engine. Up to date information can be found in [749, 609]. Most search engines are based in the United States and focus on documents in English. Nevertheless, there are search engines specialized in different countries and/or languages, which are able, for instance, to query and retrieve documents written in Kanji (Chinese, Japanese, and Korean). Also there are search engines that take other approaches, like Ask Jeeves! which simulates an interview [34] or DirectHit [215] which ranks the Web pages in the answer in order of their popularity. We should also mention those search engines aimed at specific topics, for example the Search Broker [537] which allows us to search in many specific topics and DejaNews [212] which searches the USENET archives.    Search engine AltaVista AOL Netfind Excite Google GoTo HotBot Infoseek Lycos Magellan Microsoft NorthernLight WebCrawler URL www.altavista.com www.aol.com/netfind/ www.excite.com google.stanford.edu goto.com www.hotbot.com www.infoseek.com www.lycos.com www.mckinley.com search.msn.com www.nlsearch.com www.webcrawler.com SEARCH ENGINES 375 Web pages indexed 140 55 25 110 30 30 55 67 2 Table 13.2 URLs and estimated size (millions) of the largest search engines (May 1998). There are also engines to retrieve specific Web pages such as personal or institutional home pages or specific objects such as electronic mail addresses, images, or software applets.  ",
2291,mir,mir-2291,13.4.2 Distributed Architecture," 13.4.2 Distributed Architecture 'There are several variants of the crawler-indexer architecture. Among them, the most important is Harvest [108]. Harvest uses a distributed architecture to gather and distribute data, which is more efficient than the crawler architecture. The main drawback is that Harvest requires the coordination of several Web servers. The Harvest distributed approach addresses several of the problems of the crawler-indexer architecture, such as: (1) Web servers receive requests from different crawlers, increasing their load; (2) Web traffic increases because crawlers retrieve entire objects, but most of their content is discarded; and (3) information is gathered independently by each crawler, without coordination between all the search engines. . To solve these problems, Harvest introduces two main elements: gatherers and brokers. A gatherer collects and extracts indexing information from one or more Web servers. Gathering times are defined by the system and are periodic (i.e. there are harvesting times as the name of the system suggests). A broker provides the indexing mechanism and the query interface to the data gathered. Brokers retrieve information from one or more gatherers or other brokers, updating incrementally their indices. Depending on the configuration of gatherers and brokers, different improvements on server load and network traffic can be    376 SEARCHING THE WEB User Figure 13.4 Harvest architecture. achieved. For example, a gatherer can run on a Web server, generating no external traffic for that server. Also, a gatherer can send information to several brokers, avoiding work repetition. Brokers can also filter information and send it to other brokers. This design allows the sharing of work and information in a very flexible and generic manner. An example of the Harvest architecture is shown in Figure 13.4 [1081. One of the goals of Harvest is to build topic-specific brokers, focusing the index contents and avoiding many of the vocabulary and scaling problems of generic indices. Harvest includes a distinguished broker that allows other brokers to register information about gatherers and brokers. This is useful to search for an appropriate broker or gatherer when building a new system. The Harvest architecture also provides replicators and object caches. A replicator can be used to replicate servers, enhancing user-base scalability. For example, the registration broker can be replicated in different geographic regions to allow faster access. Replication can also be used to divide the gathering process between many Web servers. Finally, the object cache reduces network and server load, as well as response latency when accessing Web pages. More details on the system can be found in [1081. Currently, there are hundreds of Harvest applications on the Web (for example, the CIA, NASA, the US National Academy of Sciences, and the US Government Printing Office), as this software is on the public domain.t Netscape's Catalog Server is a commercial version of Harvest and Network Appliances' cache is a commercial version of the Harvest Cache. ~ Information is available at harvest. trans arc . com.    SEARCH ENGINES 377 Figure 13.5 Query interface for complex queries in AltaVista.  ",
2292,mir,mir-2292,13.4.3 User Interfaces," 13.4.3 User Interfaces There are two important aspects of the user interface of search engines: the query interface and the answer interface (see also Chapter 10). The basic query interface is a box where one or more words can be typed. Although a user would expect that a given sequence of words represents the same query in all search engines, it does not. For example, in AltaVista a sequence of words is a reference to the union of all the Web pages having at least one of those words, while in HotBot it is a reference to the Web pages having all the words. Another problem is that the logical view of the text is not known, that is, some search engines use stopwords, some do stemming, and some are not case sensitive (see Chapter 7). All search engines also provide a query interface for complex queries as well as a command language including Boolean operators and other features, such as phrase search, proximity search, and wild cards. Figures 13.5 and 13.6 show the query interfaces for complex queries for the three largest search engines. They provide several filtering functions. The results can be filtered by additional words that must be present or absent from the answer or in a particular field such as the URL or title, language, geographic region or Internet domain, date range, or inclusion of specific data types such as images or audio. The answer usually consists of a. list of the ten top ranked Web pages. Figure 13.7 shows the three top documents for the main four search engines for the query searching and Web and engine. Each entry in this list includes some information about the document it represents. Typically, the information includes the URL, size, the date when the page was indexed, and a couple of lines with its content (title plus first lines or selected headings or sentences). Some search engines allow the user to change the number of pages returned in the list and the amount of information per page, but in most cases this is fixed or limited to a few choices. The order of the list is typically by relevance, but sorting by URL or date is also available in some engines. In addition, most search engines also have an option to find documents similar to each Web page in the answer.    The user can also refine the query by constructing more complex queries based on the previous answer. The Web pages retrieved by the search engine in response to a user query are ranked, usually using statistics related to the terms in the query. In some cases this may not have any meaning, because relevance is not fully correlated with statistics about term occurrence within the collection. Some search engines also taking into account terms included in metatags or the title, or the popularity of a Web page to improve the ranking. This topic is covered next.  ",
2293,mir,mir-2293,13.4.4 Ranking," 13.4.4 Ranking Most search engines use variations of the Boolean or vector model (see Chapter 2) to do ranking. As with searching, ranking has to be performed without accessing the text, just the index. There is not much public information about the specific ranking algorithms used by current search engines. Further, it is difficult to compare fairly different search engines given their differences, and continuous improvements. More important, it is almost impossible to measure recall, as the number of relevant pages can be quite large for simple queries. Some inconclusive studies include [327, 498). Yuwono and Lee [844) propose three ranking algorithms in addition to the classical tf-idf scheme (see Chapter 2). They are called Boolean spread, vector spread, and most-cited. The first two are the normal ranking algorithms of the Boolean and vector model extended to include pages pointed to by a page in the answer or pages that point to a page in the answer. The third, most-cited, is based only on the terms included in pages having a link to the pages in the answer. A comparison of these techniques considering 56 queries over a collection of 2400 Web pages indicates that the vector model yields a better recall-precision curve, with an average precision of 75%. . Some of the new ranking algorithms also use hyperlink information. This is an important difference between the Web and normal IR databases. The number of hyperlinks that point to a page provides a measure of its popularity and quality. Also, many links in common between pages or pages referenced by the same page often indicates a relationship between those pages. We now present three examples of ranking techniques that exploit these facts, but they differ in that two of them depend on the query and the last does not. The first is WebQuery [148), which also allows visual browsing of Web pages. WebQuery takes a set of Web pages (for example, the answer to a query) and ranks them based on how connected each Web page is. Additionally, it extends the set by finding Web pages that are highly connected to the original set. A related approach is presented by Li [512). A better idea is due to Kleinberg [444) and used in HITS (Hypertext Induced Topic Search). This ranking scheme depends on the query and considers the set of pages S that point to or are pointed by pages in the answer. Pages that have many links pointing to them in S are called authorities (that is, they should have relevant content). Pages that have many outgoing links are called hubs (they should point to similar content). A positive two-way feedback exists:    SEARCH ENGINES 381 better authority pages come from incoming edges from good hubs and better hub pages come from outgoing edges to good authorities. Let H (p) and A(P) be the hub and authority value of page p. These values are defined such that the following equations are satisfied for all pages p: H(p) = L A(u) , 'UES I P---''U A(p) = L H(v) vES I v---.p where H(p) and A(p) for all pages are normalized (in the original paper, the sum of the squares of each measure is set to one). These values can be determined through an iterative algorithm, and they converge to the principal eigenvector of the link matrix of S. In the case of the Web, to avoid an explosion ofthe size of S, a maximal number of pages pointing to the answer can be defined. This technique does not work with non-existent, repeated, or automatically generated links. One solution is to weight each link based on the surrounding content. A second problem is that the topic of the result can become diffused. For example, a particular query is enlarged by a more general topic that contains the original answer. One solution to this problem is to analyze the content of each page and assign a score to it, as in traditional IR ranking. The link weight and the page score can be included on the previous formula multiplying each term of the summation [154, 93, 153]. Experiments show that the recall and precision on the first ten answers increases significantly [93]. The order of the links can also be used by dividing the links into subgroups and using the HITS algorithm on those subgroups instead of the original Web pages [153]. The last example is PageRank, which is part of the ranking algorithm used by Google [117]. PageRank simulates a user navigating randomly in the Web who jumps to a random page with probability q or follows a random hyperlink (on the current page) with probability 1 - q. It is further assumed that this user never goes back to a previously visited page following an already traversed hyperlink backwards. This process can be modeled with a Markov chain, from where the stationary probability of being in each page can be computed. This value is then used as part of the ranking mechanism of G06g1e. Let C(a) be the number of outgoing links of page a and suppose that page a is pointed to by pages PI to Pn. Then, the PageRank, P R( a) of a is defined as n PR(a) = q \+ (1 - q) L PR(Pi)/C(Pi) i=I where q must be set by the system (a typical value is 0.15). Notice that the ranking (weight) of other pages is normalized by the number of links in the page. PageRank can be computed using an iterative algorithm, and corresponds to the principal eigenvector of the normalized link matrix of the Web (which is the transition matrix of the Markov chain). Crawling the Web using this ordering has been shown to be better than other crawling schemes [168] (see next section).    382 SEARCHING THE WEB Therefore, to help ranking algorithms, page designers should include informative titles, headings, and meta fields, as well as good links. However, keywords should not be repeated as some search engines penalize repeating words (spamming). Using full terms instead of indirect ways to refer to subjects should also be considered.  ",
2294,mir,mir-2294,13.4.5 Crawling the Web," 13.4.5 Crawling the Web In this section we discuss how to crawl the Web, as there are several techniques. The simplest is to start with a set of URLs and from there extract other URLs which are followed recursively in a breadth-first or depth-first fashion. For that reason, search engines allow users to submit top Web sites that will be added to the URL set. A variation is to start with a set of populars URLs, because we can expect that they have information frequently requested. Both cases work well for one crawler, but it is difficult to coordinate several crawlers to avoid visiting the same page more than once. Another technique is to partition the Web using country codes or Internet names, and assign one or more robots to each partition, and explore each partition exhaustively. Considering how the Web is traversed, the index of a search engine can be thought of as analogous to the stars in an sky. What we see has never existed, as the light has traveled different distances to reach our eye. Similarly, Web pages referenced in an index were also explored at different dates and they may not exist any more. Nevertheless, when we retrieve a page, we obtain its actual content. How fresh are the Web pages referenced in an index? The pages will be from one day to two months old. For that reason, most search engines show in the answer the date when the page was indexed. The percentage of invalid links stored in search engines vary from 2 to 9%. User submitted pages are usually crawled after a few days or weeks. Starting there, some engines traverse the whole Web site, while others select just a sample of pages or pages up to a certain depth. Non-submitted pages will wait from weeks up to a couple of months to be detected. There are some engines that learn the change frequency of a page and visit it accordingly [175]. They may also crawl more frequently popular pages (for example, pages having many links pointing to them). Overall, the current fastest crawlers are able to traverse up to 10 million Web pages per day. The order in which the URLs are traversed is important. As already mentioned, the links in a Web page can be traversed breadth first or depth first. Using a breadth first policy, we first look at all the pages linked by the current page, and so on. This matches well Web sites that are structured by related topics. On the other hand, the coverage will be wide but shallow and a Web server can be bombarded with many rapid requests. In the depth first case, we follow the first link of a page and we do the same on that page until we cannot go deeper, returning recursively. This provides a narrow but deep traversal. Only recently, some research on this problem has appeared [168], showing that good ordering schemes can make a difference if crawling better pages first (using the PageRank scheme mentioned above).    SEARCH ENGINES 383 Due to the fact that robots can overwhelm a server with rapid requests and can use significant Internet bandwidth (in particular the whole bandwidth of small domains can be saturated), a set of guidelines for robot behavior has been developed [457]. For this purpose, a special file is placed at the root of every Web server indicating the restrictions at that site, in particular the pages that should not be indexed. Crawlers can also have problems with HTML pages that use frames (a mechanism to divide a page in two or more parts) or image maps (hyperlinks associated to images). In addition, dynamically generated pages cannot be indexed as well as password protected pages.  ",
2295,mir,mir-2295,13.4.6 Indices," 13.4.6 Indices Most indices use variants ofthe inverted file (see Chapter 8). In short, an inverted file is a list of sorted words (vocabulary), each one having a set of pointers to the pages where it occurs. Some search engines use elimination of stopwords to reduce the size of the index. Also, it is important to remember that a logical view of the text is indexed. Normalization operations may include removal of punctuation and multiple spaces to just one space between each word, uppercase to lowercase letters, etc. (see Chapter 7). To give the user some idea about each document retrieved, the index is complemented with a short description of each Web page (creation date, size, the title and the first lines or a few headings are typical). Assuming that 500 bytes are required to store the VRL and the description of each Web page, we need 50 Gb to store the description for 100 million pages. As the user initially receives only a subset of the complete answer to each query, the search engine usually keeps the whole answer set in memory, to avoid having to recompute it if the user asks for more documents. State of the art indexing techniques can reduce the size of an inverted file to about 30% of the size of the text (less if stopwords are used). For 100 million pages, this implies about 150 Gb of disk space. By using compression techniques, the index size can be reduced to 10% of the text [825]. A query is answered by doing a binary search on the sorted list of words of the inverted file. If we are searching multiple words, the results have to be combined to generate the final answer. This step will be efficient if each word is not too frequent. Another possibility is to compute the complete answer while the user requests more Web pages, using a lazy evaluation scheme. More details on searching over an inverted file can be found in Chapter 8\. Inverted files can also point to the actual occurrences of a word within a document (full inversion). However, that is too costly in space for the Web, because each pointer has to specify a page and a position inside the page (word numbers can be used instead of actual bytes). On the other hand, having the positions of the words in a page, we can answer phrase searches or proximity queries by finding words that are near each other in a page. Currently, some search engines are providing phrase searches, but the actual implementation is not known. Finding words which start with a given prefix requires two binary searches in the sorted list of words. More complex searches, like words with errors,    384 SEARCHING THE WEB arbitrary wild cards or, in general, any regular expression on a word, can be performed by doing a sequential scan over the vocabulary (see Chapter 8). This may seem slow, but the best sequential algorithms for this type of query can search around 20 Mb of text stored in RAM in one second (5 Mb is more or less the vocabulary size for 1 Gb of text). Thus, for several gigabytes we can answer those queries in a few seconds. For the Web this is still too slow but not completely out of the question. In fact, using Heaps' law and assuming (3 = 0.7 for the Web, the vocabulary size for 1 Tb is 630 Mb which implies a searching time of half a minute. Pointing to pages or to word positions is an indication of the granularity of the index. The index can be less dense if we point to logical blocks instead of pages. In this way we reduce the variance of the different document sizes, by making all blocks roughly the same size. This not only reduces the size of the pointers (because there are fewer blocks than documents) but also reduces the number of pointers because words have locality of reference (that is, all the occurrences of a non-frequent word will tend to be clustered in the same block). This idea was used in Glimpse [540] which is at the core of Harvest [108]. Queries are resolved as for inverted files, obtaining a list of blocks that are then searched sequentially (exact sequential search can be done over 30 Mb per second in RAM). Glimpse originally used only 256 blocks, which was efficient up to 200 Mb for searching words that were not too frequent, obtaining an index of only 2% of the text. By tuning the number of blocks and the block size, reasonable space-time trade-offs can be achieved for larger document collections (for more details see Chapter 8). These ideas cannot be used (yet) for the Web because sequential search cannot be afforded, as it implies a network access. However, in a distributed architecture where the index is also distributed, logical blocks make sense.  ",
2296,mir,mir-2296,13.5 Browsing," 13.5 Browsing In this section we cover Web tools which are based on browsing and searching, in particular Web directories. Although the Web coverage provided by directories is very low (less than 1% of all Web pages), the answers returned to the user are usually much more relevant. 13.5.1 Web Directories The best and oldest example of a Web directory is Yahoo! [839], which is likely the most used searching tool. Other large Web directories include eBLAST, LookSmart, Magellan, and NewHoo. Some of them are hybrids, because they also provide searches in the whole Web. Most search engines also provide subject categories nowadays, including AltaVista Categories, AOL Netfind, Excite Channels, HotBot, Infoseek, Lycos Subjects, and WebCrawler Select. are specific to some areas. For example, there are Web sites focused on business, news,  ",
2317,iir,iir-2317,13.5 Feature selection," 13.5 Feature selection 271 SELECTFEATURES(D,c,k) 1V←EXTRACTVOCABULARY(D) 2L←[] 3for each t∈V 4do A(t,c)←COMPUTEFEATUREUTILITY(D,t,c) 5 APPEND(L,hA(t,c),ti) 6return FEATURESWITHLARGESTVALUES(L,k) ◮Figure 13.6 Basic feature selection algorithm for selecting the kbest features. ?Exercise 13.2 [⋆] Which of the documents in Table 13.5 have identical and different bag of words representations for (i) the Bernoulli model (ii) the multinomial model? If there are differences, describe them. Exercise 13.3 The rationale for the positional independence assumption is that there is no useful information in the fact that a term occurs in position kof a document. Find exceptions. Consider formulaic documents with a fixed document structure. Exercise 13.4 Table 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the difference. 13.5 Feature selection Feature selection is the process of selecting a subset of the terms occurring in the training set and using only this subset as features in text classification. Feature selection serves two main purposes. First, it makes training and applying a classifier more efficient by decreasing the size of the effective vocabulary. This is of particular importance for classifiers that, unlike NB, are expensive to train. Second, feature selection often increases classification accuracy by eliminating noise features. A noise feature is one that, when added to the document representation, increases the classification error on new data. Suppose a rare term, say arachnocentric, has no information about a class, say China, but all instances of arachnocentric happen to occur in China documents in our training set. Then the learning method might produce a classifier that misassigns test documents containing arachnocentric to China. Such an incorrect generalization from an accidental property of the training set is called overfitting.OVERFITTING We can view feature selection as a method for replacing a complex classifier (using all features) with a simpler one (using a subset of the features).      272 13 Text classification and Naive Bayes It may appear counterintuitive at first that a seemingly weaker classifier is advantageous in statistical text classification, but when discussing the biasvariance tradeoff in Section 14.6 (page 308), we will see that weaker models are often preferable when limited training data are available. The basic feature selection algorithm is shown in Figure 13.6. For a given class c, we compute a utility measure A(t,c)for each term of the vocabulary and select the kterms that have the highest values of A(t,c). All other terms are discarded and not used in classification. We will introduce three different utility measures in this section: mutual information, A(t,c) = I(Ut;Cc); the χ2test, A(t,c) = X2(t,c); and frequency, A(t,c) = N(t,c). Of the two NB models, the Bernoulli model is particularly sensitive to noise features. A Bernoulli NB classifier requires some form of feature selection or else its accuracy will be low. This section mainly addresses feature selection for two-class classification tasks like China versus not-China. Section 13.5.5 briefly discusses optimizations for systems with more than two classes.  ",13.5
2318,iir,iir-2318,13.5.1 Mutual information," 13.5.1 Mutual information A common feature selection method is to compute A(t,c)as the expected mutual information (MI) of term tand class c.5MI measures how much in-MUTUAL INFORMATION formation the presence/absence of a term contributes to making the correct classification decision on c. Formally: I(U;C) = ∑ et∈{1,0} ∑ ec∈{1,0} P(U=et,C=ec)log2 P(U=et,C=ec) P(U=et)P(C=ec), (13.16) where Uis a random variable that takes values et=1 (the document contains term t) and et=0 (the document does not contain t), as defined on page 266, and Cis a random variable that takes values ec=1 (the document is in class c) and ec=0 (the document is not in class c). We write Utand Ccif it is not clear from context which term tand class cwe are referring to. For MLEs of the probabilities, Equation (13.16) is equivalent to Equation (13.17): I(U;C) = N11 Nlog2 NN11 N1.N.1 +N01 Nlog2 NN01 N0.N.1 (13.17) +N10 Nlog2 NN10 N1.N.0 +N00 Nlog2 NN00 N0.N.0 where the Ns are counts of documents that have the values of etand ecthat are indicated by the two subscripts. For example, N10 is the number of doc5. Take care not to confuse expected mutual information with pointwise mutual information, which is defined as log N11/E11 where N11 and E11 are defined as in Equation (13.18). The two measures have different properties. See Section 13.7.      13.5 Feature selection 273 uments that contain t(et=1) and are not in c(ec=0). N1\. =N10 +N11 is the number of documents that contain t(et=1) and we count documents independent of class membership (ec∈ {0, 1}). N=N00 +N01 +N10 +N11 is the total number of documents. An example of one of the MLE estimates that transform Equation (13.16) into Equation (13.17) is P(U=1, C=1) = N11/N. ✎Example 13.3: Consider the class poultry and the term export in Reuters-RCV1. The counts of the number of documents with the four possible combinations of indicator values are as follows: ec=epoultry =1ec=epoultry =0 et=eexport =1N11 =49 N10 =27,652 et=eexport =0N01 =141 N00 =774,106 After plugging these values into Equation (13.17) we get: I(U;C) = 49 801,948 log2 801,948 ·49 (49+27,652)(49+141) +141 801,948 log2 801,948 ·141 (141+774,106)(49+141) +27,652 801,948 log2 801,948 ·27,652 (49+27,652)(27,652+774,106) +774,106 801,948 log2 801,948 ·774,106 (141+774,106)(27,652+774,106) ≈0.0001105 To select kterms t1, . . . , tkfor a given class, we use the feature selection algorithm in Figure 13.6: We compute the utility measure as A(t,c) = I(Ut,Cc) and select the kterms with the largest values. Mutual information measures how much information – in the informationtheoretic sense – a term contains about the class. If a term’s distribution is the same in the class as it is in the collection as a whole, then I(U;C) = 0. MI reaches its maximum value if the term is a perfect indicator for class membership, that is, if the term is present in a document if and only if the document is in the class. Figure 13.7 shows terms with high mutual information scores for the six classes in Figure 13.1.6The selected terms (e.g., london,uk,british for the class UK) are of obvious utility for making classification decisions for their respective classes. At the bottom of the list for UK we find terms like peripherals and tonight (not shown in the figure) that are clearly not helpful in deciding 6. Feature scores were computed on the first 100,000 documents, except for poultry, a rare class, for which 800,000 documents were used. We have omitted numbers and other special words from the top ten lists.      274 13 Text classification and Naive Bayes UK london 0.1925 uk 0.0755 british 0.0596 stg 0.0555 britain 0.0469 plc 0.0357 england 0.0238 pence 0.0212 pounds 0.0149 english 0.0126 China china 0.0997 chinese 0.0523 beijing 0.0444 yuan 0.0344 shanghai 0.0292 hong 0.0198 kong 0.0195 xinhua 0.0155 province 0.0117 taiwan 0.0108 poultry poultry 0.0013 meat 0.0008 chicken 0.0006 agriculture 0.0005 avian 0.0004 broiler 0.0003 veterinary 0.0003 birds 0.0003 inspection 0.0003 pathogenic 0.0003 coffee coffee 0.0111 bags 0.0042 growers 0.0025 kg 0.0019 colombia 0.0018 brazil 0.0016 export 0.0014 exporters 0.0013 exports 0.0013 crop 0.0012 elections election 0.0519 elections 0.0342 polls 0.0339 voters 0.0315 party 0.0303 vote 0.0299 poll 0.0225 candidate 0.0202 campaign 0.0202 democratic 0.0198 sports soccer 0.0681 cup 0.0515 match 0.0441 matches 0.0408 played 0.0388 league 0.0386 beat 0.0301 game 0.0299 games 0.0284 team 0.0264 ◮Figure 13.7 Features with high mutual information scores for six Reuters-RCV1 classes. whether the document is in the class. As you might expect, keeping the informative terms and eliminating the non-informative ones tends to reduce noise and improve the classifier’s accuracy. Such an accuracy increase can be observed in Figure 13.8, which shows F1as a function of vocabulary size after feature selection for Reuters-RCV1.7 Comparing F1at 132,776 features (corresponding to selection of all features) and at 10–100 features, we see that MI feature selection increases F1by about 0.1 for the multinomial model and by more than 0.2 for the Bernoulli model. For the Bernoulli model, F1peaks early, at ten features selected. At that point, the Bernoulli model is better than the multinomial model. When basing a classification decision on only a few features, it is more robust to consider binary occurrence only. For the multinomial model (MI feature selection), the peak occurs later, at 100 features, and its effectiveness recovers somewhat at 7. We trained the classifiers on the first 100,000 documents and computed F1on the next 100,000. The graphs are averages over five classes.      13.5 Feature selection 275 ### # # # ## # ## # ### 110 100 1000 10000 0.0 0.2 0.4 0.6 0.8 number of features selected F1 measure ooooo o o o oo oo ooo x x xx x x xxxxx x xxx b b b bb bb b b bbbbbb # o x b multinomial, MI multinomial, chisquare multinomial, frequency binomial, MI ◮Figure 13.8 Effect of feature set size on accuracy for multinomial and Bernoulli models. the end when we use all features. The reason is that the multinomial takes the number of occurrences into account in parameter estimation and classification and therefore better exploits a larger number of features than the Bernoulli model. Regardless of the differences between the two methods, using a carefully selected subset of the features results in better effectiveness than using all features. 13.5.2 χ2Feature selection Another popular feature selection method is χ2. In statistics, the χ2test isχ2FEATURE SELECTION applied to test the independence of two events, where two events A and B are defined to be independent if P(AB) = P(A)P(B)or, equivalently, P(A|B) =INDEPENDENCE P(A)and P(B|A) = P(B). In feature selection, the two events are occurrence of the term and occurrence of the class. We then rank terms with respect to the following quantity: X2(D,t,c) = ∑ et∈{0,1} ∑ ec∈{0,1} (Netec−Eetec)2 Eetec (13.18)  ",13.5
2297,mir,mir-2297,13.5.1 Web Directories," 13.5.1 Web Directories The best and oldest example of a Web directory is Yahoo! [839], which is likely the most used searching tool. Other large Web directories include eBLAST, LookSmart, Magellan, and NewHoo. Some of them are hybrids, because they also provide searches in the whole Web. Most search engines also provide subject categories nowadays, including AltaVista Categories, AOL Netfind, Excite Channels, HotBot, Infoseek, Lycos Subjects, and WebCrawler Select. are specific to some areas. For example, there are Web sites focused on business, news,    Web directory eBLAST LookSmart Lycos Subjects Magellan NewHoo Netscape Search.com Snap Yahoo! URL www.eblast.com www.looksmart.com a2z. lycos. com www.mckinley.com www.newhoo.com www.netscape.com www.search.com www.snap.com www.yahoo.com Web sites 125 300 50 60 100 750 BROWSING 385 Categories 24 23 Table 13.3 URLs, Web pages indexed and categories (both in thousands) of some Web directories (beginning of 1998). Arts &amp; Humanities Automotive Business &amp; Economy Computers &amp; Internet Education Employment Entertainment &amp; Leisure Games Government Health &amp; Fitness Hobbies &amp; Interests Home Investing Kids &amp; Family Life &amp; Style Living Local News Oddities People Philosophy &amp; Religion Politics Recreation Reference Regional Science &amp; Technology Shopping &amp; Services Social Science Society &amp; Culture Sports Travel &amp; Tourism World Table 13.4 The first level categories in Web directories. and, in particular, research bibliography. Web directories are also called catalogs, yellow pages, or subject directories. Table 13.3 gives the URLs of the most important Web directories (not including the search engines already listed in section 13.4). Directories are hierarchical taxonomies that classify human knowledge. Table 13.4 shows the first level of the taxonomies used by Web directories (the number of first level categories ranges from 12 to 26). Some subcategories are also available in the main page of Web directories, adding around 70 more topics. The largest directory, Yahoo!, has close to one million pages classified, followed by LookSmart, which has about 24,000 categories in total. Yahoo! also offers    386 SEARCHING THE WEB 14 regional or country specialized directories in other languages including Chinese, Danish, French, German, Italian, Japanese, Korean, Norwegian, Spanish, and Swedish. In most cases, pages have to be submitted to the Web directory, where they are reviewed, and, if accepted, classified in one or more categories of the hierarchy. Although the taxonomy can be seen as a tree, there are cross references, so it is really a directed acyclic graph. The main advantage of this technique is that if we find what we are looking for, the answer will be useful in most cases. On the other hand, the main disadvantage is that the classification is not specialized enough and that not all Web pages are classified. The last problem becomes worse every day as the Web grows. The efforts to do automatic classification, by using clustering or other techniques, are very old. However, up to now, natural language processing is not 100% effective in extracting relevant terms from a document. Thus, classification is done manually by a limited number of people. This is a potential problem with users having a different notion of categories than the manmade categorization. Web directories also allow the user to perform a search on the taxonomy descriptors or in the Web pages pointed to by the taxonomy. In fact, as the number of classified Web pages is small, we can even afford to have a copy of all pages. In that case they must be updated frequently, which may pose performance and temporal validity problems. In addition, most Web directories also send the query to a search engine (through a strategic alliance) and allow the whole Web to be searched.  ",
2319,iir,iir-2319,13.5.2 Chi-square Feature selection,"    13.5 Feature selection 275 ### # # # ## # ## # ### 110 100 1000 10000 0.0 0.2 0.4 0.6 0.8 number of features selected F1 measure ooooo o o o oo oo ooo x x xx x x xxxxx x xxx b b b bb bb b b bbbbbb # o x b multinomial, MI multinomial, chisquare multinomial, frequency binomial, MI ◮Figure 13.8 Effect of feature set size on accuracy for multinomial and Bernoulli models. the end when we use all features. The reason is that the multinomial takes the number of occurrences into account in parameter estimation and classification and therefore better exploits a larger number of features than the Bernoulli model. Regardless of the differences between the two methods, using a carefully selected subset of the features results in better effectiveness than using all features. 13.5.2 χ2Feature selection Another popular feature selection method is χ2. In statistics, the χ2test isχ2FEATURE SELECTION applied to test the independence of two events, where two events A and B are defined to be independent if P(AB) = P(A)P(B)or, equivalently, P(A|B) =INDEPENDENCE P(A)and P(B|A) = P(B). In feature selection, the two events are occurrence of the term and occurrence of the class. We then rank terms with respect to the following quantity: X2(D,t,c) = ∑ et∈{0,1} ∑ ec∈{0,1} (Netec−Eetec)2 Eetec (13.18)      276 13 Text classification and Naive Bayes where etand ecare defined as in Equation (13.16). Nis the observed frequency in Dand Ethe expected frequency. For example, E11 is the expected frequency of tand coccurring together in a document assuming that term and class are independent. ✎Example 13.4: We first compute E11 for the data in Example 13.3: E11 =N×P(t)×P(c) = N×N11 +N10 N×N11 +N01 N =N×49 +141 N×49 +27652 N≈6.6 where Nis the total number of documents as before. We compute the other Eetecin the same way: epoultry =1epoultry =0 eexport =1N11 =49 E11 ≈6.6 N10 =27,652 E10 ≈27,694.4 eexport =0N01 =141 E01 ≈183.4 N00 =774,106 E00 ≈774,063.6 Plugging these values into Equation (13.18), we get a X2value of 284: X2(D,t,c) = ∑ et∈{0,1} ∑ ec∈{0,1} (Netec−Eetec)2 Eetec≈284 X2is a measure of how much expected counts Eand observed counts N deviate from each other. A high value of X2indicates that the hypothesis of independence, which implies that expected and observed counts are similar, is incorrect. In our example, X2≈284 &gt;10.83. Based on Table 13.6, we can reject the hypothesis that poultry and export are independent with only a 0.001 chance of being wrong.8Equivalently, we say that the outcome X2≈ 284 &gt;10.83 is statistically significant at the 0.001 level. If the two events are dependent, then the occurrence of the term makes the occurrence of the class more likely (or less likely), so it should be helpful as a feature. This is the rationale of χ2feature selection. An arithmetically simpler way of computing X2is the following: X2(D,t,c) = (N11 +N10 +N01 +N00)×(N11N00 −N10N01)2 (N11 +N01)×(N11 +N10)×(N10 +N00)×(N01 +N00) (13.19) This is equivalent to Equation (13.18) (Exercise 13.14). 8. We can make this inference because, if the two events are independent, then X2∼χ2, where χ2is the χ2distribution. See, for example, Rice (2006).      13.5 Feature selection 277 ◮Table 13.6 Critical values of the χ2distribution with one degree of freedom. For example, if the two events are independent, then P(X2&gt;6.63)&lt;0.01. So for X2&gt; 6.63 the assumption of independence can be rejected with 99% confidence. pχ2critical value 0.1 2.71 0.05 3.84 0.01 6.63 0.005 7.88 0.001 10.83 ✄Assessing χ2as a feature selection method From a statistical point of view, χ2feature selection is problematic. For a test with one degree of freedom, the so-called Yates correction should be used (see Section 13.7), which makes it harder to reach statistical significance. Also, whenever a statistical test is used multiple times, then the probability of getting at least one error increases. If 1,000 hypotheses are rejected, each with 0.05 error probability, then 0.05 ×1000 =50 calls of the test will be wrong on average. However, in text classification it rarely matters whether a few additional terms are added to the feature set or removed from it. Rather, the relative importance of features is important. As long as χ2feature selection only ranks features with respect to their usefulness and is not used to make statements about statistical dependence or independence of variables, we need not be overly concerned that it does not adhere strictly to statistical theory.  ",13.5
2298,mir,mir-2298,13.5.2 Combining Searching with Browsing," 13.5.2 Combining Searching with Browsing Usually, users either browse following hypertext links or they search a Web site (or the whole Web). Currently, in Web directories, a search can be reduced to a subtree of the taxonomy. However, the search may miss related pages that are not in that part of the taxonomy. Some search engines find similar pages using common words, but often this is not effective. WebGlimpse is a tool that tries to solve these problems by combining browsing with searching [539]. WebGlimpse attaches a small search box to the bottom of every HTML page, and allows the search to cover the neighborhood of that page or the whole site, without having to stop browsing. This is equivalent to following hypertext links that are constructed on the fly through a neighborhood search. WebGlimpse can be useful in building indices for personal Web pages or collections of favorite URLs. First, WebGlimpse indexes a Web site (or a collection of specific documents) and computes neighborhoods according to user specifications. As a result, WebGlimpse adds the search boxes to selected pages, collects remote pages that are relevant, and caches those pages locally. Later, the users can search in the neighborhood of a page using the search boxes. As the name suggests, WebGlimpse uses Glimpse as its search engine [540]. The neighborhood of a Web page is defined as the set of Web pages that are reachable by a path of hypertext links within a maximum predefined distance. This distance can be set differently for local and remote pages. For example, it    METASEARCHERS 387 can be unlimited locally, but be only three at any remote site. The neighborhood can also include all the subdirectories of the directory where the Web page is. The result is a graph of all the neighborhoods of the Web site or collection, and for each Web page, a file with all the Web pages in its neighborhood. When searching, any query in the whole index can be intersected with a neighborhood list, obtaining the relevant Web pages. A nice addition to WebGlimpse would be to visualize the neighborhoods. This problem is the topic of the next section.  ",
2320,iir,iir-2320,13.5.3 Frequency-based feature selection," 13.5.3 Frequency-based feature selection A third feature selection method is frequency-based feature selection, that is, selecting the terms that are most common in the class. Frequency can be either defined as document frequency (the number of documents in the class cthat contain the term t) or as collection frequency (the number of tokens of tthat occur in documents in c). Document frequency is more appropriate for the Bernoulli model, collection frequency for the multinomial model. Frequency-based feature selection selects some frequent terms that have no specific information about the class, for example, the days of the week (Monday,Tuesday, . ..), which are frequent across classes in newswire text. When many thousands of features are selected, then frequency-based feature selection often does well. Thus, if somewhat suboptimal accuracy is acceptable, then frequency-based feature selection can be a good alternative to more complex methods. However, Figure 13.8 is a case where frequency      278 13 Text classification and Naive Bayes based feature selection performs a lot worse than MI and χ2and should not be used.  ",13.5
2299,mir,mir-2299,13.5.3 Helpful Tools," 13.5.3 Helpful Tools There are many software tools to help browsing and searching. Some of them are add-ons to browsers, such as Alexa [10]. Alexa is a free Web navigation service that can be attached as a tool bar at the bottom of any browser and accompanies the user in his surfing. It provides useful information about the sites that are visited, including their popularity, speed of access, freshness, and overall quality (obtained from votes of Alexa users). Alexa also suggests related sites helping one's navigation. Another navigation service and searching guide is WebTaxi [805]. .. There are other tools that use VIsual metaphors, which can be broadly classified into two types: tools designed to visualize a subset of the Web and tools designed to visualize large answers. Both cases need to represent a large graph in a meaningful way. Specific commercial examples of tools to visualize Web subsets are Microsoft's SiteAnalyst (formerly from NetCarta), MAPA from Dynamic Diagrams, IBM's Mapuccino (formerly Web Cutter [527], shown in Figure 10.22), SurfSerf, Merzscope from Merzcom, CLEARweb, Astra SiteManager, WebAnalyzer from InContext, HistoryTree from SmartBrowser, etc. Non- commercial works include WebMap [220], Sitemap, Ptolomeaus, and many earlier research [234, 578, 564, 20]. We have not included more generic visualization software, where Web visualization is just a particular case, or other related visualization tools such as Web usage analysis [642, 294, 737]. Metaphors to visualize large answers are covered in Chapter 10\. Visual tools are not yet deployed in the whole Web because there is no standard way of communicating visualizers and search engines. One possible approach is to use a markup language based on XML (see Chapter 6), as proposed in [15]. 13.6 Metasearchers Metasearchers are Web servers that send a given query to several search engines, Web directories and other databases, collect the answers and unify them. Examples are Metacrawler [715] and SavvySearch [383, 223]. The main advantages of metasearchers are the ability to combine the results of many sources and the fact that the user can pose the same query to various sources through a single common interface. Metasearchers differ from each other in how ranking  ",
2321,iir,iir-2321,13.5.4 Feature selection for multiple classifiers," 13.5.4 Feature selection for multiple classifiers In an operational system with a large number of classifiers, it is desirable to select a single set of features instead of a different one for each classifier. One way of doing this is to compute the X2statistic for an n×2 table where the columns are occurrence and nonoccurrence of the term and each row corresponds to one of the classes. We can then select the kterms with the highest X2statistic as before. More commonly, feature selection statistics are first computed separately for each class on the two-class classification task cversus cand then combined. One combination method computes a single figure of merit for each feature, for example, by averaging the values A(t,c)for feature t, and then selects the kfeatures with highest figures of merit. Another frequently used combination method selects the top k/nfeatures for each of nclassifiers and then combines these nsets into one global feature set. Classification accuracy often decreases when selecting kcommon features for a system with nclassifiers as opposed to ndifferent sets of size k. But even if it does, the gain in efficiency owing to a common document representation may be worth the loss in accuracy. 13.5.5 Comparison of feature selection methods Mutual information and χ2represent rather different feature selection methods. The independence of term tand class cc ",13.5
2322,iir,iir-2322,13.5.5 Comparison of feature selection methods," 13.5.5 Comparison of feature selection methods Mutual information and χ2represent rather different feature selection methods. The independence of term tand class ccan sometimes be rejected with high confidence even if tcarries little information about membership of a document in c. This is particularly true for rare terms. If a term occurs once in a large collection and that one occurrence is in the poultry class, then this is statistically significant. But a single occurrence is not very informative according to the information-theoretic definition of information. Because its criterion is significance, χ2selects more rare terms (which are often less reliable indicators) than mutual information. But the selection criterion of mutual information also does not necessarily select the terms that maximize classification accuracy. Despite the differences between the two methods, the classification accuracy of feature sets selected with χ2and MI does not seem to differ systematically. In most text classification problems, there are a few strong indicators and many weak indicators. As long as all strong indicators and a large number of weak indicators are selected, accuracy is expected to be good. Both methods do this. Figure 13.8 compares MI and χ2feature selection for the multinomial model.       ",13.5
2323,iir,iir-2323,13.6 Evaluation of text classification," 13.6 Evaluation of text classification 279 Peak effectiveness is virtually the same for both methods. χ2reaches this peak later, at 300 features, probably because the rare, but highly significant features it selects initially do not cover all documents in the class. However, features selected later (in the range of 100–300) are of better quality than those selected by MI. All three methods – MI, χ2and frequency based – are greedy methods.GREEDY FEATURE SELECTION They may select features that contribute no incremental information over previously selected features. In Figure 13.7,kong is selected as the seventh term even though it is highly correlated with previously selected hong and therefore redundant. Although such redundancy can negatively impact accuracy, non-greedy methods (see Section 13.7 for references) are rarely used in text classification due to their computational cost. ?Exercise 13.5 Consider the following frequencies for the class coffee for four terms in the first 100,000 documents of Reuters-RCV1: term N00 N01 N10 N11 brazil 98,012 102 1835 51 council 96,322 133 3525 20 producers 98,524 119 1118 34 roasted 99,824 143 23 10 Select two of these four terms based on (i) χ2, (ii) mutual information, (iii) frequency. 13.6 Evaluation of text classification ] Historically, the classic Reuters-21578 collection was the main benchmark for text classification evaluation. This is a collection of 21,578 newswire articles, originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE text classification system. It is much smaller than and predates the Reuters-RCV1 collection discussed in Chapter 4(page 69). The articles are assigned classes from a set of 118 topic categories. A document may be assigned several classes or none, but the commonest case is single assignment (documents with at least one class received an average of 1.24 classes). The standard approach to this any-of problem (Chapter 14, page 306) is to learn 118 two-class classifiers, one for each class, where the two-class classifier for class cis the classifier for the twoTWO-CLASS CLASSIFIER classes cand its complement c. For each of these classifiers, we can measure recall, precision, and accuracy. In recent work, people almost invariably use the ModApte split, which includes only documents that were viewed and assessed by a human indexer,      280 13 Text classification and Naive Bayes ◮Table 13.7 The ten largest classes in the Reuters-21578 collection with number of documents in training and test sets. class # train # testclass # train # test earn 2877 1087 trade 369 119 acquisitions 1650 179 interest 347 131 money-fx 538 179 ship 197 89 grain 433 149 wheat 212 71 crude 389 189 corn 182 56 and comprises 9,603 training documents and 3,299 test documents. The distribution of documents in classes is very uneven, and some work evaluates systems on only documents in the ten largest classes. They are listed in Table 13.7. A typical document with topics is shown in Figure 13.9. In Section 13.1, we stated as our goal in text classification the minimization of classification error on test data. Classification error is 1.0 minus classification accuracy, the proportion of correct decisions, a measure we introduced in Section 8.3 (page 155). This measure is appropriate if the percentage of documents in the class is high, perhaps 10% to 20% and higher. But as we discussed in Section 8.3, accuracy is not a good measure for “small” classes because always saying no, a strategy that defeats the purpose of building a classifier, will achieve high accuracy. The always-no classifier is 99% accurate for a class with relative frequency 1%. For small classes, precision, recall and F1are better measures. We will use effectiveness as a generic term for measures that evaluate theEFFECTIVENESS quality of classification decisions, including precision, recall, F1, and accuracy. Performance refers to the computational efficiency of classification and IR systems in this book. However, many researchers mean effectiveness, not efficiency of text classification when they use the term performance. When we process a collection with several two-class classifiers (such as Reuters-21578 with its 118 classes), we often want to compute a single aggregate measure that combines the measures for individual classifiers. There are two methods for doing this. Macroaveraging computes a simple aver- MACROAVERAGING age over classes. Microaveraging pools per-document decisions across classes,MICROAVERAGING and then computes an effectiveness measure on the pooled contingency table. Table 13.8 gives an example. The differences between the two methods can be large. Macroaveraging gives equal weight to each class, whereas microaveraging gives equal weight to each per-document classification decision. Because the F1measure ignores true negatives and its magnitude is mostly determined by the number of true positives, large classes dominate small classes in microaveraging. In the example, microaveraged precision (0.83) is much closer to the precision of   13.6 Evaluation of text classification 281 &lt;REUTERS TOPICS=’’YES’’ LEWISSPLIT=’’TRAIN’’ CGISPLIT=’’TRAINING-SET’’ OLDID=’’12981’’ NEWID=’’798’’&gt; &lt;DATE&gt; 2-MAR-1987 16:51:43.42&lt;/DATE&gt; &lt;TOPICS&gt;&lt;D&gt;livestock&lt;/D&gt;&lt;D&gt;hog&lt;/D&gt;&lt;/TOPICS&gt; &lt;TITLE&gt;AMERICAN PORK CONGRESS KICKS OFF TOMORROW&lt;/TITLE&gt; &lt;DATELINE&gt; CHICAGO, March 2 - &lt;/DATELINE&gt;&lt;BODY&gt;The American Pork Congress kicks off tomorrow, March 3, in Indianapolis with 160 of the nations pork producers from 44 member states determining industry positions on a number of issues, according to the National Pork Producers Council, NPPC. Delegates to the three day Congress will be considering 26 resolutions concerning various issues, including the future direction of farm policy and the tax law as it applies to the agriculture sector. The delegates will also debate whether to endorse concepts of a national PRV (pseudorabies virus) control and eradication program, the NPPC said. A large trade show, in conjunction with the congress, will feature the latest in technology in all areas of the industry, the NPPC added. Reuter \&amp;\\#3;&lt;/BODY&gt;&lt;/TEXT&gt;&lt;/REUTERS&gt; ◮Figure 13.9 A sample document from the Reuters-21578 collection. c2(0.9) than to the precision of c1(0.5) because c2is five times larger than c1. Microaveraged results are therefore really a measure of effectiveness on the large classes in a test collection. To get a sense of effectiveness on small classes, you should compute macroaveraged results. In one-of classification (Section 14.5, page 306), microaveraged F1is the same as accuracy (Exercise 13.6). Table 13.9 gives microaveraged and macroaveraged effectiveness of Naive Bayes for the ModApte split of Reuters-21578. To give a sense of the relative effectiveness of NB, we compare it with linear SVMs (rightmost column; see Chapter 15), one of the most effective classifiers, but also one that is more expensive to train than NB. NB has a microaveraged F1of 80%, which is 9% less than the SVM (89%), a 10% relative decrease (row “micro-avg-L (90 classes)”). So there is a surprisingly small effectiveness penalty for its simplicity and efficiency. However, on small classes, some of which only have on the order of ten positive examples in the training set, NB does much worse. Its macroaveraged F1is 13% below the SVM, a 22% relative decrease (row “macro-avg (90 classes)”). The table also compares NB with the other classifiers we cover in this book:      282 13 Text classification and Naive Bayes ◮Table 13.8 Macro- and microaveraging. “Truth” is the true class and “call” the decision of the classifier. In this example, macroaveraged precision is [10/(10 +10) + 90/(10 +90)]/2 = (0.5 +0.9)/2 =0.7. Microaveraged precision is 100/(100 +20)≈ 0.83. class 1 truth: truth: yes no call: yes 10 10 call: no 10 970 class 2 truth: truth: yes no call: yes 90 10 call: no 10 890 pooled table truth: truth: yes no call: yes 100 20 call: no 20 1860 ◮Table 13.9 Text classification effectiveness numbers on Reuters-21578 for F1(in percent). Results from Li and Yang (2003) (a), Joachims (1998) (b: kNN) and Dumais et al. (1998) (b: NB, Rocchio, trees, SVM). (a) NB Rocchio kNN SVM micro-avg-L (90 classes) 80 85 86 89 macro-avg (90 classes) 47 59 60 60 (b) NB Rocchio kNN trees SVM earn 96 93 97 98 98 acq 88 65 92 90 94 money-fx 57 47 78 66 75 grain 79 68 82 85 95 crude 80 70 86 85 89 trade 64 65 77 73 76 interest 65 63 74 67 78 ship 85 49 79 74 86 wheat 70 69 77 93 92 corn 65 48 78 92 90 micro-avg (top 10) 82 65 82 88 92 micro-avg-D (118 classes) 75 62 n/a n/a 87 Rocchio and kNN. In addition, we give numbers for decision trees, an impor- DECISION TREES tant classification method we do not cover. The bottom part of the table shows that there is considerable variation from class to class. For instance, NB beats kNN on ship, but is much worse on money-fx. Comparing parts (a) and (b) of the table, one is struck by the degree to which the cited papers’ results differ. This is partly due to the fact that the numbers in (b) are break-even scores (cf. page 161) averaged over 118 classes, whereas the numbers in (a) are true F1scores (computed without any know   13.6 Evaluation of text classification 283 ledge of the test set) averaged over ninety classes. This is unfortunately typical of what happens when comparing different results in text classification: There are often differences in the experimental setup or the evaluation that complicate the interpretation of the results. These and other results have shown that the average effectiveness of NB is uncompetitive with classifiers like SVMs when trained and tested on independent and identically distributed (i.i.d.) data, that is, uniform data with all the good properties of statistical sampling. However, these differences may often be invisible or even reverse themselves when working in the real world where, usually, the training sample is drawn from a subset of the data to which the classifier will be applied, the nature of the data drifts over time rather than being stationary (the problem of concept drift we mentioned on page 269), and there may well be errors in the data (among other problems). Many practitioners have had the experience of being unable to build a fancy classifier for a certain problem that consistently performs better than NB. Our conclusion from the results in Table 13.9 is that, although most researchers believe that an SVM is better than kNN and kNN better than NB, the ranking of classifiers ultimately depends on the class, the document collection, and the experimental setup. In text classification, there is always more to know than simply which machine learning algorithm was used, as we further discuss in Section 15.3 (page 334). When performing evaluations like the one in Table 13.9, it is important to maintain a strict separation between the training set and the test set. We can easily make correct classification decisions on the test set by using information we have gleaned from the test set, such as the fact that a particular term is a good predictor in the test set (even though this is not the case in the training set). A more subtle example of using knowledge about the test set is to try a large number of values of a parameter (e.g., the number of selected features) and select the value that is best for the test set. As a rule, accuracy on new data – the type of data we will encounter when we use the classifier in an application – will be much lower than accuracy on a test set that the classifier has been tuned for. We discussed the same problem in ad hoc retrieval in Section 8.1 (page 153). In a clean statistical text classification experiment, you should never run any program on or even look at the test set while developing a text classification system. Instead, set aside a development set for testing while you develop your method. When such a set serves the primary purpose of finding a good value for a parameter, for example, the number of selected features, then it is also called held-out data. Train the classifier on the rest of the training set with different parameter values, and then select the value that gives best results on the held-out part of the training set. Ideally, at the very end, when all parameters have been set and the method is fully specified, you run one final experiment on the test set and publish the results. Because no informa      284 13 Text classification and Naive Bayes ◮Table 13.10 Data for parameter estimation exercise. docID words in document in c=China? training set 1 Taipei Taiwan yes 2 Macao Taiwan Shanghai yes 3 Japan Sapporo no 4 Sapporo Osaka Taiwan no test set 5 Taiwan Taiwan Sapporo ? tion about the test set was used in developing the classifier, the results of this experiment should be indicative of actual performance in practice. This ideal often cannot be met; researchers tend to evaluate several systems on the same test set over a period of several years. But it is nevertheless highly important to not look at the test data and to run systems on it as sparingly as possible. Beginners often violate this rule, and their results lose validity because they have implicitly tuned their system to the test data simply by running many variant systems and keeping the tweaks to the system that worked best on the test set. ?Exercise 13.6 [⋆⋆] Assume a situation where every document in the test collection has been assigned exactly one class, and that a classifier also assigns exactly one class to each document. This setup is called one-of classification (Section 14.5, page 306). Show that in one-of classification (i) the total number of false positive decisions equals the total number of false negative decisions and (ii) microaveraged F1and accuracy are identical. Exercise 13.7 The class priors in Figure 13.2 are computed as the fraction of documents in the class as opposed to the fraction of tokens in the class. Why? Exercise 13.8 The function APPLYMULTINOMIALNB in Figure 13.2 has time complexity Θ(La+ |C|La). How would you modify the function so that its time complexity is Θ(La+ |C|Ma)? Exercise 13.9 Based on the data in Table 13.10, (i) estimate a multinomial Naive Bayes classifier, (ii) apply the classifier to the test document, (iii) estimate a Bernoulli NB classifier, (iv) apply the classifier to the test document. You need not estimate parameters that you don’t need for classifying the test document. Exercise 13.10 Your task is to classify words as English or not English. Words are generated by a source with the following distribution:   13.6 Evaluation of text classification 285 event word English? probability 1 ozb no 4/9 2 uzu no 4/9 3 zoo yes 1/18 4 bun yes 1/18 (i) Compute the parameters (priors and conditionals) of a multinomial NB classifier that uses the letters b, n, o, u, and z as features. Assume a training set that reflects the probability distribution of the source perfectly. Make the same independence assumptions that are usually made for a multinomial classifier that uses terms as features for text classification. Compute parameters using smoothing, in which computed-zero probabilities are smoothed into probability 0.01, and computed- nonzero probabilities are untouched. (This simplistic smoothing may cause P(A) + P(A)&gt;1. Solutions are not required to correct this.) (ii) How does the classifier classify the word zoo? (iii) Classify the word zoo using a multinomial classifier as in part (i), but do not make the assumption of positional independence. That is, estimate separate parameters for each position in a word. You only need to compute the parameters you need for classifying zoo. Exercise 13.11 What are the values of I(Ut;Cc)and X2(D,t,c)if term and class are completely independent? What are the values if they are completely dependent? Exercise 13.12 The feature selection method in Equation (13.16) is most appropriate for the Bernoulli model. Why? How could one modify it for the multinomial model? Exercise 13.13 Features can also be selected according toinformation gain (IG), which is defined as:INFORMATION GAIN IG(D,t,c) = H(pD)−∑ x∈{Dt+,Dt−} |x| |D|H(px) where His entropy, Dis the training set, and Dt+, and Dt−are the subset of Dwith term t, and the subset of Dwithout term t, respectively. pAis the class distribution in (sub)collection A, e.g., pA(c) = 0.25, pA(c) = 0.75 if a quarter of the documents in Aare in class c. Show that mutual information and information gain are equivalent. Exercise 13.14 Show that the two X2formulas (Equations (13.18) and (13.19)) are equivalent. Exercise 13.15 In the χ2example on page 276 we have |N11 −E11|=|N10 −E10|=|N01 −E01|= |N00 −E00|. Show that this holds in general. Exercise 13.16 χ2and mutual information do not distinguish between positively and negatively correlated features. Because most good text classification features are positively correlated (i.e., they occur more often in cthan in c), one may want to explicitly rule out the selection of negative indicators. How would you do this?     286 13 Text classification and Naive Bayes  ",13.6
2300,mir,mir-2300,13.6 Metasearchers," 13.6 Metasearchers Metasearchers are Web servers that send a given query to several search engines, Web directories and other databases, collect the answers and unify them. Examples are Metacrawler [715] and SavvySearch [383, 223]. The main advantages of metasearchers are the ability to combine the results of many sources and the fact that the user can pose the same query to various sources through a single common interface. Metasearchers differ from each other in how ranking    388 SEARCHING THE WEB Metasearcher Cyber 411 Dogpile Highway61 Inference Find Mamma MetaCrawler MetaFind MetaMiner MetaSearch SavvySearch URL www.cyber411.com www.dogpile.com www.highway61.com www.infind.com www.mamma.com www.metacrawler.com www.metafind.com www.miner.uol.com.br www.metasearch.com savvy.cs.colostate.edu:2000 Sources used 14 25 5 6 7 7 7 13 &gt;13 Table 13.5 URLs of metasearchers and number of sources that they use (October 1998). is performed in the unified result (in some cases no ranking is done), and how well they translate the user query to the specific query language of each search engine or Web directory (the query language common to all of them could be small). Table 13.5 shows the URLs of the main metasearch engines as well as the number of search engines, Web directories and other databases that they search. Metasearchers can also run on the client, for example, Copernic, EchoSearch, WebFerret, WebCompass, and WebSeeker. There are others that search several sources and show the different answers in separate windows, such as A1l40ne, OneSeek, Proteus, and Search Spaniel. The advantages of metasearchers are that the results can be sorted by different attributes such as host, keyword, date, etc; which can be more informative than the output of a single search engine. Therefore browsing the results should be simpler. On the other hand, the result is not necessarily all the Web pages matching the query, as the number of results per search engine retrieved by the metasearcher is limited (it can be changed by the user, but there is an upper limit). Nevertheless, pages returned by more than one search engine should be more relevant. We expect that new metasearchers will do better ranking. A first step in this direction is the NEC Research Institute metasearch engine, Inquirus [488, 489]. The main difference is that Inquirus actually downloads and analyzes each Web page obtained and then displays each page, highlighting the places where the query terms were found. The results are displayed as soon as they are available in a progressive manner, otherwise the waiting time would be too long. This technique also allows non-existent pages or pages that have changed and do not contain the query any more to be discarded, and, more important, provides for better ranking than normal search engines. On the other hand, this metasearcher is not available to the general public.    FINDING THE NEEDLE IN THE HAYSTACK 389 Measure N umber of words Number of operators Repetitions of each query Queries per user session Screens per query Average value 2.35 0.41 3.97 2.02 1.39 Range o to 393 o to 958 1-1. 5 million 1-173,325 1-78,496 Table 13.6 Queries on the Web: average values. The use of metasearchers is justified by coverage studies that show that a small percentage of Web pages are in all search engines [91]. In fact, fewer than 1 % of the Web pages indexed by AltaVista, HotBot, Excite, and Infoseek are in all of those search engines. This fact is quite surprising and has not been explained (yet). Metasearchers for specific topics can be considered as software agents and are covered in section 13.8.2.  ",
2301,mir,mir-2301,13.7 Finding the Needle in the Haystack," 13.7 Finding the Needle in the Haystack 13.7.1 User Problems We have already glanced at some of the problems faced by the user when interacting with the query interfaces currently provided by search engines. First, the user does not exactly understand the meaning of searching using a set of words, as discussed in Chapter 10\. Second, the user may get unexpected answers because he is not aware of the logical view of the text adopted by the system. An example is the use of uppercase letters when the search engine is not case sensitive. Hence, a word like 'Bank' loses part of its semantics if we search for 'bank.' Simple experiments also show that due to typos or variations of a word, even if correctly capitalized, 10-20% of the matches can be lost. Similarly, forei ",
2324,iir,iir-2324,13.7 References and further reading," 13.7 References and further reading General introductions to statistical classification and machine learning can be found in (Hastie et al. 2001), (Mitchell 1997), and (Duda et al. 2000), including many important methods (e.g., decision trees and boosting) that we do not cover. A comprehensive review of text classification methods and results is (Sebastiani 2002). Manning and Schütze (1999, Chapter 16) give an accessible introduction to text classification with coverage of decision trees, perceptrons and maximum entropy models. More information on the superlinear time complexity of learning methods that are more accurate than Naive Bayes can be found in (Perkins et al. 2003) and (Joachims 2006a). Maron and Kuhns (1960) described one of the first NB text classifiers. Lewis (1998) focuses on the history of NB classification. Bernoulli and multinomial models and their accuracy for different collections are discussed by McCallum and Nigam (1998). Eyheramendy et al. (2003) present additional NB models. Domingos and Pazzani (1997), Friedman (1997), and Hand and Yu (2001) analyze why NB performs well although its probability estimates are poor. The first paper also discusses NB’s optimality when the independence assumptions are true of the data. Pavlov et al. (2004) propose a modified document representation that partially addresses the inappropriateness of the independence assumptions. Bennett (2000) attributes the tendency of NB probability estimates to be close to either 0 or 1 to the effect of document length. Ng and Jordan (2001) show that NB is sometimes (although rarely) superior to discriminative methods because it more quickly reaches its optimal error rate. The basic NB model presented in this chapter can be tuned for better effectiveness (Rennie et al. 2003;Kołcz and Yih 2007). The problem of concept drift and other reasons why state-of-the-art classifiers do not always excel in practice are discussed by Forman (2006) and Hand (2006). Early uses of mutual information and χ2for feature selection in text classification are Lewis and Ringuette (1994) and Schütze et al. (1995), respectively. Yang and Pedersen (1997) review feature selection methods and their impact on classification effectiveness. They find that pointwise mutual infor- POINTWISE MUTUAL INFORMATION mation is not competitive with other methods. Yang and Pedersen refer to expected mutual information (Equation (13.16)) as information gain (see Exercise 13.13, page 285). (Snedecor and Cochran 1989) is a good reference for the χ2test in statistics, including the Yates’ correction for continuity for 2 ×2 tables. Dunning (1993) discusses problems of the χ2test when counts are small. Nongreedy feature selection techniques are described by Hastie et al. (2001). Cohen (1995) discusses the pitfalls of using multiple significance tests and methods to avoid them. Forman (2004) evaluates different methods for feature selection for multiple classifiers. David D. Lewis defines the ModApte split at www.daviddlewis.com/resources/testcollections/reuters21578/readme based on Apté et al. (1994). Lewis (1995) describes utility measures for the   13.7 References and further reading 287 evaluation of text classification systems. Yang and Liu (1999) employ significance tests in the evaluation of text classification methods. Lewis et al. (2004) find that SVMs (Chapter 15) perform better on ReutersRCV1 than kNN and Rocchio (Chapter 14).  ",13.7
2302,mir,mir-2302,13.7.1 User Problems," 13.7.1 User Problems We have already glanced at some of the problems faced by the user when interacting with the query interfaces currently provided by search engines. First, the user does not exactly understand the meaning of searching using a set of words, as discussed in Chapter 10\. Second, the user may get unexpected answers because he is not aware of the logical view of the text adopted by the system. An example is the use of uppercase letters when the search engine is not case sensitive. Hence, a word like 'Bank' loses part of its semantics if we search for 'bank.' Simple experiments also show that due to typos or variations of a word, even if correctly capitalized, 10-20% of the matches can be lost. Similarly, foreign names or words that are difficult to spell may appear incorrectly which may result in a loss of up to 50% of the relevant answers, as mentioned in section 13.2. Another problem is that most users have trouble with Boolean logic. In natural language, sometimes we use 'and' and 'or' with different meaning depending on the context. For example, when choosing between two things, we use an exclusive 'or,' which does not match the Boolean interpretation. Because of this, several studies show that around 80% of the queries do not use any Boolean or other operation. For these reasons many people have trouble using command query languages, and query forms should clearly specify which words must or must not be contained in a document that belongs to the answer. There are a few surveys and analyses of query logs with respect to the usage of search engines [647, 403, 728]. The latter reference is based on 285 million user sessions containing 575 million queries. Table 13.6 gives the main results    390 SEARCHING THE WEB of that study, carried out in September 1998\. Some of the strange results might be due to queries done by mechanized search agents. The number of queries submitted per day to AltaVista is over 13 million. Users select a search engine mainly based on ease of use, speed, coverage, relevance of the answer, and habit. The main purposes are research, leisure, business, and education. The main problems found are that novice users do not know how to start and lack the general knowledge that would help in finding better answers. Other problems are that search engines are slow, that the answer is too large, not very relevant, and not always up to date. Also, most people do not care about advertising, which is one of the main sources of funding for search engines. When searching, 25% of the users use a single keyword, and on average their queries have only two or three terms. In addition, about 15% of the users restrict the search to a predefined topic and most of them (nearly 80%) do not modify the query. In addition, most users (about 85%) only look at the first screen with results and 64% of the queries are unique. Also, many words appear in the same sentence, suggesting that proximity search should be used. There are also studies about users' demographics and software and hardware used.  ",
2303,mir,mir-2303,13.7.2 Some Examples," 13.7.2 Some Examples Now we give a couple of search examples. One problem with full-text retrieval is that although many queries can be effective, many others are a total deception. The main reason is that a set of words does not capture all the semantics of a document. There is too much contextual information (that can be explicit or even implicit) lost at indexing time, which is essential for proper understanding. For example, suppose that we want to learn an oriental game such as Shogi or Go. For the first case, searching for Shogi will quickly give us good Web pages where we can find what Shogi is (a variant of chess) and its rules. However, for Go the task is complicated, because unlike Shogi, Go is not a unique word in English (in particular, because uppercase letters are converted to lowercase letters, see Chapter 7). The problem of having more than one meaning for a word is called polysemy. We can add more terms to the query, such as game and Japanese but still we are out of luck, as the pages found are almost all about Japanese games written in English where the common verb go is used. Another common problem comes from synonyms. If we are searching for a certain word, but a relevant page uses a synonym, we will not find it. The following example (taken from [152]) better explains the polysemy problem, where the ambiguity comes from the same language. Suppose that we want to find the running speed of the jaguar, a big South American cat. A first naive search in AltaVista would be jaguar speed. The results are pages that talk about the Jaguar car, an Atari video game, a US football team, a local network server, etc. The first page about the animal is ranked 183 and is a fable, without information about the speed. In a second try, we add the term cat. The answers are about the Clans Nova Cat and Smoke Jaguar, LMG Enterprises, fine cars, etc. Only the page ranked    FINDING THE NEEDLE IN THE HAYSTACK 391 25 has some information on jaguars but not the speed. Suppose we try Yahoo!. We look at 'Science: Biology: Zoology: Animals: Cats: Wild_Cats' and 'Science:Biology:Animal_Behavior.' No information about jagu8!s there.  ",
2304,mir,mir-2304,13.7.3 Teaching the User," 13.7.3 Teaching the User Interfaces are slowly improving in assisting the user with the task of acquiring a better grasp of what Web pages are being retrieved. Query forms must specify clearly if one or all the words must be in a page, which words should not be in a page, etc., without using a written Boolean query language. Second, users should try to give as many terms as possible, in particular terms that must be or should not be in the pages. In particular, a user should include all possible synonyms of a word. If the user can restrict the search to a field (for example, the page title) or limit some attribute (date, country), this will certainly reduce the size of the answer. In case of doubt, the user should remember to look at the help information provided by the search engine. If he cannot find where one of the relevant terms is in a page, he can use the Find option of the browser. Even if we are able to pose a good query, the answer can still be quite large. Considering that the visual tools mentioned before are not yet available for the general public, the user must learn from experience. There are many strategies for quickly finding relevant answers. If the user is looking for an institution, he can always try to guess the corresponding URL by using the www prefix followed by a guessed institution acronym or brief name and ending with a top level domain (country code or com, edu, org, gOY for the US). If this does not work, the user can search the institution name in a Web directory. If we are looking for work related to a specific topic, a possible strategy is: (1) select an article relevant to the topic, if possible with non-common author surnames or title keywords (if it is not available, try any bibliographic database or a Web directory search for a first reference); and (2) use a search engine to find all Web pages that have all those surnames and keywords. Many of the results are likely to be relevant, because we can find: (a) newer papers that reference the initial reference, (b) personal Web pages of the authors, and most important, (c) pages about the topic that already contain many relevant references. This strategy can be iterated by changing the reference used as better references appear during the search. As mentioned at the beginning of this chapter, the Web poses so many problems, that it is easier and more effective to teach the user how to properly profit from search engines and Web directories, rather than trying to guess what the user really wants. Given that the coverage of the search engines is low, use several engines or a metasearcher. Also, remember that you have to evaluate the quality of each answer, even if it appears to be relevant. Remember that anybody can publish in the Web, and that does not mean that the data is correct or still valid. The lessons learned in the examples shown above are: (1) search engines still return too much hay together with the needle; and (2) Web directories do not have enough depth to find the needle. So, we can usethe following rules of thumb:    392 SEARCHING THE WEB • Specific queries Look in an encyclopedia, that is the reason that they exist. In other words, do not forget libraries. • Broad queries Use Web directories to find good starting points. • Vague queries Use Web search engines and improve the query formulation based on relevant answers.  ",
2305,mir,mir-2305,13.8 Searching using Hyperlinks," 13.8 Searching using Hyperlinks In this section we cover other paradigms to search the Web, which are based on exploiting its hyper links. They include Web query languages and dynamic searching. These ideas are still not widely used due to several reasons, including performance limitations and lack of commercial products. 13.8.1 Web Query Languages Up to this point, queries have been based on the content of each page. However, queries can also include the link structure connecting Web pages. For example, we would like to search for all the Web pages that contain at least one image and are reachable from a given site following  ",
2306,mir,mir-2306,13.8.1 Web Query Languages," 13.8.1 Web Query Languages Up to this point, queries have been based on the content of each page. However, queries can also include the link structure connecting Web pages. For example, we would like to search for all the Web pages that contain at least one image and are reachable from a given site following at most three links. To be able to pose this type of query, different data models have been used. The most important are a labeled graph model to represent Web pages (nodes) and hyperlinks (edges) between Web pages, and a semi-structured data model to represent the content of Web pages. In the latter model, the data schema is not usually known, may change over time, may be large and descriptive, etc. [2, 129]. Although some models and languages for querying hypertext were proposed before the Web appeared [563, 72, 184]' the first generation of Web query languages were aimed at combining content with structure (see also Chapter 4). These languages combine patterns that appear within the documents with graph queries describing link structure (using path regular expressions). They include W3QL [450], WebSQL [556, 33], WebLog [476], and WQL [511]. The second generation of languages, called Web data manipulation languages, maintain the emphasis on semi-structured data. However, they extend the previous languages by providing access to the structure of Web pages (the model also includes the internal structure) and by allowing the creation of new structures as a result of a query. Languages in this category include STRUQL [253], FLORID [373], and WebOQL [32]. All the languages mentioned are meant to be used by programs, not final users. Nevertheless, there are some examples of query interfaces for these languages. Web query languages have been extended to other Web tasks, such as extracting and integrating information from Web pages, and constructing and restructuring Web sites. More details about Web query languages can be found in the excellent survey by Florescu, Levy, and Mendelzon [258].    TRENDS AND RESEARCH ISSUES 393  ",
2307,mir,mir-2307,13.8.2 Dynamic Search and Software Agents," 13.8.2 Dynamic Search and Software Agents Dynamic search in the Web is equivalent to sequential text searching. The idea is to use an online search to discover relevant information by following links. The main advantage is that you are searching in the current structure of the Web, and not in what is stored in the index of a search engine. While this approach is slow for the entire Web, it might be used. in small and dynamic subsets of the Web. The first heuristic devised was the fish search [113], which exploits the intuition that relevant documents often have neighbors that are relevant. Hence, the search is guided by following links in relevant documents. This was improved by shark search [366], which does a better relevance assessment of neighboring pages. This algorithm has been embedded in Mapuccino (see section 13.5.3), and Figure 10.22 shows a Web subset generated by this type of search. The main idea of these algorithms is to follow links in some priority, starting from a single page and a given query. At each step, the page with highest priority is analyzed. If it is found to be relevant, a heuristic decides to follow or not to follow the links on that page. If so, new pages are added to the priority list in the appropriate positions. Related work includes software agents for searching specific information on the Web [602, 477]. This implies dealing with heterogeneous sources of information which have to be combined. Important issues in this case are how to determine relevant sources (see also Chapters 9 and 15, as well as section 10.4.4) and and how to merge the results retrieved (the fusion problem). Examples are shopping robots such as Jango [401], Junglee [180], and Express [241]. 13.9 Trends and Research Issues  ",
2308,mir,mir-2308,13.9 Trends and Research Issues," 13.9 Trends and Research Issues The future of the Web might surprise us, considering that its massive use started less than five years ago. There are many distinct trends and each one opens up new and particular research problems. What follows is a compilation of the major trends as we have perceived them. • Modeling: Special IR models tailored for the Web are needed [308, 155, 652]. As we have seen, Web user queries are different. We also have the pull/push dichotomy: Will we search for information or will the information reach us? In both cases we need better search paradigms and better information filtering [782]. • Querying: Further work on combining structure and content in the queries is needed as well as new visual metaphors to pose those queries and visualize the answers [44]. Future query languages may include concept-based search and natural language processing, as well as searching by example (this implies document clustering and categorization on the Web [810,120,157]). • Distributed architectures: New distributed schemes to traverse and search the Web must be devised to cope with its growth. This will have an impact on current crawling and indexing techniques, as well as caching    394 SEARCHING THE WEB techniques for the Web. Which will be the bottleneck in the future? Server capacity or network bandwidth? • Ranking: Better ranking schemes are needed, exploiting both content and structure (internal to a page and hyperlinks); in particular, combining and comparing query-dependent and independent techniques. One problem related to advertisements is that search engines may rank some pages higher due to reasons that are not based on the real relevance of a page (this is called the search engine persuasion problem in [543]). • Indexing: Which is the best logical view for the text? What should be indexed? How to exploit better text compression schemes to achieve fast searching and get lower network traffic? How to compress efficiently word lists, URL tables, etc. and update them without significant run-time penalty? Many implementation details must be improved. • Dynamic pages: A large number of Web pages are created on demand and current techniques are not able to search on those dynamic pages. This is called the hidden Web. • Duplicated data: Better mechanisms to detect and eliminate repeated Web pages (or pages that are syntactically very similar) are needed. Initial approaches are based on resemblance measures using document fingerprints [121, 120]. This is related to an important problem in databases: finding similar objects. • Multimedia: Searching for non-textual objects will gain importance in the near future. There are already some research results in the literature [579, 80, 136]. • User interfaces: Better user interfaces are clearly needed. The output should also be improved, for example allowing better extraction of the main content of a page or the formulation of content-based queries [766]. • Browsing: More tools will appear, exploiting links, popularity of Web pages, content similarity, collaboration, 3D, and virtual reality [384, 638, 385, 421]. An important trend would be to unify further searching with browsing. An important issue to be settled in the future is a standard protocol to query search engines. One proposal for such a protocol is STARTS [316], which could allow us to choose the best sources for querying, evaluate the query at these sources, and merge the query results. This protocol would make it easier to build metasearchers, but at the same time that is one of the reasons for not having a standard. In that way, metasearchers cannot profit from the work done by search engines and Web directories. This is a particular case of the federated searching problem from heterogeneous sources as it is called in the database community [656]. This is a problem already studied in the case of the Web, including discovery and ranking of sources [161, 845, 319]. These issues are also very important for digital libraries [649] (see also Chapter 15) and visualization issues [15]. A related topic is metadata standards for the Web (see Chapter 6)    BIBLIOGRAPHIC DISCUSSION 395 and their limitations [544]. XML helps [436, 213, 306], but semantic integration is still needed. Hyperlinks can also be used to infer information about the Web. Although this is not exactly searching the Web, this is an important trend called Web mining. Traditionally, Web mining had been focused on text mining, that is, extracting information from Web pages. However, the hyperlink structure can be exploited to obtain useful information. For example, the ParaSite system [736] uses hyperlink information to find pages that have moved, related pages, and personal Web pages. HITS, already mentioned in Section 13.4.4, has also been used to find communities and similar pages [444, 298]. Other results on exploiting hyperlink structure can be found in [639, 543, 154]. Further improvements in this problem include Web document clustering [810, 120, 162] (already mentioned), connectivity services (for example, asking which Web pages point to a given page [92]), automatic link generation [320], extracting information [100, 115], etc. Another trend is intranet applications. Many companies do not want their private networks to be public. However, for business reasons they want to allow Web users to search inside their intranets obtaining partial information. This idea leads to the concept of portals for which there are already several commercial products. New models to see Web sites as databases and/or information systems are also important.  ",
2325,iir,iir-2325,14 Vector space classification," 14 Vector space classification The document representation in Naive Bayes is a sequence of terms or a binary vector he1, . . . , e|V|i ∈ {0, 1}|V|. In this chapter we adopt a different representation for text classification, the vector space model, developed in Chapter 6. It represents each document as a vector with one real-valued component, usually a tf-idf weight, for each term. Thus, the document space X, the domain of the classification function γ, is R|V|. This chapter introduces a number of classification methods that operate on real-valued vectors. The basic hypothesis in using the vector space model for classification is the contiguity hypothesis.CONTIGUITY HYPOTHESIS Contiguity hypothesis. Documents in the same class form a contiguous region and regions of different classes do not overlap. There are many classification tasks, in particular the type of text classification that we encountered in Chapter 13, where classes can be distinguished by word patterns. For example, documents in the class China tend to have high values on dimensions like Chinese,Beijing, and Mao whereas documents in the class UK tend to have high values for London,British and Queen. Documents of the two classes therefore form distinct contiguous regions as shown in Figure 14.1 and we can draw boundaries that separate them and classify new documents. How exactly this is done is the topic of this chapter. Whether or not a set of documents is mapped into a contiguous region depends on the particular choices we make for the document representation: type of weighting, stop list etc. To see that the document representation is crucial, consider the two classes written by a group vs. written by a single person. Frequent occurrence of the first person pronoun Iis evidence for the single-person class. But that information is likely deleted from the document representation if we use a stop list. If the document representation chosen is unfavorable, the contiguity hypothesis will not hold and successful vector space classification is not possible. The same considerations that led us to prefer weighted representations, in particular length-normalized tf-idf representations, in Chapters 6and 7also   14 Vector space classification x xx x ⋄ ⋄⋄ ⋄ ⋄ ⋄ China Kenya UK ⋆ ◮Figure 14.1 Vector space classification into three classes. apply here. For example, a term with 5 occurrences in a document should get a higher weight than a term with one occurrence, but a weight 5 times larger would give too much emphasis to the term. Unweighted and unnormalized counts should not be used in vector space classification. We introduce two vector space classification methods in this chapter, Rocchio and kNN. Rocchio classification (Section 14.2) divides the vector space into regions centered on centroids or prototypes, one for each class, computed as the center of mass of all documents in the class. Rocchio classification is simple and efficient, but inaccurate if classes are not approximately spheres with similar radii. kNN or knearest neighbor classification (Section 14.3) assigns the majority class of the knearest neighbors to a test document. kNN requires no explicit training and can use the unprocessed training set directly in classification. It is less efficient than other classification methods in classifying documents. If the training set is large, then kNN can handle non-spherical and other complex classes better than Rocchio. A large number of text classifiers can be viewed as linear classifiers – classifiers that classify based on a simple linear combination of the features (Section 14.4). Such classifiers partition the space of features into regions separated by linear decision hyperplanes, in a manner to be detailed below. Because of the bias-variance tradeoff (Section 14.6) more complex nonlinear models       ",14.1
2326,iir,iir-2326,14.1 Document representations and measures of relatedness in vector spaces," 14.1 Document representations and measures of relatedness in vector spaces 291 dtrue dprojected x1 x2x3x4 x5 x′ 1x′ 2x′ 3x′ 4x′ 5 x′ 1x′ 2x′ 3x′ 4x′ 5 ◮Figure 14.2 Projections of small areas of the unit sphere preserve distances. Left: A projection of the 2D semicircle to 1D. For the points x1,x2,x3,x4,x5at x coordinates −0.9, −0.2, 0, 0.2, 0.9 the distance |x2x3| ≈ 0.201 only differs by 0.5% from |x′ 2x′ 3|= 0.2; but |x1x3|/|x′ 1x′ 3|=dtrue/dprojected ≈1.06/0.9 ≈1.18 is an example of a large distortion (18%) when projecting a large area. Right: The corresponding projection of the 3D hemisphere to 2D. are not systematically better than linear models. Nonlinear models have more parameters to fit on a limited amount of training data and are more likely to make mistakes for small and noisy data sets. When applying two-class classifiers to problems with more than two classes, there are one-of tasks – a document must be assigned to exactly one of several mutually exclusive classes – and any-of tasks – a document can be assigned to any number of classes as we will explain in Section 14.5. Two-class classifiers solve any-of problems and can be combined to solve one-of problems. 14.1 Document representations and measures of relatedness in vector spaces As in Chapter 6, we represent documents as vectors in R|V|in this chapter. To illustrate properties of document vectors in vector classification, we will render these vectors as points in a plane as in the example in Figure 14.1. In reality, document vectors are length-normalized unit vectors that point to the surface of a hypersphere. We can view the 2D planes in our figures as projections onto a plane of the surface of a (hyper-)sphere as shown in Figure 14.2. Distances on the surface of the sphere and on the projection plane are approximately the same as long as we restrict ourselves to small areas of the surface and choose an appropriate projection (Exercise 14.1).      292 14 Vector space classification Decisions of many vector space classifiers are based on a notion of distance, e.g., when computing the nearest neighbors in kNN classification. We will use Euclidean distance in this chapter as the underlying distance measure. We observed earlier (Exercise 6.18, page 131) that there is a direct correspondence between cosine similarity and Euclidean distance for lengthnormalized vectors. In vector space classification, it rarely matters whether the relatedness of two documents is expressed in terms of similarity or distance. However, in addition to documents, centroids or averages of vectors also play an important role in vector space classification. Centroids are not lengthnormalized. For unnormalized vectors, dot product, cosine similarity and Euclidean distance all have different behavior in general (Exercise 14.6). We will be mostly concerned with small local regions when computing the similarity between a document and a centroid, and the smaller the region the more similar the behavior of the three measures is. ?Exercise 14.1 For small areas, distances on the surface of the hypersphere are approximated well by distances on its projection (Figure 14.2) because α≈sin αfor small angles. For what size angle is the distortion α/ sin(α)(i) 1.01, (ii) 1.05 and (iii) 1.1?  ",14.1
2327,iir,iir-2327,14.2 Rocchio classification," 14.2 Rocchio classification Figure 14.1 shows three classes, China,UK and Kenya, in a two-dimensional (2D) space. Documents are shown as circles, diamonds and X’s. The boundaries in the figure, which we call decision boundaries, are chosen to separate the three classes, but are otherwise arbitrary. To classify a new document, depicted as a star in the figure, we determine the region it occurs in and assign it the class of that region – China in this case. Our task in vector space classification is to devise algorithms that compute good boundaries where “good” means high classification accuracy on data unseen during training. Perhaps the best-known way of computing good class boundaries is Roc-ROCCHIO CLASSIFICATION chio classification, which uses centroids to define the boundaries. The centroid CENTROID of a class cis computed as the vector average or center of mass of its members: ~µ(c) = 1 |Dc|∑ d∈Dc ~v(d) (14.1) where Dcis the set of documents in Dwhose class is c:Dc={d:hd,ci ∈ D}. We denote the normalized vector of dby ~v(d)(Equation (6.11), page 122). Three example centroids are shown as solid circles in Figure 14.3. The boundary between two classes in Rocchio classification is the set of points with equal distance from the two centroids. For example, |a1|=|a2|,   14.2 Rocchio classification 293 x xx x ⋄ ⋄⋄ ⋄ ⋄ ⋄ China Kenya UK ⋆a1 a2 b1 b2 c1 c2 ◮Figure 14.3 Rocchio classification. |b1|=|b2|, and |c1|=|c2|in the figure. This set of points is always a line. The generalization of a line in M-dimensional space is a hyperplane, which we define as the set of points ~xthat satisfy: ~wT~x=b (14.2) where ~wis the M-dimensional normal vector1of the hyperplane and bis a constant. This definition of hyperplanes includes lines (any line in 2D can be defined by w1x1+w2x2=b) and 2-dimensional planes (any plane in 3D can be defined by w1x1+w2x2+w3x3=b). A line divides a plane in two, a plane divides 3-dimensional space in two, and hyperplanes divide higherdimensional spaces in two. Thus, the boundaries of class regions in Rocchio classification are hyperplanes. The classification rule in Rocchio is to classify a point in accordance with the region it falls into. Equivalently, we determine the centroid ~µ(c)that the point is closest to and then assign it to c. As an example, consider the star in Figure 14.3. It is located in the China region of the space and Rocchio therefore assigns it to China. We show the Rocchio algorithm in pseudocode in Figure 14.4. 1. Recall from basic linear algebra that ~v·~w=~vT~w, i.e., the dot product of ~vand ~wequals the product by matrix multiplication of the transpose of ~vand ~w.      294 14 Vector space classification term weights vector Chinese Japan Tokyo Macao Beijing Shanghai ~ d10 0 0 0 1.0 0 ~ d20 0 0 0 0 1.0 ~ d30 0 0 1.0 0 0 ~ d40 0.71 0.71 0 0 0 ~ d50 0.71 0.71 0 0 0 ~µc0 0 0 0.33 0.33 0.33 ~µc0 0.71 0.71 0 0 0 ◮Table 14.1 Vectors and class centroids for the data in Table 13.1. ✎Example 14.1: Table 14.1 shows the tf-idf vector representations of the five documents in Table 13.1 (page 261), using the formula (1+log10 tft,d)log10(4/dft)if tft,d&gt; 0 (Equation (6.14), page 127). The two class centroids are µc=1/3 ·(~ d1+~ d2+~ d3) and µc=1/1 ·(~ d4). The distances of the test document from the centroids are |µc−~ d5| ≈ 1.15 and |µc−~ d5|=0.0. Thus, Rocchio assigns d5to c. The separating hyperplane in this case has the following parameters: ~w≈(0−0.71 −0.71 1/3 1/3 1/3)T b=−1/3 See Exercise 14.15 for how to compute ~wand b. We can easily verify that this hyperplane separates the documents as desired: ~wT~ d1≈0·0+−0.71 ·0+−0.71 ·0+ 1/3 ·0+1/3 ·1.0 +1/3 ·0=1/3 &gt;b(and, similarly, ~wT~ di&gt;bfor i=2 and i=3) and ~wT~ d4=−1&lt;b. Thus, documents in care above the hyperplane (~wT~ d&gt;b) and documents in care below the hyperplane (~wT~ d&lt;b). The assignment criterion in Figure 14.4 is Euclidean distance (APPLYROCCHIO, line 1). An alternative is cosine similarity: Assign dto class c=arg max c′ cos(~µ(c′),~v(d)) As discussed in Section 14.1, the two assignment criteria will sometimes make different classification decisions. We present the Euclidean distance variant of Rocchio classification here because it emphasizes Rocchio’s close correspondence to K-means clustering (Section 16.4, page 360). Rocchio classification is a form of Rocchio relevance feedback (Section 9.1.1, page 178). The average of the relevant documents, corresponding to the most important component of the Rocchio vector in relevance feedback (Equation (9.3), page 182), is the centroid of the “class” of relevant documents. We omit the query component of the Rocchio formula in Rocchio classification since there is no query in text classification. Rocchio classification can be   14.2 Rocchio classification 295 TRAINROCCHIO(C,D) 1for each cj∈C 2do Dj← {d:hd,cji ∈ D} 3~µj←1 |Dj|∑d∈Dj~v(d) 4return {~µ1, . . . ,~µJ} APPLYROCCHIO({~µ1, . . . ,~µJ},d) 1return arg minj|~µj−~v(d)| ◮Figure 14.4 Rocchio classification: Training and testing. a a a a a a aa a a a a a a a a aa a a a a a a a a a a a aa a a aa a a a a a b b b b b b b bb b b b b b b b b b b XX A B o ◮Figure 14.5 The multimodal class “a” consists of two different clusters (small upper circles centered on X’s). Rocchio classification will misclassify “o” as “a” because it is closer to the centroid A of the “a” class than to the centroid B of the “b” class. applied to J&gt;2 classes whereas Rocchio relevance feedback is designed to distinguish only two classes, relevant and nonrelevant. In addition to respecting contiguity, the classes in Rocchio classification must be approximate spheres with similar radii. In Figure 14.3, the solid square just below the boundary between UK and Kenya is a better fit for the class UK since UK is more scattered than Kenya. But Rocchio assigns it to Kenya because it ignores details of the distribution of points in a class and only uses distance from the centroid for classification. The assumption of sphericity also does not hold in Figure 14.5. We can      296 14 Vector space classification mode time complexity training Θ(|D|Lave +|C||V|) testing Θ(La+|C|Ma) = Θ(|C|Ma) ◮Table 14.2 Training and test times for Rocchio classification. Lave is the average number of tokens per document. Laand Maare the numbers of tokens and types, respectively, in the test document. Computing Euclidean distance between the class centroids and a document is Θ(|C|Ma). not represent the “a” class well with a single prototype because it has two clusters. Rocchio often misclassifies this type of multimodal class. A text clas-MULTIMODAL CLASS sification example for multimodality is a country like Burma, which changed its name to Myanmar in 1989. The two clusters before and after the name change need not be close to each other in space. We also encountered the problem of multimodality in relevance feedback (Section 9.1.2, page 184). Two-class classification is another case where classes are rarely distributed like spheres with similar radii. Most two-class classifiers distinguish between a class like China that occupies a small region of the space and its widely scattered complement. Assuming equal radii will result in a large number of false positives. Most two-class classification problems therefore require a modified decision rule of the form: Assign dto class ciff |~µ(c)−~v(d)|&lt;|~µ(c)−~v(d)| − b for a positive constant b. As in Rocchio relevance feedback, the centroid of the negative documents is often not used at all, so that the decision criterion simplifies to |~µ(c)−~v(d)|&lt;b′for a positive constant b′. Table 14.2 gives the time complexity of Rocchio classification.2Adding all documents to their respective (unnormalized) centroid is Θ(|D|Lave)(as opposed to Θ(|D||V|)) since we need only consider non-zero entries. Dividing each vector sum by the size of its class to compute the centroid is Θ(|V|). Overall, training time is linear in the size of the collection (cf. Exercise 13.1). Thus, Rocchio classification and Naive Bayes have the same linear training time complexity. In the next section, we will introduce another vector space classification method, kNN, that deals better with classes that have non-spherical, disconnected or other irregular shapes. ?Exercise 14.2 [⋆] Show that Rocchio classification can assign a label to a document that is different from its training set label. 2. We write Θ(|D|Lave)for Θ(T)and assume that the length of test documents is bounded as we did on page 262.       ",14.2
2328,iir,iir-2328,14.3 k nearest neighbor," 14.3 k nearest neighbor 297 x x xx x xx xxx x ⋄ ⋄⋄ ⋄ ⋄ ⋄ ⋄ ⋄ ⋄ ⋄⋄ ⋆ ◮Figure 14.6 Voronoi tessellation and decision boundaries (double lines) in 1NN classification. The three classes are: X, circle and diamond. 14.3 knearest neighbor Unlike Rocchio, k nearest neighbor or kNN classification determines the decision boundary locally. For 1NN we assign each document to the class of its closest neighbor. For kNN we assign each document to the majority class of its kclosest neighbors where kis a parameter. The rationale of kNN classification is that, based on the contiguity hypothesis, we expect a test document dto have the same label as the training documents located in the local region surrounding d. Decision boundaries in 1NN are concatenated segments of the Voronoi tes- VORONOI TESSELLATION sellation as shown in Figure 14.6. The Voronoi tessellation of a set of objects decomposes space into Voronoi cells, where each object’s cell consists of all points that are closer to the object than to other objects. In our case, the objects are documents. The Voronoi tessellation then partitions the plane into |D|convex polygons, each containing its corresponding document (and no other) as shown in Figure 14.6, where a convex polygon is a convex region in 2-dimensional space bounded by lines. For general k∈Nin kNN, consider the region in the space for which the set of knearest neighbors is the same. This again is a convex polygon and the space is partitioned into convex polygons, within each of which the set of k      298 14 Vector space classification TRAIN-KNN(C,D) 1D′←PREPROCESS(D) 2k←SELECT-K(C,D′) 3return D′,k APPLY-KNN(C,D′,k,d) 1Sk←COMPUTENEARESTNEIGHBORS(D′,k,d) 2for each cj∈C 3do pj← |Sk∩cj|/k 4return arg maxjpj ◮Figure 14.7 kNN training (with preprocessing) and testing. pjis an estimate for P(cj|Sk) = P(cj|d).cjdenotes the set of all documents in the class cj. nearest neighbors is invariant (Exercise 14.11).3 1NN is not very robust. The classification decision of each test document relies on the class of a single training document, which may be incorrectly labeled or atypical. kNN for k&gt;1 is more robust. It assigns documents to the majority class of their kclosest neighbors, with ties broken randomly. There is a probabilistic version of this kNN classification algorithm. We can estimate the probability of membership in class cas the proportion of the knearest neighbors in c. Figure 14.6 gives an example for k=3. Probability estimates for class membership of the star are ˆ P(circle class|star) = 1/3, ˆ P(X class|star) = 2/3, and ˆ P(diamond class|star) = 0. The 3nn estimate (ˆ P1(circle class|star) = 1/3) and the 1nn estimate ( ˆ P1(circle class|star) = 1) differ with 3nn preferring the X class and 1nn preferring the circle class . The parameter kin kNN is often chosen based on experience or knowledge about the classification problem at hand. It is desirable for kto be odd to make ties less likely. k=3 and k=5 are common choices, but much larger values between 50 and 100 are also used. An alternative way of setting the parameter is to select the kthat gives best results on a held-out portion of the training set. We can also weight the “votes” of the knearest neighbors by their cosine 3. The generalization of a polygon to higher dimensions is a polytope. A polytope is a region in M-dimensional space bounded by (M−1)-dimensional hyperplanes. In Mdimensions, the decision boundaries for kNN consist of segments of (M−1)-dimensional hyperplanes that form the Voronoi tessellation into convex polytopes for the training set of documents. The decision criterion of assigning a document to the majority class of its knearest neighbors applies equally to M=2 (tessellation into polygons) and M&gt;2 (tessellation into polytopes).   14.3 k nearest neighbor 299 kNN with preprocessing of training set training Θ(|D|Lave) testing Θ(La+|D|MaveMa) = Θ(|D|MaveMa) kNN without preprocessing of training set training Θ(1) testing Θ(La+|D|Lave Ma) = Θ(|D|Lave Ma) ◮Table 14.3 Training and test times for kNN classification. Mave is the average size of the vocabulary of documents in the collection. similarity. In this scheme, a class’s score is computed as: score(c,d) = ∑ d′∈Sk(d) Ic(d′)cos(~v(d′),~v(d)) where Sk(d)is the set of d’s knearest neighbors and Ic(d′) = 1 iff d′is in class cand 0 otherwise. We then assign the document to the class with the highest score. Weighting by similarities is often more accurate than simple voting. For example, if two classes have the same number of neighbors in the top k, the class with the more similar neighbors wins. Figure 14.7 summarizes the kNN algorithm. ✎Example 14.2: The distances of the test document from the four training documents in Table 14.1 are |~ d1−~ d5|=|~ d2−~ d5|=|~ d3−~ d5| ≈ 1.41 and |~ d4−~ d5|=0.0. d5’s nearest neighbor is therefore d4and 1NN assigns d5to d4’s class, c. ✄14.3.1 Time complexity and optimality  ",14.3
2329,iir,iir-2329,14.3.1 Time complexity and optimality of kNN," 14.3.1 Time complexity and optimality of kNN Table 14.3 gives the time complexity of kNN. kNN has properties that are quite different from most other classification algorithms. Training a kNN classifier simply consists of determining kand preprocessing documents. In fact, if we preselect a value for kand do not preprocess, then kNN requires no training at all. In practice, we have to perform preprocessing steps like tokenization. It makes more sense to preprocess training documents once as part of the training phase rather than repeatedly every time we classify a new test document. Test time is Θ(|D|Mave Ma)for kNN. It is linear in the size of the training set as we need to compute the distance of each training document from the test document. Test time is independent of the number of classes J. kNN therefore has a potential advantage for problems with large J. In kNN classification, we do not perform any estimation of parameters as we do in Rocchio classification (centroids) or in Naive Bayes (priors and conditional probabilities). kNN simply memorizes all examples in the training     300 14 Vector space classification set and then compares the test document to them. For this reason, kNN is also called memory-based learning or instance-based learning. It is usually desir-MEMORY-BASED LEARNING able to have as much training data as possible in machine learning. But in kNN large training sets come with a severe efficiency penalty in classification. Can kNN testing be made more efficient than Θ(|D|MaveMa)or, ignoring the length of documents, more efficient than Θ(|D|)? There are fast kNN algorithms for small dimensionality M(Exercise 14.12). There are also approximations for large Mthat give error bounds for specific efficiency gains (see Section 14.7). These approximations have not been extensively tested for text classification applications, so it is not clear whether they can achieve much better efficiency than Θ(|D|)without a significant loss of accuracy. The reader may have noticed the similarity between the problem of finding nearest neighbors of a test document and ad hoc retrieval, where we search for the documents with the highest similarity to the query (Section 6.3.2, page 123). In fact, the two problems are both knearest neighbor problems and only differ in the relative density of (the vector of) the test document in kNN (10s or 100s of non-zero entries) versus the sparseness of (the vector of) the query in ad hoc retrieval (usually fewer than 10 non-zero entries). We introduced the inverted index for efficient ad hoc retrieval in Section 1.1 (page 6). Is the inverted index also the solution for efficient kNN? An inverted index restricts a search to those documents that have at least one term in common with the query. Thus in the context of kNN, the inverted index will be efficient if the test document has no term overlap with a large number of training documents. Whether this is the case depends on the classification problem. If documents are long and no stop list is used, then less time will be saved. But with short documents and a large stop list, an inverted index may well cut the average test time by a factor of 10 or more. The search time in an inverted index is a function of the length of the postings lists of the terms in the query. Postings lists grow sublinearly with the length of the collection since the vocabulary increases according to Heaps’ law – if the probability of occurrence of some terms increases, then the probability of occurrence of others must decrease. However, most new terms are infrequent. We therefore take the complexity of inverted index search to be Θ(T)(as discussed in Section 2.4.2, page 41) and, assuming average document length does not change over time, Θ(T) = Θ(|D|). As we will see in the next chapter, kNN’s effectiveness is close to that of the most accurate learning methods in text classification (Table 15.2, page 334). A measure of the quality of a learning method is its Bayes error rate, the average error rate of classifiers learned by it for a particular problem. kNN is not optimal for problems with a non-zero Bayes error rate – that is, for problems where even the best possible classifier has a non-zero classification error. The error of 1NN is asymptotically (as the training set increases) bounded by       ",14.3
2330,iir,iir-2330,14.4 Linear versus nonlinear classifiers," 14.4 Linear versus nonlinear classifiers 301 ◮Figure 14.8 There are an infinite number of hyperplanes that separate two linearly separable classes. twice the Bayes error rate. That is, if the optimal classifier has an error rate of x, then 1NN has an asymptotic error rate of less than 2x. This is due to the effect of noise – we already saw one example of noise in the form of noisy features in Section 13.5 (page 271), but noise can also take other forms as we will discuss in the next section. Noise affects two components of kNN: the test document and the closest training document. The two sources of noise are additive, so the overall error of 1NN is twice the optimal error rate. For problems with Bayes error rate 0, the error rate of 1NN will approach 0 as the size of the training set increases. ?Exercise 14.3 Explain why kNN handles multimodal classes better than Rocchio. 14.4 Linear versus nonlinear classifiers In this section, we show that the two learning methods Naive Bayes and Rocchio are instances of linear classifiers, the perhaps most important group of text classifiers, and contrast them with nonlinear classifiers. To simplify the discussion, we will only consider two-class classifiers in this section and define a linear classifier as a two-class classifier that decides class membership by comparing a linear combination of the features to a threshold. In two dimensions, a linear classifier is a line. Five examples are shown in Figure 14.8. These lines have the functional form w1x1+w2x2=b. The classification rule of a linear classifier is to assign a document to cif w1x1+ w2x2&gt;band to cif w1x1+w2x2≤b. Here, (x1,x2)Tis the two-dimensional vector representation of the document and (w1,w2)Tis the parameter vector      302 14 Vector space classification APPLYLINEARCLASSIFIER(~w,b,~x) 1score ←∑M i=1wixi 2if score &gt;b 3then return 1 4else return 0 ◮Figure 14.9 Linear classification algorithm. that defines (together with b) the decision boundary. An alternative geometric interpretation of a linear classifier is provided in Figure 15.7 (page 343). We can generalize this 2D linear classifier to higher dimensions by defining a hyperplane as we did in Equation (14.2), repeated here as Equation (14.3): ~wT~x=b(14.3) The assignment criterion then is: assign to cif ~wT~x&gt;band to cif ~wT~x≤b. We call a hyperplane that we use as a linear classifier a decision hyperplane.DECISION HYPERPLANE The corresponding algorithm for linear classification in Mdimensions is shown in Figure 14.9. Linear classification at first seems trivial given the simplicity of this algorithm. However, the difficulty is in training the linear classifier, that is, in determining the parameters ~wand bbased on the training set. In general, some learning methods compute much better parameters than others where our criterion for evaluating the quality of a learning method is the effectiveness of the learned linear classifier on new data. We now show that Rocchio and Naive Bayes are linear classifiers. To see this for Rocchio, observe that a vector ~xis on the decision boundary if it has equal distance to the two class centroids: |~µ(c1)−~x|=|~µ(c2)−~x| (14.4) Some basic arithmetic shows that this corresponds to a linear classifier with normal vector ~w=~µ(c1)−~µ(c2)and b=0.5 ∗(|~µ(c1)|2− |~µ(c2)|2)(Exercise 14.15). We can derive the linearity of Naive Bayes from its decision rule, which chooses the category cwith the largest ˆ P(c|d)(Figure 13.2, page 260) where: ˆ P(c|d)∝ˆ P(c)∏ 1≤k≤nd ˆ P(tk|c) and ndis the number of tokens in the document that are part of the vocabulary. Denoting the complement category as ¯ c, we obtain for the log odds: log ˆ P(c|d) ˆ P(¯ c|d)=log ˆ P(c) ˆ P(¯ c)+∑ 1≤k≤nd log ˆ P(tk|c) ˆ P(tk|¯ c) (14.5)   14.4 Linear versus nonlinear classifiers 303 tiwid1id2itiwid1id2i prime 0.70 0 1 dlrs -0.71 1 1 rate 0.67 1 0 world -0.35 1 0 interest 0.63 0 0 sees -0.33 0 0 rates 0.60 0 0 year -0.25 0 0 discount 0.46 1 0 group -0.24 0 0 bundesbank 0.43 0 0 dlr -0.24 0 0 ◮Table 14.4 A linear classifier. The dimensions tiand parameters wiof a linear classifier for the class interest (as in interest rate) in Reuters-21578. The threshold is b=0. Terms like dlr and world have negative weights because they are indicators for the competing class currency. We choose class cif the odds are greater than 1 or, equivalently, if the log odds are greater than 0. It is easy to see that Equation (14.5) is an instance of Equation (14.3) for wi=log[ˆ P(ti|c)/ˆ P(ti|¯ c)],xi=number of occurrences of tiin d, and b=−log[ˆ P(c)/ˆ P(¯ c)]. Here, the index i, 1 ≤i≤M, refers to terms of the vocabulary (not to positions in das kdoes; cf. Section 13.4.1, page 270) and ~xand ~ware M-dimensional vectors. So in log space, Naive Bayes is a linear classifier. ✎Example 14.3: Table 14.4 defines a linear classifier for the category interest in Reuters-21578 (see Section 13.6, page 279). We assign document ~ d1“rate discount dlrs world” to interest since ~wT~ d1=0.67 ·1+0.46 ·1\+ (−0.71)·1\+ (−0.35)·1= 0.07 &gt;0=b. We assign ~ d2“prime dlrs” to the complement class (not in interest) since ~wT~ d2=−0.01 ≤b. For simplicity, we assume a simple binary vector representation in this example: 1 for occurring terms, 0 for non-occurring terms. Figure 14.10 is a graphical example of a linear problem, which we define to mean that the underlying distributions P(d|c)and P(d|c)of the two classes are separated by a line. We call this separating line the class boundary. It is the “true” boundary of the two classes and we distinguish it from the decision boundary that the learning method computes to approximate the class boundary. As is typical in text classification, there are some noise documents in Fig- NOISE DOCUMENT ure 14.10 (marked with arrows) that do not fit well into the overall distribution of the classes. In Section 13.5 (page 271), we defined a noise feature as a misleading feature that, when included in the document representation, on average increases the classification error. Analogously, a noise document is a document that, when included in the training set, misleads the learning method and increases classification error. Intuitively, the underlying distribution partitions the representation space into areas with mostly ho      304 14 Vector space classification ◮Figure 14.10 A linear problem with noise. In this hypothetical web page classification scenario, Chinese-only web pages are solid circles and mixed Chinese- English web pages are squares. The two classes are separated by a linear class boundary (dashed line, short dashes), except for three noise documents (marked with arrows). mogeneous class assignments. A document that does not conform with the dominant class in its area is a noise document. Noise documents are one reason why training a linear classifier is hard. If we pay too much attention to noise documents when choosing the decision hyperplane of the classifier, then it will be inaccurate on new data. More fundamentally, it is usually difficult to determine which documents are noise documents and therefore potentially misleading. If there exists a hyperplane that perfectly separates the two classes, then we call the two classes linearly separable. In fact, if linear separability holds,LINEAR SEPARABILITY then there is an infinite number of linear separators (Exercise 14.4) as illustrated by Figure 14.8, where the number of possible separating hyperplanes is infinite. Figure 14.8 illustrates another challenge in training a linear classifier. If we are dealing with a linearly separable problem, then we need a criterion for selecting among all decision hyperplanes that perfectly separate the training data. In general, some of these hyperplanes will do well on new data, some   14.4 Linear versus nonlinear classifiers 305 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 ◮Figure 14.11 A nonlinear problem. will not. An example of a nonlinear classifier is kNN. The nonlinearity of kNN is intuitively clear when looking at examples like Figure 14.6. The decision boundaries of kNN (the double lines in Figure 14.6) are locally linear segments, but in general have a complex shape that is not equivalent to a line in 2D or a hyperplane in higher dimensions. Figure 14.11 is another example of a nonlinear problem: there is no good linear separator between the distributions P(d|c)and P(d|c)because of the circular “enclave” in the upper left part of the graph. Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough. If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier. ?Exercise 14.4 Prove that the number of linear separators of two classes is either infinite or zero.      306 14 Vector space classification  ",14.4
2331,iir,iir-2331,14.5 Classification with more than two classes," 14.5 Classification with more than two classes We can extend two-class linear classifiers to J&gt;2 classes. The method to use depends on whether the classes are mutually exclusive or not. Classification for classes that are not mutually exclusive is called any-of ,ANY-OF CLASSIFICATION multilabel, or multivalue classification. In this case, a document can belong to several classes simultaneously, or to a single class, or to none of the classes. A decision on one class leaves all options open for the others. It is sometimes said that the classes are independent of each other, but this is misleading since the classes are rarely statistically independent in the sense defined on page 275. In terms of the formal definition of the classification problem in Equation (13.1) (page 256), we learn Jdifferent classifiers γjin any-of classification, each returning either cjor cj:γj(d)∈ {cj,cj}. Solving an any-of classification task with linear classifiers is straightforward: 1\. Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). 2\. Given the test document, apply each classifier separately. The decision of one classifier has no influence on the decisions of the other classifiers. The second type of classification with more than two classes is one-of clas- ONE-OF CLASSIFICATION sification. Here, the classes are mutually exclusive. Each document must belong to exactly one of the classes. One-of classification is also called multinomial,polytomous4,multiclass, or single-label classification. Formally, there is a single classification function γin one-of classification whose range is C, i.e., γ(d)∈ {c1, . . . , cJ}. kNN is a (nonlinear) one-of classifier. True one-of problems are less common in text classification than any-of problems. With classes like UK,China,poultry, or coffee, a document can be relevant to many topics simultaneously – as when the prime minister of the UK visits China to talk about the coffee and poultry trade. Nevertheless, we will often make a one-of assumption, as we did in Figure 14.1, even if classes are not really mutually exclusive. For the classification problem of identifying the language of a document, the one-of assumption is a good approximation as most text is written in only one language. In such cases, imposing a one-of constraint can increase the classifier’s effectiveness because errors that are due to the fact that the any-of classifiers assigned a document to either no class or more than one class are eliminated. Jhyperplanes do not divide R|V|into Jdistinct regions as illustrated in Figure 14.12. Thus, we must use a combination method when using twoclass linear classifiers for one-of classification. The simplest method is to 4. A synonym of polytomous is polychotomous.   14.5 Classification with more than two classes 307 ? ◮Figure 14.12 Jhyperplanes do not divide space into Jdisjoint regions. rank classes and then select the top-ranked class. Geometrically, the ranking can be with respect to the distances from the Jlinear separators. Documents close to a class’s separator are more likely to be misclassified, so the greater the distance from the separator, the more plausible it is that a positive classification decision is correct. Alternatively, we can use a direct measure of confidence to rank classes, e.g., probability of class membership. We can state this algorithm for one-of classification with linear classifiers as follows: 1\. Build a classifier for each class, where the training set consists of the set of documents in the class (positive labels) and its complement (negative labels). 2\. Given the test document, apply each classifier separately. 3\. Assign the document to the class with •the maximum score, •the maximum confidence value, •or the maximum probability. An important tool for analyzing the performance of a classifier for J&gt;2 classes is the confusion matrix. The confusion matrix shows for each pair of classes hc1,c2i, how many documents from c1were incorrectly assigned to c2. In Table 14.5, the classifier manages to distinguish the three financial classes money-fx,trade, and interest from the three agricultural classes wheat,corn, and grain, but makes many errors within these two groups. The confusion matrix can help pinpoint opportunities for improving the accuracy of the      308 14 Vector space classification assigned class money-fx trade interest wheat corn grain true class money-fx 95 0 10 0 0 0 trade 1 1 90 0 1 0 interest 13 0 0 0 0 0 wheat 0 0 1 34 3 7 corn 1 0 2 13 26 5 grain 0 0 2 14 5 10 ◮Table 14.5 A confusion matrix for Reuters-21578. For example, 14 documents from grain were incorrectly assigned to wheat. Adapted from Picca et al. (2006). system. For example, to address the second largest error in Table 14.5 (14 in the row grain), one could attempt to introduce features that distinguish wheat documents from grain documents. ?Exercise 14.5 Create a training set of 300 documents, 100 each from three different languages (e.g., English, French, Spanish). Create a test set by the same procedure, but also add 100 documents from a fourth language. Train (i) a one-of classifier (ii) an any-of classifier on this training set and evaluate it on the test set. (iii) Are there any interesting differences in how the two classifiers behave on this task? ✄ ",14.5
2332,iir,iir-2332,14.6 The bias-variance tradeoff," 14.6 The bias-variance tradeoff Nonlinear classifiers are more powerful than linear classifiers. For some problems, there exists a nonlinear classifier with zero classification error, but no such linear classifier. Does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification? To answer this question, we introduce the bias-variance tradeoff in this section, one of the most important concepts in machine learning. The tradeoff helps explain why there is no universally optimal learning method. Selecting an appropriate learning method is therefore an unavoidable part of solving a text classification problem. Throughout this section, we use linear and nonlinear classifiers as prototypical examples of “less powerful” and “more powerful” learning, respectively. This is a simplification for a number of reasons. First, many nonlinear models subsume linear models as a special case. For instance, a nonlinear learning method like kNN will in some cases produce a linear classifier. Second, there are nonlinear models that are less complex than linear models. For instance, a quadratic polynomial with two parameters is less powerful than a 10,000-dimensional linear classifier. Third, the complexity of learning is not really a property of the classifier because there are many aspects   14.6 The bias-variance tradeoff 309 of learning (such as feature selection, cf. (Section 13.5, page 271), regularization, and constraints such as margin maximization in Chapter 15) that make a learning method either more powerful or less powerful without affecting the type of classifier that is the final result of learning – regardless of whether that classifier is linear or nonlinear. We refer the reader to the publications listed in Section 14.7 for a treatment of the bias-variance tradeoff that takes into account these complexities. In this section, linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning methods in text classification. We first need to state our objective in text classification more precisely. In Section 13.1 (page 256), we said that we want to minimize classification error on the test set. The implicit assumption was that training documents and test documents are generated according to the same underlying distribution. We will denote this distribution P(hd,ci)where dis the document and cits label or class. Figures 13.4 and 13.5 were examples of generative models that decompose P(hd,ci)into the product of P(c)and P(d|c). Figures 14.10 and 14.11 depict generative models for hd,ciwith d∈R2and c∈ {square, solid circle}. In this section, instead of using the number of correctly classified test documents (or, equivalently, the error rate on test documents) as evaluation measure, we adopt an evaluation measure that addresses the inherent uncertainty of labeling. In many text classification problems, a given document representation can arise from documents belonging to different classes. This is because documents from different classes can be mapped to the same document representation. For example, the one-sentence documents China sues France and France sues China are mapped to the same document representation d′={China,France,sues}in a bag of words model. But only the latter document is relevant to the class c′=legal actions brought by France (which might be defined, for example, as a standing query by an international trade lawyer). To simplify the calculations in this section, we do not count the number of errors on the test set when evaluating a classifier, but instead look at how well the classifier estimates the conditional probability P(c|d)of a document being in a class. In the above example, we might have P(c′|d′) = 0.5. Our goal in text classification then is to find a classifier γsuch that, averaged over documents d,γ(d)is as close as possible to the true probability P(c|d). We measure this using mean squared error: MSE(γ) = Ed[γ(d)−P(c|d)]2 (14.6) where Edis the expectation with respect to P(d). The mean squared error term gives partial credit for decisions by γthat are close if not completely right.      310 14 Vector space classification E[x−α]2=Ex2−2Exα+α2 (14.8) = (Ex)2−2Exα+α2 +Ex2−2(Ex)2\+ (Ex)2 = [Ex −α]2 +Ex2−E2x(Ex) + E(Ex)2 = [Ex −α]2+E[x−Ex]2 EDEd[ΓD(d)−P(c|d)]2=EdED[ΓD(d)−P(c|d)]2 (14.9) =Ed[ [EDΓD(d)−P(c|d)]2 +ED[ΓD(d)−EDΓD(d)]2] ◮Figure 14.13 Arithmetic transformations for the bias-variance decomposition. For the derivation of Equation (14.9), we set α=P(c|d)and x=ΓD(d)in Equation (14.8). We define a classifier γto be optimal for a distribution P(hd,ci)if it mini- OPTIMAL CLASSIFIER mizes MSE(γ). Minimizing MSE is a desideratum for classifiers. We also need a criterion for learning methods. Recall that we defined a learning method Γas a function that takes a labeled training set Das input and returns a classifier γ. For learning methods, we adopt as our goal to find a Γthat, averaged over training sets, learns classifiers γwith minimal MSE. We can formalize this as minimizing learning error:LEARNING ERROR learning-error(Γ) = ED[MSE(Γ(D))] (14.7) where EDis the expectation over labeled training sets. To keep things simple, we can assume that training sets have a fixed size – the distribution P(hd,ci) then defines a distribution P(D)over training sets. We can use learning error as a criterion for selecting a learning method in statistical text classification. A learning method Γis optimal for a distribution P(D)if it minimizes the learning error. Writing ΓDfor Γ(D)for better readability, we can transform Equation (14.7) as follows: learning-error(Γ) = ED[MSE(ΓD)] =EDEd[ΓD(d)−P(c|d)]2 (14.10) =Ed[bias(Γ,d) + variance(Γ,d)](14.11)   14.6 The bias-variance tradeoff 311 bias(Γ,d) = [P(c|d)−EDΓD(d)]2 (14.12) variance(Γ,d) = ED[ΓD(d)−EDΓD(d)]2 (14.13) where the equivalence between Equations (14.10) and (14.11) is shown in Equation (14.9) in Figure 14.13. Note that dand Dare independent of each other. In general, for a random document dand a random training set D,D does not contain a labeled instance of d. Bias is the squared difference between P(c|d), the true conditional prob-BIAS ability of dbeing in c, and ΓD(d), the prediction of the learned classifier, averaged over training sets. Bias is large if the learning method produces classifiers that are consistently wrong. Bias is small if (i) the classifiers are consistently right or (ii) different training sets cause errors on different documents or (iii) different training sets cause positive and negative errors on the same documents, but that average out to close to 0. If one of these three conditions holds, then EDΓD(d), the expectation over all training sets, is close to P(c|d). Linear methods like Rocchio and Naive Bayes have a high bias for nonlinear problems because they can only model one type of class boundary, a linear hyperplane. If the generative model P(hd,ci)has a complex nonlinear class boundary, the bias term in Equation (14.11) will be high because a large number of points will be consistently misclassified. For example, the circular enclave in Figure 14.11 does not fit a linear model and will be misclassified consistently by linear classifiers. We can think of bias as resulting from our domain knowledge (or lack thereof) that we build into the classifier. If we know that the true boundary between the two classes is linear, then a learning method that produces linear classifiers is more likely to succeed than a nonlinear method. But if the true class boundary is not linear and we incorrectly bias the classifier to be linear, then classification accuracy will be low on average. Nonlinear methods like kNN have low bias. We can see in Figure 14.6 that the decision boundaries of kNN are variable – depending on the distribution of documents in the training set, learned decision boundaries can vary greatly. As a result, each document has a chance of being classified correctly for some training sets. The average prediction EDΓD(d)is therefore closer to P(c|d)and bias is smaller than for a linear learning method. Variance is the variation of the prediction of learned classifiers: the aver- VARIANCE age squared difference between ΓD(d)and its average EDΓD(d). Variance is large if different training sets Dgive rise to very different classifiers ΓD. It is small if the training set has a minor effect on the classification decisions ΓD makes, be they correct or incorrect. Variance measures how inconsistent the decisions are, not whether they are correct or incorrect. Linear learning methods have low variance because most randomly drawn training sets produce similar decision hyperplanes. The decision lines pro     312 14 Vector space classification duced by linear learning methods in Figures 14.10 and 14.11 will deviate slightly from the main class boundaries, depending on the training set, but the class assignment for the vast majority of documents (with the exception of those close to the main boundary) will not be affected. The circular enclave in Figure 14.11 will be consistently misclassified. Nonlinear methods like kNN have high variance. It is apparent from Figure 14.6 that kNN can model very complex boundaries between two classes. It is therefore sensitive to noise documents of the sort depicted in Figure 14.10. As a result the variance term in Equation (14.11) is large for kNN: Test documents are sometimes misclassified – if they happen to be close to a noise document in the training set – and sometimes correctly classified – if there are no noise documents in the training set near them. This results in high variation from training set to training set. High-variance learning methods are prone to overfitting the training data.OVERFITTING The goal in classification is to fit the training data to the extent that we capture true properties of the underlying distribution P(hd,ci). In overfitting, the learning method also learns from noise. Overfitting increases MSE and frequently is a problem for high-variance learning methods. We can also think of variance as the model complexity or, equivalently, mem- MEMORY CAPACITY ory capacity of the learning method – how detailed a characterization of the training set it can remember and then apply to new data. This capacity corresponds to the number of independent parameters available to fit the training set. Each kNN neighborhood Skmakes an independent classification decision. The parameter in this case is the estimate ˆ P(c|Sk)from Figure 14.7. Thus, kNN’s capacity is only limited by the size of the training set. It can memorize arbitrarily large training sets. In contrast, the number of parameters of Rocchio is fixed – Jparameters per dimension, one for each centroid – and independent of the size of the training set. The Rocchio classifier (in form of the centroids defining it) cannot “remember” fine-grained details of the distribution of the documents in the training set. According to Equation (14.7), our goal in selecting a learning method is to minimize learning error. The fundamental insight captured by Equation (14.11), which we can succinctly state as: learning-error = bias + variance, is that the learning error has two components, bias and variance, which in general cannot be minimized simultaneously. When comparing two learning methods Γ1and Γ2, in most cases the comparison comes down to one method having higher bias and lower variance and the other lower bias and higher variance. The decision for one learning method vs. another is then not simply a matter of selecting the one that reliably produces good classifiers across training sets (small variance) or the one that can learn classification problems with very difficult decision boundaries (small bias). Instead, we have to weigh the respective merits of bias and variance in our application and choose accordingly. This tradeoff is called the bias-variance tradeoff .BIAS-VARIANCE TRADEOFF   14.6 The bias-variance tradeoff 313 Figure 14.10 provides an illustration, which is somewhat contrived, but will be useful as an example for the tradeoff. Some Chinese text contains English words written in the Roman alphabet like CPU,ONLINE, and GPS. Consider the task of distinguishing Chinese-only web pages from mixed Chinese-English web pages. A search engine might offer Chinese users without knowledge of English (but who understand loanwords like CPU) the option of filtering out mixed pages. We use two features for this classification task: number of Roman alphabet characters and number of Chinese characters on the web page. As stated earlier, the distribution P(hd,ci) of the generative model generates most mixed (respectively, Chinese) documents above (respectively, below) the short-dashed line, but there are a few noise documents. In Figure 14.10, we see three classifiers: •One-feature classifier. Shown as a dotted horizontal line. This classifier uses only one feature, the number of Roman alphabet characters. Assuming a learning method that minimizes the number of misclassifications in the training set, the position of the horizontal decision boundary is not greatly affected by differences in the training set (e.g., noise documents). So a learning method producing this type of classifier has low variance. But its bias is high since it will consistently misclassify squares in the lower left corner and “solid circle” documents with more than 50 Roman characters. •Linear classifier. Shown as a dashed line with long dashes. Learning linear classifiers has less bias since only noise documents and possibly a few documents close to the boundary between the two classes are misclassified. The variance is higher than for the one-feature classifiers, but still small: The dashed line with long dashes deviates only slightly from the true boundary between the two classes, and so will almost all linear decision boundaries learned from training sets. Thus, very few documents (documents close to the class boundary) will be inconsistently classified. •“Fit-training-set-perfectly” classifier. Shown as a solid line. Here, the learning method constructs a decision boundary that perfectly separates the classes in the training set. This method has the lowest bias because there is no document that is consistently misclassified – the classifiers sometimes even get noise documents in the test set right. But the variance of this learning method is high. Because noise documents can move the decision boundary arbitrarily, test documents close to noise documents in the training set will be misclassified – something that a linear learning method is unlikely to do. It is perhaps surprising that so many of the best-known text classification algorithms are linear. Some of these methods, in particular linear SVMs, reg     314 14 Vector space classification ularized logistic regression and regularized linear regression, are among the most effective known methods. The bias-variance tradeoff provides insight into their success. Typical classes in text classification are complex and seem unlikely to be modeled well linearly. However, this intuition is misleading for the high-dimensional spaces that we typically encounter in text applications. With increased dimensionality, the likelihood of linear separability increases rapidly (Exercise 14.17). Thus, linear models in high-dimensional spaces are quite powerful despite their linearity. Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane, but they are also more sensitive to noise in the training data. Nonlinear learning methods sometimes perform better if the training set is large, but by no means in all cases.  ",14.6
2333,iir,iir-2333,14.7 References and further reading," 14.7 References and further reading As discussed in Chapter 9, Rocchio relevance feedback is due to Rocchio (1971). Joachims (1997) presents a probabilistic analysis of the method. Rocchio classification was widely used as a classification method in TREC in the 1990s (Buckley et al. 1994a;b,Voorhees and Harman 2005). Initially, it was used as a form of routing. Routing merely ranks documents according to rel- ROUTING evance to a class without assigning them. Early work on filtering, a true clas- FILTERING sification approach that makes an assignment decision on each document, was published by Ittner et al. (1995) and Schapire et al. (1998). The definition of routing we use here should not be confused with another sense. Routing can also refer to the electronic distribution of documents to subscribers, the so-called push model of document distribution. In a pull model, each transfer PULL MODEL of a document to the user is initiated by the user – for example, by means of search or by selecting it from a list of documents on a news aggregation website. Some authors restrict the name Roccchio classification to two-class problems and use the terms cluster-based (Iwayama and Tokunaga 1995) and centroid- CENTROID-BASED CLASSIFICATION based classification (Han and Karypis 2000,Tan and Cheng 2007) for Rocchio classification with J&gt;2. A more detailed treatment of kNN can be found in (Hastie et al. 2001), including methods for tuning the parameter k. An example of an approximate fast kNN algorithm is locality-based hashing (Andoni et al. 2006). Kleinberg (1997) presents an approximate Θ((Mlog2M)(M+log N)) kNN algorithm (where Mis the dimensionality of the space and Nthe number of data points), but at the cost of exponential storage requirements: Θ((Nlog M)2M). Indyk (2004) surveys nearest neighbor methods in high-dimensional spaces. Early work on kNN in text classification was motivated by the availability of massively parallel hardware architectures (Creecy et al. 1992). Yang (1994)      ",14.7
2334,iir,iir-2334,14.8 Exercises," 14.8 Exercises 315 uses an inverted index to speed up kNN classification. The optimality result for 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart (1967). The effectiveness of Rocchio classification and kNN is highly dependent on careful parameter tuning (in particular, the parameters b′for Rocchio on page 296 and kfor kNN), feature engineering (Section 15.3, page 334) and feature selection (Section 13.5, page 271). Buckley and Salton (1995), Schapire et al. (1998), Yang and Kisiel (2003) and Moschitti (2003) address these issues for Rocchio and Yang (2001) and Ault and Yang (2002) for kNN. Zavrel et al. (2000) compare feature selection methods for kNN. The bias-variance tradeoff was introduced by Geman et al. (1992). The derivation in Section 14.6 is for MSE(γ), but the tradeoff applies to many loss functions (cf. Friedman (1997), Domingos (2000)). Schütze et al. (1995) and Lewis et al. (1996) discuss linear classifiers for text and Hastie et al. (2001) linear classifiers in general. Readers interested in the algorithms mentioned, but not described in this chapter may wish to consult Bishop (2006) for neural networks, Hastie et al. (2001) for linear and logistic regression, and Minsky and Papert (1988) for the perceptron algorithm. Anagnostopoulos et al. (2006) show that an inverted index can be used for highly efficient document classification with any linear classifier, provided that the classifier is still effective when trained on a modest number of features via feature selection. We have only presented the simplest method for combining two-class classifiers into a one-of classifier. Another important method is the use of errorcorrecting codes, where a vector of decisions of different two-class classifiers is constructed for each document. A test document’s decision vector is then “corrected” based on the distribution of decision vectors in the training set, a procedure that incorporates information from all two-class classifiers and their correlations into the final classification decision (Dietterich and Bakiri 1995). Ghamrawi and McCallum (2005) also exploit dependencies between classes in any-of classification. Allwein et al. (2000) propose a general framework for combining two-class classifiers. 14.8 Exercises ?Exercise 14.6 In Figure 14.14, which of the three vectors~a,~ b, and~cis (i) most similar to ~xaccording to dot product similarity, (ii) most similar to ~xaccording to cosine similarity, (iii) closest to ~xaccording to Euclidean distance? Exercise 14.7 Download Reuters-21578 and train and test Rocchio and kNN classifiers for the classes acquisitions,corn,crude,earn,grain,interest,money-fx,ship,trade, and wheat. Use the ModApte split. You may want to use one of a number of software packages that im      316 14 Vector space classification 0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8 ax b c ◮Figure 14.14 Example for differences between Euclidean distance, dot product similarity and cosine similarity. The vectors are ~a= (0.5 1.5)T,~x= (2 2)T,~ b= (4 4)T, and~c= (8 6)T. plement Rocchio classification and kNN classification, for example, the Bow toolkit (McCallum 1996). Exercise 14.8 Download 20 Newgroups (page 154) and train and test Rocchio and kNN classifiers for its 20 classes. Exercise 14.9 Show that the decision boundaries in Rocchio classification are, as in kNN, given by the Voronoi tessellation. Exercise 14.10 [⋆] Computing the distance between a dense centroid and a sparse vector is Θ(M)for a naive implementation that iterates over all Mdimensions. Based on the equality ∑(xi−µi)2=1.0 +∑µ2 i−2∑xiµiand assuming that ∑µ2 ihas been precomputed, write down an algorithm that is Θ(Ma)instead, where Mais the number of distinct terms in the test document. Exercise 14.11 [⋆⋆⋆] Prove that the region of the plane consisting of all points with the same knearest neighbors is a convex polygon. Exercise 14.12 Design an algorithm that performs an efficient 1NN search in 1 dimension (where efficiency is with respect to the number of documents N). What is the time complexity of the algorithm? Exercise 14.13 [⋆⋆⋆] Design an algorithm that performs an efficient 1NN search in 2 dimensions with at most polynomial (in N) preprocessing time.   14.8 Exercises 317 ◮Figure 14.15 A simple non-separable set of points. Exercise 14.14 [⋆ ⋆ ⋆] Can one design an exact efficient algorithm for 1NN for very large Malong the ideas you used to solve the last exercise? Exercise 14.15 Show that Equation (14.4) defines a hyperplane with ~w=~µ(c1)−~µ(c2)and b= 0.5 ∗(|~µ(c1)|2−|~µ(c2)|2). Exercise 14.16 We can easily construct non-separable data sets in high dimensions by embedding a non-separable set like the one shown in Figure 14.15. Consider embedding Figure 14.15 in 3D and then perturbing the 4 points slightly (i.e., moving them a small distance in a random direction). Why would you expect the resulting configuration to be linearly separable? How likely is then a non-separable set of m≪Mpoints in M-dimensional space? Exercise 14.17 Assuming two classes, show that the percentage of non-separable assignments of the vertices of a hypercube decreases with dimensionality Mfor M&gt;1. For example, for M=1 the proportion of non-separable assignments is 0, for M=2, it is 2/16. One of the two non-separable cases for M=2 is shown in Figure 14.15, the other is its mirror image. Solve the exercise either analytically or by simulation. Exercise 14.18 Although we point out the similarities of Naive Bayes with linear vector space classifiers, it does not make sense to represent count vectors (the document representations in NB) in a continuous vector space. There is however a formalization of NB that is analogous to Rocchio. Show that NB assigns a document to the class (represented as a parameter vector) whose Kullback-Leibler (KL) divergence (Section 12.4, page 251) to the document (represented as a count vector as in Section 13.4.1 (page 270), normalized to sum to 1) is smallest.  ",14.8
2335,iir,iir-2335,16 Flat clustering," 16 Flat clustering Clustering algorithms group a set of documents into subsets or clusters. The algorithms’ goal is to create clusters that are coherent internally, but clearly different from each other. In other words, documents within a cluster should be as similar as possible; and documents in one cluster should be as dissimilar as possible from documents in other clusters. 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 2.5 ◮Figure 16.1 An example of a data set with a clear cluster structure. Clustering is the most common form of unsupervised learning. No super- UNSUPERVISED LEARNING vision means that there is no human expert who has assigned documents to classes. In clustering, it is the distribution and makeup of the data that will determine cluster membership. A simple example is Figure 16.1. It is visually clear that there are three distinct clusters of points. This chapter and Chapter 17 introduce algorithms that find such clusters in an unsupervised fashion. The difference between clustering and classification may not seem great at first. After all, in both cases we have a partition of a set of documents into groups. But as we will see the two problems are fundamentally different. Classification is a form of supervised learning (Chapter 13, page 256): our goal is to replicate a categorical distinction that a human supervisor im   16 Flat clustering poses on the data. In unsupervised learning, of which clustering is the most important example, we have no such teacher to guide us. The key input to a clustering algorithm is the distance measure. In Figure 16.1, the distance measure is distance in the 2D plane. This measure suggests three different clusters in the figure. In document clustering, the distance measure is often also Euclidean distance. Different distance measures give rise to different clusterings. Thus, the distance measure is an important means by which we can influence the outcome of clustering. Flat clustering creates a flat set of clusters without any explicit structure that would relate clusters to each other. Hierarchical clustering creates a hierarchy of clusters and will be covered in Chapter 17. Chapter 17 also addresses the difficult problem of labeling clusters automatically. A second important distinction can be made between hard and soft clustering algorithms. Hard clustering computes a hard assignment – each document is a member of exactly one cluster. The assignment of soft clustering algo- SOFT CLUSTERING rithms is soft – a document’s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. Latent semantic indexing, a form of dimensionality reduction, is a soft clustering algorithm (Chapter 18, page 417). This chapter motivates the use of clustering in information retrieval by introducing a number of applications (Section 16.1), defines the problem we are trying to solve in clustering (Section 16.2) and discusses measures for evaluating cluster quality (Section 16.3). It then describes two flat clustering algorithms, K-means (Section 16.4), a hard clustering algorithm, and the Expectation-Maximization (or EM) algorithm (Section 16.5), a soft clustering algorithm. K-means is perhaps the most widely used flat clustering algorithm due to its simplicity and efficiency. The EM algorithm is a generalization of K-means and can be applied to a large variety of document representations and distributions. 16.1 Clustering in information retrieval  ",16.1
2336,iir,iir-2336,16.1 Clustering in information retrieval," 16.1 Clustering in information retrieval The cluster hypothesis states the fundamental assumption we make when us- CLUSTER HYPOTHESIS ing clustering in information retrieval. Cluster hypothesis. Documents in the same cluster behave similarly with respect to relevance to information needs. The hypothesis states that if there is a document from a cluster that is relevant to a search request, then it is likely that other documents from the same cluster are also relevant. This is because clustering puts together documents that share many terms. The cluster hypothesis essentially is the contiguity   16.1 Clustering in information retrieval 351 Application What is Benefit Example clustered? Search result clustering search results more effective information presentation to user Figure 16.2 Scatter-Gather (subsets of) collection alternative user interface: “search without typing” Figure 16.3 Collection clustering collection effective information presentation for exploratory browsing McKeown et al. (2002), http://news.google.com Language modeling collection increased precision and/or recall Liu and Croft (2004) Cluster-based retrieval collection higher efficiency: faster search Salton (1971a) ◮Table 16.1 Some applications of clustering in information retrieval. hypothesis in Chapter 14 (page 289). In both cases, we posit that similar documents behave similarly with respect to relevance. Table 16.1 shows some of the main applications of clustering in information retrieval. They differ in the set of documents that they cluster – search results, collection or subsets of the collection – and the aspect of an information retrieval system they try to improve – user experience, user interface, effectiveness or efficiency of the search system. But they are all based on the basic assumption stated by the cluster hypothesis. The first application mentioned in Table 16.1 is search result clustering where by search results we mean the documents that were returned in response to a query. The default presentation of search results in information retrieval is a simple list. Users scan the list from top to bottom until they have found the information they are looking for. Instead, search result clustering clusters the search results, so that similar documents appear together. It is often easier to scan a few coherent groups than many individual documents. This is particularly useful if a search term has different word senses. The example in Figure 16.2 is jaguar. Three frequent senses on the web refer to the car, the animal and an Apple operating system. The Clustered Results panel returned by the Vivísimo search engine (http://vivisimo.com) can be a more effective user interface for understanding what is in the search results than a simple list of documents. A better user interface is also the goal of Scatter-Gather, the second ap- SCATTER-GATHER plication in Table 16.1. Scatter-Gather clusters the whole collection to get groups of documents that the user can select or gather. The selected groups are merged and the resulting set is again clustered. This process is repeated until a cluster of interest is found. An example is shown in Figure 16.3.      352 16 Flat clustering ◮Figure 16.2 Clustering of search results to improve recall. None of the top hits cover the animal sense of jaguar, but users can easily access it by clicking on the cat cluster in the Clustered Results panel on the left (third arrow from the top). Automatically generated clusters like those in Figure 16.3 are not as neatly organized as a manually constructed hierarchical tree like the Open Directory at http://dmoz.org. Also, finding descriptive labels for clusters automatically is a difficult problem (Section 17.7, page 396). But cluster-based navigation is an interesting alternative to keyword searching, the standard information retrieval paradigm. This is especially true in scenarios where users prefer browsing over searching because they are unsure about which search terms to use. As an alternative to the user-mediated iterative clustering in Scatter-Gather, we can also compute a static hierarchical clustering of a collection that is not influenced by user interactions (“Collection clustering” in Table 16.1). Google News and its precursor, the Columbia NewsBlaster system, are examples of this approach. In the case of news, we need to frequently recompute the clustering to make sure that users can access the latest breaking stories. Clustering is well suited for access to a collection of news stories since news reading is not really search, but rather a process of selecting a subset of stories about recent events.   16.1 Clustering in information retrieval 353 ◮Figure 16.3 An example of a user session in Scatter-Gather. A collection of New York Times news stories is clustered (“scattered”) into eight clusters (top row). The user manually gathers three of these into a smaller collection International Stories and performs another scattering operation. This process repeats until a small cluster with relevant documents is found (e.g., Trinidad). The fourth application of clustering exploits the cluster hypothesis directly for improving search results, based on a clustering of the entire collection. We use a standard inverted index to identify an initial set of documents that match the query, but we then add other documents from the same clusters even if they have low similarity to the query. For example, if the query is car and several car documents are taken from a cluster of automobile documents, then we can add documents from this cluster that use terms other than car (automobile,vehicle etc). This can increase recall since a group of documents with high mutual similarity is often relevant as a whole. More recently this idea has been used for language modeling. Equation (12.10), page 245, showed that to avoid sparse data problems in the language modeling approach to IR, the model of document dcan be interpolated with a     354 16 Flat clustering collection model. But the collection contains many documents with terms untypical of d. By replacing the collection model with a model derived from d’s cluster, we get more accurate estimates of the occurrence probabilities of terms in d. Clustering can also speed up search. As we saw in Section 6.3.2 (page 123) search in the vector space model amounts to finding the nearest neighbors to the query. The inverted index supports fast nearest-neighbor search for the standard IR setting. However, sometimes we may not be able to use an inverted index efficiently, e.g., in latent semantic indexing (Chapter 18). In such cases, we could compute the similarity of the query to every document, but this is slow. The cluster hypothesis offers an alternative: Find the clusters that are closest to the query and only consider documents from these clusters. Within this much smaller set, we can compute similarities exhaustively and rank documents in the usual way. Since there are many fewer clusters than documents, finding the closest cluster is fast; and since the documents matching a query are all similar to each other, they tend to be in the same clusters. While this algorithm is inexact, the expected decrease in search quality is small. This is essentially the application of clustering that was covered in Section 7.1.6 (page 141). ?Exercise 16.1 Define two documents as similar if they have at least two proper names like Clinton or Sarkozy in common. Give an example of an information need and two documents, for which the cluster hypothesis does not hold for this notion of similarity. Exercise 16.2 Make up a simple one-dimensional example (i.e. points on a line) with two clusters where the inexactness of cluster-based retrieval shows up. In your example, retrieving clusters close to the query should do worse than direct nearest neighbor search.  ",16.1
2337,iir,iir-2337,16.2 Problem statement," 16.2 Problem statement We can define the goal in hard flat clustering as follows. Given (i) a set of documents D={d1, . . . , dN}, (ii) a desired number of clusters K, and (iii) an objective function that evaluates the quality of a clustering, we want to compute an assignment γ:D→ {1, . . . , K}that minimizes (or, in other cases, maximizes) the objective function. In most cases, we also demand that γis surjective, i.e., that none of the Kclusters is empty. The objective function is often defined in terms of similarity or distance between documents. Below, we will see that the objective in K-means clustering is to minimize the average distance between documents and their centroids or, equivalently, to maximize the similarity between documents and their centroids. The discussion of similarity measures and distance metrics   16.2 Problem statement 355 in Chapter 14 (page 291) also applies to this chapter. As in Chapter 14, we use both similarity and distance to talk about relatedness between documents. For documents, the type of similarity we want is usually topic similarity or high values on the same dimensions in the vector space model. For example, documents about China have high values on dimensions like Chinese, Beijing, and Mao whereas documents about the UK tend to have high values for London,Britain and Queen. We approximate topic similarity with cosine similarity or Euclidean distance in vector space (Chapter 6). If we intend to capture similarity of a type other than topic, for example, similarity of language, then a different representation may be appropriate. When computing topic similarity, stop words can be safely ignored, but they are important cues for separating clusters of English (in which the occurs frequently and la infrequently) and French documents (in which the occurs infrequently and la frequently). A note on terminology. An alternative definition of hard clustering is that a document can be a full member of more than one cluster. Partitional clus- PARTITIONAL CLUSTERING tering always refers to a clustering where each document belongs to exactly one cluster. (But in a partitional hierarchical clustering (Chapter 17) all members of a cluster are of course also members of its parent.) On the definition of hard clustering that permits multiple membership, the difference between soft clustering and hard clustering is that membership values in hard clustering are either 0 or 1, whereas they can take on any non-negative value in soft clustering. Some researchers distinguish between exhaustive clusterings that assign each document to a cluster and non-exhaustive clusterings, in which some documents will be assigned to no cluster. Non-exhaustive clusterings in which each document is a member of either no cluster or one cluster are called exclusive. We define clustering to be exhaustive in this book.EXCLUSIVE 16.2.1 Cardinality – the number of cl ",16.2
2338,iir,iir-2338,16.2.1 Cardinality – the number of clusters," 16.2.1 Cardinality – the number of clusters A difficult issue in clustering is determining the number of clusters or cardi- CARDINALITY nality of a clustering, which we denote by K. Often Kis nothing more than a good guess based on experience or domain knowledge. But for K-means, we will also introduce a heuristic method for choosing Kand an attempt to incorporate the selection of Kinto the objective function. Sometimes the application puts constraints on the range of K. For example, the Scatter-Gather interface in Figure 16.3 could not display more than about K=10 clusters per layer because of the size and resolution of computer monitors in the early 1990s. Since our goal is to optimize an objective function, clustering is essentially      356 16 Flat clustering a search problem. The brute force solution would be to enumerate all possible clusterings and pick the best. However, there are exponentially many partitions, so this approach is not feasible.1For this reason, most flat clustering algorithms refine an initial partitioning iteratively. If the search starts at an unfavorable initial point, we may miss the global optimum. Finding a good starting point is therefore another important problem we have to solve in flat clustering.  ",16.2
2339,iir,iir-2339,16.3 Evaluation of clustering," 16.3 Evaluation of clustering Typical objective functions in clustering formalize the goal of attaining high intra-cluster similarity (documents within a cluster are similar) and low intercluster similarity (documents from different clusters are dissimilar). This is an internal criterion for the quality of a clustering. But good scores on an OF QUALITY internal criterion do not necessarily translate into good effectiveness in an application. An alternative to internal criteria is direct evaluation in the application of interest. For search result clustering, we may want to measure the time it takes users to find an answer with different clustering algorithms. This is the most direct evaluation, but it is expensive, especially if large user studies are necessary. As a surrogate for user judgments, we can use a set of classes in an evaluation benchmark or gold standard (see Section 8.5, page 164, and Section 13.6, page 279). The gold standard is ideally produced by human judges with a good level of inter-judge agreement (see Chapter 8, page 152). We can then compute an external criterion that evaluates how well the clustering matches OF QUALITY the gold standard classes. For example, we may want to say that the optimal clustering of the search results for jaguar in Figure 16.2 consists of three classes corresponding to the three senses car,animal, and operating system. In this type of evaluation, we only use the partition provided by the gold standard, not the class labels. This section introduces four external criteria of clustering quality. Purity is a simple and transparent evaluation measure. Normalized mutual information can be information-theoretically interpreted. The Rand index penalizes both false positive and false negative decisions during clustering. The F measure in addition supports differential weighting of these two types of errors. To compute purity, each cluster is assigned to the class which is most fre- PURITY quent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by N. 1. An upper bound on the number of clusterings is KN/K!. The exact number of different partitions of Ndocuments into Kclusters is the Stirling number of the second kind. See http://mathworld.wolfram.com/StirlingNumberoftheSecondKind.html or Comtet (1974).   16.3 Evaluation of clustering 357 x o x x x x o x o o⋄ o x ⋄⋄ ⋄ x cluster 1 cluster 2 cluster 3 ◮Figure 16.4 Purity as an external evaluation criterion for cluster quality. Majority class and number of members of the majority class for the three clusters are: x, 5 (cluster 1); o, 4 (cluster 2); and ⋄, 3 (cluster 3). Purity is (1/17)×(5+4+3)≈0.71. purity NMI RI F5 lower bound 0.0 0.0 0.0 0.0 maximum 1 1 1 1 value for Figure 16.4 0.71 0.36 0.68 0.46 ◮Table 16.2 The four external evaluation measures applied to the clustering in Figure 16.4. Formally: purity(Ω,C) = 1 N∑ k max j|ωk∩cj| (16.1) where Ω={ω1,ω2, . . ., ωK}is the set of clusters and C={c1,c2, . . . , cJ}is the set of classes. We interpret ωkas the set of documents in ωkand cjas the set of documents in cjin Equation (16.1). We present an example of how to compute purity in Figure 16.4.2Bad clusterings have purity values close to 0, a perfect clustering has a purity of 1. Purity is compared with the other three measures discussed in this chapter in Table 16.2. High purity is easy to achieve when the number of clusters is large – in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the quality of the clustering against the number of clusters. A measure that allows us to make this tradeoff is normalized mutual infor- NORMALIZED MUTUAL INFORMATION 2. Recall our note of caution from Figure 14.2 (page 291) when looking at this and other 2D figures in this and the following chapter: these illustrations can be misleading because 2D projections of length-normalized vectors distort similarities and distances between points.      358 16 Flat clustering mation or NMI: NMI(Ω,C) = I(Ω;C) [H(Ω) + H(C)]/2 (16.2) Iis mutual information (cf. Chapter 13, page 272): I(Ω;C) = ∑ k ∑ j P(ωk∩cj)log P(ωk∩cj) P(ωk)P(cj) (16.3) =∑ k ∑ j |ωk∩cj| Nlog N|ωk∩cj| |ωk||cj| (16.4) where P(ωk),P(cj), and P(ωk∩cj)are the probabilities of a document being in cluster ωk, class cj, and in the intersection of ωkand cj, respectively. Equation (16.4) is equivalent to Equation (16.3) for maximum likelihood estimates of the probabilities (i.e., the estimate of each probability is the corresponding relative frequency). His entropy as defined in Chapter 5(page 99): H(Ω) = −∑ k P(ωk)log P(ωk) (16.5) =−∑ k |ωk| Nlog |ωk| N (16.6) where, again, the second equation is based on maximum likelihood estimates of the probabilities. I(Ω;C)in Equation (16.3) measures the amount of information by which our knowledge about the classes increases when we are told what the clusters are. The minimum of I(Ω;C)is 0 if the clustering is random with respect to class membership. In that case, knowing that a document is in a particular cluster does not give us any new information about what its class might be. Maximum mutual information is reached for a clustering Ωexact that perfectly recreates the classes – but also if clusters in Ωexact are further subdivided into smaller clusters (Exercise 16.7). In particular, a clustering with K=Nonedocument clusters has maximum MI. So MI has the same problem as purity: it does not penalize large cardinalities and thus does not formalize our bias that, other things being equal, fewer clusters are better. The normalization by the denominator [H(Ω) + H(C)]/2 in Equation (16.2) fixes this problem since entropy tends to increase with the number of clusters. For example, H(Ω)reaches its maximum log Nfor K=N, which ensures that NMI is low for K=N. Because NMI is normalized, we can use it to compare clusterings with different numbers of clusters. The particular form of the denominator is chosen because [H(Ω) + H(C)]/2 is a tight upper bound on I(Ω;C)(Exercise 16.8). Thus, NMI is always a number between 0 and 1.   16.3 Evaluation of clustering 359 An alternative to this information-theoretic interpretation of clustering is to view it as a series of decisions, one for each of the N(N−1)/2 pairs of documents in the collection. We want to assign two documents to the same cluster if and only if they are similar. A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A false positive (FP) decision assigns two dissimilar documents to the same cluster. A false negative (FN) decision assigns two similar documents to different clusters. The Rand index (RI) measures the percentage of decisions that are correct. That is, it is simply accuracy (Section 8.3, page 155). RI =TP +TN TP +FP +FN +TN As an example, we compute RI for Figure 16.4. We first compute TP +FP. The three clusters contain 6, 6, and 5 points, respectively, so the total number of “positives” or pairs of documents that are in the same cluster is: TP +FP =6 2+6 2+5 2=40 Of these, the x pairs in cluster 1, the o pairs in cluster 2, the ⋄pairs in cluster 3, and the x pair in cluster 3 are true positives: TP =5 2+4 2+3 2+2 2=20 Thus, FP =40 −20 =20. FN and TN are computed similarly, resulting in the following contingency table: Same cluster Different clusters Same class TP =20 FN =24 Different classes FP =20 TN =72 RI is then (20 +72)/(20 +20 +24 +72)≈0.68. The Rand index gives equal weight to false positives and false negatives. Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster. We can use the F measure (Section 8.3,FMEASURE page 154) to penalize false negatives more strongly than false positives by selecting a value β&gt;1, thus giving more weight to recall. P=TP TP +FP R=TP TP +FN Fβ=(β2+1)PR β2P+R      360 16 Flat clustering Based on the numbers in the contingency table, P=20/40 =0.5 and R= 20/44 ≈0.455. This gives us F1≈0.48 for β=1 and F5≈0.456 for β=5. In information retrieval, evaluating clustering with Fhas the advantage that the measure is already familiar to the research community. ?Exercise 16.3 Replace every point din Figure 16.4 with two identical copies of din the same class. (i) Is it less difficult, equally difficult or more difficult to cluster this set of 34 points as opposed to the 17 points in Figure 16.4? (ii) Compute purity, NMI, RI, and F5for the clustering with 34 points. Which measures increase and which stay the same after doubling the number of points? (iii) Given your assessment in (i) and the results in (ii), which measures are best suited to compare the quality of the two clusterings?  ",16.3
2340,iir,iir-2340,16.4 K-means," 16.4 K-means K-means is the most important flat clustering algorithm. Its objective is to minimize the average squared Euclidean distance (Chapter 6, page 131) of documents from their cluster centers where a cluster center is defined as the mean or centroid ~µof the documents in a cluster ω:CENTROID ~µ(ω) = 1 |ω|∑ ~x∈ω ~x The definition assumes that documents are represented as length-normalized vectors in a real-valued space in the familiar way. We used centroids for Rocchio classification in Chapter 14 (page 292). They play a similar role here. The ideal cluster in K-means is a sphere with the centroid as its center of gravity. Ideally, the clusters should not overlap. Our desiderata for classes in Rocchio classification were the same. The difference is that we have no labeled training set in clustering for which we know which documents should be in the same cluster. A measure of how well the centroids represent the members of their clusters is the residual sum of squares or RSS, the squared distance of each vector from its centroid summed over all vectors: RSSk=∑ ~x∈ωk|~x−~µ(ωk)|2 RSS = K ∑ k=1 RSSk (16.7) RSS is the objective function in K-means and our goal is to minimize it. Since Nis fixed, minimizing RSS is equivalent to minimizing the average squared distance, a measure of how well centroids represent their documents.   16.4 K-means 361 K-MEANS({~x1, . . . ,~xN},K) 1(~s1,~s2, . . .,~sK)←SELECTRANDOMSEEDS({~x1, . . . ,~xN},K) 2for k←1to K 3do ~µk←~sk 4while stopping criterion has not been met 5do for k←1to K 6do ωk← {} 7for n←1to N 8do j←arg minj′|~µj′−~xn| 9ωj←ωj∪{~xn}(reassignment of vectors) 10 for k←1to K 11 do ~µk←1 |ωk|∑~x∈ωk~x (recomputation of centroids) 12 return {~µ1, . . .,~µK} ◮Figure 16.5 The K-means algorithm. For most IR applications, the vectors ~xn∈RMshould be length-normalized. Alternative methods of seed selection and initialization are discussed on page 364. The first step of K-means is to select as initial cluster centers Krandomly selected documents, the seeds. The algorithm then moves the cluster centers around in space in order to minimize RSS. As shown in Figure 16.5, this is done iteratively by repeating two steps until a stopping criterion is met: reassigning documents to the cluster with the closest centroid; and recomputing each centroid based on the current members of its cluster. Figure 16.6 shows snapshots from nine iterations of the K-means algorithm for a set of points. The “centroid” column of Table 17.2 (page 397) shows examples of centroids. We can apply one of the following termination conditions. •A fixed number of iterations Ihas been completed. This condition limits the runtime of the clustering algorithm, but in some cases the quality of the clustering will be poor because of an insufficient number of iterations. •Assignment of documents to clusters (the partitioning function γ) does not change between iterations. Except for cases with a bad local minimum, this produces a good clustering, but runtimes may be unacceptably long. •Centroids ~µkdo not change between iterations. This is equivalent to γnot changing (Exercise 16.5). •Terminate when RSS falls below a threshold. This criterion ensures that the clustering is of a desired quality after termination. In practice, we      362 16 Flat clustering 0123456 0 1 2 3 4 × × selection of seeds 0123456 0 1 2 3 4 × × assignment of documents (iter. 1) 0123456 0 1 2 3 4 + + + + + + + + +++ oo + o + + + + ++ ++o + + o + ++ +o o + o + +o +o × × ×× recomputation/movement of ~µ’s (iter. 1) 0123456 0 1 2 3 4 + + + + + + + + +++ + + + o + + + + +o +oo o o + o +o +o + o o o +o +o ×× ~µ’s after convergence (iter. 9) 0123456 0 1 2 3 4 . . . . . . . . ... .. . . . . .. .. ... . . . . .. .. . . . . .. .. movement of ~µ’s in 9 iterations ◮Figure 16.6 AK-means example for K=2 in R2. The position of the two centroids (~µ’s shown as X’s in the top four panels) converges after nine iterations.   16.4 K-means 363 need to combine it with a bound on the number of iterations to guarantee termination. •Terminate when the decrease in RSS falls below a threshold θ. For small θ, this indicates that we are close to convergence. Again, we need to combine it with a bound on the number of iterations to prevent very long runtimes. We now show that K-means converges by proving that RSS monotonically decreases in each iteration. We will use decrease in the meaning decrease or does not change in this section. First, RSS decreases in the reassignment step since each vector is assigned to the closest centroid, so the distance it contributes to RSS decreases. Second, it decreases in the recomputation step because the new centroid is the vector ~vfor which RSSkreaches its minimum. RSSk(~v) = ∑ ~x∈ωk|~v−~x|2=∑ ~x∈ωk M ∑ m=1 (vm−xm)2 (16.8) ∂RSSk(~v) ∂vm =∑ ~x∈ωk 2(vm−xm) (16.9) where xmand vmare the mth components of their respective vectors. Setting the partial derivative to zero, we get: vm=1 |ωk|∑ ~x∈ωk xm (16.10) which is the componentwise definition of the centroid. Thus, we minimize RSSkwhen the old centroid is replaced with the new centroid. RSS, the sum of the RSSk, must then also decrease during recomputation. Since there is only a finite set of possible clusterings, a monotonically decreasing algorithm will eventually arrive at a (local) minimum. Take care, however, to break ties consistently, e.g., by assigning a document to the cluster with the lowest index if there are several equidistant centroids. Otherwise, the algorithm can cycle forever in a loop of clusterings that have the same cost. While this proves the convergence of K-means, there is unfortunately no guarantee that a global minimum in the objective function will be reached. This is a particular problem if a document set contains many outliers, doc- OUTLIER uments that are far from any other documents and therefore do not fit well into any cluster. Frequently, if an outlier is chosen as an initial seed, then no other vector is assigned to it during subsequent iterations. Thus, we end up with a singleton cluster (a cluster with only one document) even though there is probably a clustering with lower RSS. Figure 16.7 shows an example of a suboptimal clustering resulting from a bad choice of initial seeds.      364 16 Flat clustering 01234 0 1 2 3 × × × × × × d1d2d3 d4d5d6 ◮Figure 16.7 The outcome of clustering in K-means depends on the initial seeds. For seeds d2and d5,K-means converges to {{d1,d2,d3},{d4,d5,d6}}, a suboptimal clustering. For seeds d2and d3, it converges to {{d1,d2,d4,d5},{d3,d6}}, the global optimum for K=2. Another type of suboptimal clustering that frequently occurs is one with empty clusters (Exercise 16.11). Effective heuristics for seed selection include (i) excluding outliers from the seed set; (ii) trying out multiple starting points and choosing the clustering with lowest cost; and (iii) obtaining seeds from another method such as hierarchical clustering. Since deterministic hierarchical clustering methods are more predictable than K-means, a hierarchical clustering of a small random sample of size iK (e.g., for i=5 or i=10) often provides good seeds (see the description of the Buckshot algorithm, Chapter 17, page 399). Other initialization methods compute seeds that are not selected from the vectors to be clustered. A robust method that works well for a large variety of document distributions is to select i(e.g., i=10) random vectors for each cluster and use their centroid as the seed for this cluster. See Section 16.6 for more sophisticated initializations. What is the time complexity of K-means? Most of the time is spent on computing vector distances. One such operation costs Θ(M). The reassignment step computes KN distances, so its overall complexity is Θ(KN M). In the recomputation step, each vector gets added to a centroid once, so the complexity of this step is Θ(NM). For a fixed number of iterations I, the overall complexity is therefore Θ(IKN M). Thus, K-means is linear in all relevant factors: iterations, number of clusters, number of vectors and dimensionality of the space. This means that K-means is more efficient than the hierarchical algorithms in Chapter 17. We had to fix the number of iterations I, which can be tricky in practice. But in most cases, K-means quickly reaches either complete convergence or a clustering that is close to convergence. In the latter case, a few documents would switch membership if further iterations were computed, but this has a small effect on the overall quality of the clustering.   16.4 K-means 365 There is one subtlety in the preceding argument. Even a linear algorithm can be quite slow if one of the arguments of Θ(...)is large, and Musually is large. High dimensionality is not a problem for computing the distance between two documents. Their vectors are sparse, so that only a small fraction of the theoretically possible Mcomponentwise differences need to be computed. Centroids, however, are dense since they pool all terms that occur in any of the documents of their clusters. As a result, distance computations are time consuming in a naive implementation of K-means. However, there are simple and effective heuristics for making centroid-document similarities as fast to compute as document-document similarities. Truncating centroids to the most significant kterms (e.g., k=1000) hardly decreases cluster quality while achieving a significant speedup of the reassignment step (see references in Section 16.6). The same efficiency problem is addressed by K-medoids, a variant of K-K-MEDOIDS means that computes medoids instead of centroids as cluster centers. We define the medoid of a cluster as the document vector that is closest to the centroid. Since medoids are sparse document vectors, distance computations are fast. ✄16.4.1 Cluster cardinality in K-means ",16.4
2341,iir,iir-2341,16.4.1 Cluster cardinality in K-means," 16.4.1 Cluster cardinality in K-means We stated in Section 16.2 that the number of clusters Kis an input to most flat clustering algorithms. What do we do if we cannot come up with a plausible guess for K? A naive approach would be to select the optimal value of Kaccording to the objective function, namely the value of Kthat minimizes RSS. Defining RSSmin(K)as the minimal RSS of all clusterings with Kclusters, we observe that RSSmin(K)is a monotonically decreasing function in K(Exercise 16.13), which reaches its minimum 0 for K=Nwhere Nis the number of documents. We would end up with each document being in its own cluster. Clearly, this is not an optimal clustering. A heuristic method that gets around this problem is to estimate RSSmin(K) as follows. We first perform i(e.g., i=10) clusterings with Kclusters (each with a different initialization) and compute the RSS of each. Then we take the minimum of the iRSS values. We denote this minimum by d RSSmin(K). Now we can inspect the values d RSSmin(K)as Kincreases and find the “knee” in the curve – the point where successive decreases in d RSSmin become noticeably smaller. There are two such points in Figure 16.8, one at K=4, where the gradient flattens slightly, and a clearer flattening at K=9. This is typical: there is seldom a single best number of clusters. We still need to employ an external constraint to choose from a number of possible values of K(4 and 9 in this case).      366 16 Flat clustering 2 4 6 8 10 1750 1800 1850 1900 1950 number of clusters residual sum of squares ◮Figure 16.8 Estimated minimal residual sum of squares as a function of the number of clusters in K-means. In this clustering of 1203 Reuters-RCV1 documents, there are two points where the d RSSmin curve flattens: at 4 clusters and at 9 clusters. The documents were selected from the categories China,Germany,Russia and Sports, so the K=4 clustering is closest to the Reuters classification. A second type of criterion for cluster cardinality imposes a penalty for each new cluster – where conceptually we start with a single cluster containing all documents and then search for the optimal number of clusters Kby successively incrementing Kby one. To determine the cluster cardinality in this way, we create a generalized objective function that combines two elements: distortion, a measure of how much documents deviate from the prototype of their clusters (e.g., RSS for K-means); and a measure of model complexity. We interpret a clustering here as a model of the data. Model complexity in clustering is usually the number of clusters or a function thereof. For K-means, we then get this selection criterion for K: K=arg min K [RSSmin(K) + λK] (16.11) where λis a weighting factor. A large value of λfavors solutions with few clusters. For λ=0, there is no penalty for more clusters and K=Nis the best solution.     16.4 K-means 367 The obvious difficulty with Equation (16.11) is that we need to determine λ. Unless this is easier than determining Kdirectly, then we are back to square one. In some cases, we can choose values of λthat have worked well for similar data sets in the past. For example, if we periodically cluster news stories from a newswire, there is likely to be a fixed value of λthat gives us the right Kin each successive clustering. In this application, we would not be able to determine Kbased on past experience since Kchanges. A theoretical justification for Equation (16.11) is the Akaike Information Cri- AKAIKE INFORMATION CRITERION terion or AIC, an information-theoretic measure that trades off distortion against model complexity. The general form of AIC is: AIC: K=arg min K [−2L(K) + 2q(K)](16.12) where −L(K), the negative maximum log-likelihood of the data for Kclusters, is a measure of distortion and q(K), the number of parameters of a model with Kclusters, is a measure of model complexity. We will not attempt to derive the AIC here, but it is easy to understand intuitively. The first property of a good model of the data is that each data point is modeled well by the model. This is the goal of low distortion. But models should also be small (i.e., have low model complexity) since a model that merely describes the data (and therefore has zero distortion) is worthless. AIC provides a theoretical justification for one particular way of weighting these two factors, distortion and model complexity, when selecting a model. For K-means, the AIC can be stated as follows: AIC: K=arg min K [RSSmin(K) + 2MK] (16.13) Equation (16.13) is a special case of Equation (16.11) for λ=2M. To derive Equation (16.13) from Equation (16.12) observe that q(K) = KM in K-means since each element of the Kcentroids is a parameter that can be varied independently; and that L(K) = −(1/2)RSSmin(K)(modulo a constant) if we view the model underlying K-means as a Gaussian mixture with hard assignment, uniform cluster priors and identical spherical covariance matrices (see Exercise 16.19). The derivation of AIC is based on a number of assumptions, e.g., that the data are independent and identically distributed. These assumptions are only approximately true for data sets in information retrieval. As a consequence, the AIC can rarely be applied without modification in text clustering. In Figure 16.8, the dimensionality of the vector space is M≈50,000\. Thus, 2MK &gt;50,000 dominates the smaller RSS-based term (d RSSmin(1)&lt;5000, not shown in the figure) and the minimum of the expression is reached for K=1. But as we know, K=4 (corresponding to the four classes China,     368 16 Flat clustering Germany,Russia and Sports) is a better choice than K=1. In practice, Equation (16.11) is often more useful than Equation (16.13) – with the caveat that we need to come up with an estimate for λ. ?Exercise 16.4 Why are documents that do not use the same term for the concept car likely to end up in the same cluster in K-means clustering? Exercise 16.5 Two of the possible termination conditions for K-means were (1) assignment does not change, (2) centroids do not change (page 361). Do these two conditions imply each other? ✄ ",16.4
2342,iir,iir-2342,16.5 Model-based clustering," 16.5 Model-based clustering In this section, we describe a generalization of K-means, the EM algorithm. It can be applied to a larger variety of document representations and distributions than K-means. In K-means, we attempt to find centroids that are good representatives. We can view the set of Kcentroids as a model that generates the data. Generating a document in this model consists of first picking a centroid at random and then adding some noise. If the noise is normally distributed, this procedure will result in clusters of spherical shape. Model-based clustering assumes that the data were generated by a model and tries to recover the original model from the data. The model that we recover from the data then defines clusters and an assignment of documents to clusters. A commonly used criterion for estimating the model parameters is maximum likelihood. In K-means, the quantity exp(−RSS)is proportional to the likelihood that a particular model (i.e., a set of centroids) generated the data. For K-means, maximum likelihood and minimal RSS are equivalent criteria. We denote the model parameters by Θ. In K-means, Θ={~µ1, . . .,~µK}. More generally, the maximum likelihood criterion is to select the parameters Θthat maximize the log-likelihood of generating the data D: Θ=arg max Θ L(D|Θ) = arg max Θ log N ∏ n=1 P(dn|Θ) = arg max Θ N ∑ n=1 log P(dn|Θ) L(D|Θ)is the objective function that measures the goodness of the clustering. Given two clusterings with the same number of clusters, we prefer the one with higher L(D|Θ). This is the same approach we took in Chapter 12 (page 237) for language modeling and in Section 13.1 (page 265) for text classification. In text classification, we chose the class that maximizes the likelihood of generating a particular document. Here, we choose the clustering Θthat maximizes the   16.5 Model-based clustering 369 likelihood of generating a given set of documents. Once we have Θ, we can compute an assignment probability P(d|ωk;Θ)for each document-cluster pair. This set of assignment probabilities defines a soft clustering. An example of a soft assignment is that a document about Chinese cars may have a fractional membership of 0.5 in each of the two clusters China and automobiles, reflecting the fact that both topics are pertinent. A hard clustering like K-means cannot model this simultaneous relevance to two topics. Model-based clustering provides a framework for incorporating our knowledge about a domain. K-means and the hierarchical algorithms in Chapter 17 make fairly rigid assumptions about the data. For example, clusters in K-means are assumed to be spheres. Model-based clustering offers more flexibility. The clustering model can be adapted to what we know about the underlying distribution of the data, be it Bernoulli (as in the example in Table 16.3), Gaussian with non-spherical variance (another model that is important in document clustering) or a member of a different family. A commonly used algorithm for model-based clustering is the Expectation- EXPECTATIONMAXIMIZATION ALGORITHM Maximization algorithm or EM algorithm. EM clustering is an iterative algorithm that maximizes L(D|Θ). EM can be applied to many different types of probabilistic modeling. We will work with a mixture of multivariate Bernoulli distributions here, the distribution we know from Section 11.3 (page 222) and Section 13.3 (page 263): P(d|ωk;Θ) = ∏ tm∈d qmk ! ∏ tm/∈d (1−qmk)! (16.14) where Θ={Θ1, . . . , ΘK},Θk= (αk,q1k, . . . , qMk), and qmk =P(Um=1|ωk) are the parameters of the model.3P(Um=1|ωk)is the probability that a document from cluster ωkcontains term tm. The probability αkis the prior of cluster ωk: the probability that a document dis in ωkif we have no information about d. The mixture model then is: P(d|Θ) = K ∑ k=1 αk ∏ tm∈d qmk! ∏ tm/∈d (1−qmk)! (16.15) In this model, we generate a document by first picking a cluster kwith probability αkand then generating the terms of the document according to the parameters qmk. Recall that the document representation of the multivariate Bernoulli is a vector of MBoolean values (and not a real-valued vector). 3\. Umis the random variable we defined in Section 13.3 (page 266) for the Bernoulli Naive Bayes model. It takes the values 1 (term tmis present in the document) and 0 (term tmis absent in the document).      370 16 Flat clustering How do we use EM to infer the parameters of the clustering from the data? That is, how do we choose parameters Θthat maximize L(D|Θ)? EM is similar to K-means in that it alternates between an expectation step, corresponding to reassignment, and a maximization step, corresponding to recomputation of the parameters of the model. The parameters of K-means are the centroids, the parameters of the instance of EM in this section are the αkand qmk. The maximization step recomputes the conditional parameters qmk and the priors αkas follows: Maximization step: qmk =∑N n=1rnk I(tm∈dn) ∑N n=1rnk αk=∑N n=1rnk N (16.16) where I(tm∈dn) = 1 if tm∈dnand 0 otherwise and rnk is the soft assignment of document dnto cluster kas computed in the preceding iteration. (We’ll address the issue of initialization in a moment.) These are the maximum likelihood estimates for the parameters of the multivariate Bernoulli from Table 13.3 (page 268) except that documents are assigned fractionally to clusters here. These maximum likelihood estimates maximize the likelihood of the data given the model. The expectation step computes the soft assignment of documents to clusters given the current parameters qmk and αk: Expectation step :rnk =αk(∏tm∈dnqmk)(∏tm/∈dn(1−qmk)) ∑K k=1αk(∏tm∈dnqmk)(∏tm/∈dn(1−qmk)) (16.17) This expectation step applies Equations (16.14) and (16.15) to computing the likelihood that ωkgenerated document dn. It is the classification procedure for the multivariate Bernoulli in Table 13.3. Thus, the expectation step is nothing else but Bernoulli Naive Bayes classification (including normalization, i.e. dividing by the denominator, to get a probability distribution over clusters). We clustered a set of 11 documents into two clusters using EM in Table 16.3. After convergence in iteration 25, the first 5 documents are assigned to cluster 1 (ri,1 =1.00) and the last 6 to cluster 2 (ri,1 =0.00). Somewhat atypically, the final assignment is a hard assignment here. EM usually converges to a soft assignment. In iteration 25, the prior α1for cluster 1 is 5/11 ≈0.45 because 5 of the 11 documents are in cluster 1. Some terms are quickly associated with one cluster because the initial assignment can “spread” to them unambiguously. For example, membership in cluster 2 spreads from document 7 to document 8 in the first iteration because they share sugar (r8,1 =0 in iteration 1). For parameters of terms occurring in ambiguous contexts, convergence takes longer. Seed documents 6 and 7   16.5 Model-based clustering 371 (a) docID document text docID document text 1 hot chocolate cocoa beans 7 sweet sugar 2 cocoa ghana africa 8 sugar cane brazil 3 beans harvest ghana 9 sweet sugar beet 4 cocoa butter 10 sweet cake icing 5 butter truffles 11 cake black forest 6 sweet chocolate (b) Parameter Iteration of clustering 0 1 2 3 4 5 15 25 α10.50 0.45 0.53 0.57 0.58 0.54 0.45 r1,1 1.00 1.00 1.00 1.00 1.00 1.00 1.00 r2,1 0.50 0.79 0.99 1.00 1.00 1.00 1.00 r3,1 0.50 0.84 1.00 1.00 1.00 1.00 1.00 r4,1 0.50 0.75 0.94 1.00 1.00 1.00 1.00 r5,1 0.50 0.52 0.66 0.91 1.00 1.00 1.00 r6,1 1.00 1.00 1.00 1.00 1.00 1.00 0.83 0.00 r7,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 r8,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 r9,1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 r10,1 0.50 0.40 0.14 0.01 0.00 0.00 0.00 r11,1 0.50 0.57 0.58 0.41 0.07 0.00 0.00 qafrica,1 0.000 0.100 0.134 0.158 0.158 0.169 0.200 qafrica,2 0.000 0.083 0.042 0.001 0.000 0.000 0.000 qbrazil,1 0.000 0.000 0.000 0.000 0.000 0.000 0.000 qbrazil,2 0.000 0.167 0.195 0.213 0.214 0.196 0.167 qcocoa,1 0.000 0.400 0.432 0.465 0.474 0.508 0.600 qcocoa,2 0.000 0.167 0.090 0.014 0.001 0.000 0.000 qsugar,1 0.000 0.000 0.000 0.000 0.000 0.000 0.000 qsugar,2 1.000 0.500 0.585 0.640 0.642 0.589 0.500 qsweet,1 1.000 0.300 0.238 0.180 0.159 0.153 0.000 qsweet,2 1.000 0.417 0.507 0.610 0.640 0.608 0.667 ◮Table 16.3 The EM clustering algorithm. The table shows a set of documents (a) and parameter values for selected iterations during EM clustering (b). Parameters shown are prior α1, soft assignment scores rn,1 (both omitted for cluster 2), and lexical parameters qm,kfor a few terms. The authors initially assigned document 6 to cluster 1 and document 7 to cluster 2 (iteration 0). EM converges after 25 iterations. For smoothing, the rnk in Equation (16.16) were replaced with rnk +ǫwhere ǫ=0.0001.      372 16 Flat clustering both contain sweet. As a result, it takes 25 iterations for the term to be unambiguously associated with cluster 2. (qsweet,1 =0 in iteration 25.) Finding good seeds is even more critical for EM than for K-means. EM is prone to get stuck in local optima if the seeds are not chosen well. This is a general problem that also occurs in other applications of EM.4Therefore, as with K-means, the initial assignment of documents to clusters is often computed by a different algorithm. For example, a hard K-means clustering may provide the initial assignment, which EM can then “soften up.” ?Exercise 16.6 We saw above that the time complexity of K-means is Θ(IKNM). What is the time complexity of EM?  ",16.5
2343,iir,iir-2343,16.6 References and further reading," 16.6 References and further reading Berkhin (2006b) gives a general up-to-date survey of clustering methods with special attention to scalability. The classic reference for clustering in pattern recognition, covering both K-means and EM, is (Duda et al. 2000). Rasmussen (1992) introduces clustering from an information retrieval perspective. Anderberg (1973) provides a general introduction to clustering for applications. In addition to Euclidean distance and cosine similarity, KullbackLeibler divergence is often used in clustering as a measure of how (dis)similar documents and clusters are (Xu and Croft 1999,Muresan and Harper 2004, Kurland and Lee 2004). The cluster hypothesis is due to Jardine and van Rijsbergen (1971) who state it as follows: Associations between documents convey information about the relevance of documents to requests. Salton (1971a;1975), Croft (1978), Voorhees (1985a), Can and Ozkarahan (1990), Cacheda et al. (2003), Can et al. (2004), Singitham et al. (2004) and Altingövde et al. (2008) investigate the efficiency and effectiveness of cluster-based retrieval. While some of these studies show improvements in effectiveness, efficiency or both, there is no consensus that cluster-based retrieval works well consistently across scenarios. Clusterbased language modeling was pioneered by Liu and Croft (2004). There is good evidence that clustering of search results improves user experience and search result quality (Hearst and Pedersen 1996,Zamir and Etzioni 1999,Tombros et al. 2002,Käki 2005,Toda and Kataoka 2005), although not as much as search result structuring based on carefully edited category hierarchies (Hearst 2006). The Scatter-Gather interface for browsing collections was presented by Cutting et al. (1992). A theoretical framework for an4. For example, this problem is common when EM is used to estimate parameters of hidden Markov models, probabilistic grammars, and machine translation models in natural language processing (Manning and Schütze 1999).   16.6 References and further reading 373 alyzing the properties of Scatter/Gather and other information seeking user interfaces is presented by Pirolli (2007). Schütze and Silverstein (1997) evaluate LSI (Chapter 18) and truncated representations of centroids for efficient K-means clustering. The Columbia NewsBlaster system (McKeown et al. 2002), a forerunner to the now much more famous and refined Google News (http://news.google.com), used hierarchical clustering (Chapter 17) to give two levels of news topic granularity. See Hatzivassiloglou et al. (2000) for details, and Chen and Lin (2000) and Radev et al. (2001) for related systems. Other applications of clustering in information retrieval are duplicate detection (Yang and Callan (2006), Section 19.6, page 438), novelty detection (see references in Section 17.9, page 399) and metadata discovery on the semantic web (Alonso et al. 2006). The discussion of external evaluation measures is partially based on Strehl (2002). Dom (2002) proposes a measure Q0that is better motivated theoretically than NMI. Q0is the number of bits needed to transmit class memberships assuming cluster memberships are known. The Rand index is due to Rand (1971). Hubert and Arabie (1985) propose an adjusted Rand index that ranges between −1 and 1 and is 0 if there is only chance agreement between clusters and classes (similar to κin Chapter 8, page 165). Basu et al. (2004) argue that the three evaluation measures NMI, Rand index and F measure give very similar results. Stein et al. (2003) propose expected edge density as an internal measure and give evidence that it is a good predictor of the quality of a clustering. Kleinberg (2002) and Meil˘a (2005) present axiomatic frameworks for comparing clusterings. Authors that are often credited with the invention of the K-means algorithm include Lloyd (1982) (first distributed in 1957), Ball (1965), MacQueen (1967), and Hartigan and Wong (1979). Arthur and Vassilvitskii (2006) investigate the worst-case complexity of K-means. Bradley and Fayyad (1998), Pelleg and Moore (1999) and Davidson and Satyanarayana (2003) investigate the convergence properties of K-means empirically and how it depends on initial seed selection. Dhillon and Modha (2001) compare K-means clusters with SVD-based clusters (Chapter 18). The K-medoid algorithm was presented by Kaufman and Rousseeuw (1990). The EM algorithm was originally introduced by Dempster et al. (1977). An in-depth treatment of EM is (McLachlan and Krishnan 1996). See Section 18.5 (page 417) for publications on latent analysis, which can also be viewed as soft clustering. AIC is due to Akaike (1974) (see also Burnham and Anderson (2002)). An alternative to AIC is BIC, which can be motivated as a Bayesian model selection procedure (Schwarz 1978). Fraley and Raftery (1998) show how to choose an optimal number of clusters based on BIC. An application of BIC to K-means is (Pelleg and Moore 2000). Hamerly and Elkan (2003) propose an alternative to BIC that performs better in their experiments. Another influential Bayesian approach for determining the number of clusters (simultane     374 16 Flat clustering ously with cluster assignment) is described by Cheeseman and Stutz (1996). Two methods for determining cardinality without external criteria are presented by Tibshirani et al. (2001). We only have space here for classical completely unsupervised clustering. An important current topic of research is how to use prior knowledge to guide clustering (e.g., Ji and Xu (2006)) and how to incorporate interactive feedback during clustering (e.g., Huang and Mitchell (2006)). Fayyad et al. (1998) propose an initialization for EM clustering. For algorithms that can cluster very large data sets in one scan through the data see Bradley et al. (1998). The applications in Table 16.1 all cluster documents. Other information retrieval applications cluster words (e.g., Crouch 1988), contexts of words (e.g., Schütze and Pedersen 1995) or words and documents simultaneously (e.g., Tishby and Slonim 2000,Dhillon 2001,Zha et al. 2001). Simultaneous clustering of words and documents is an example of co-clustering or biclustering .CO-CLUSTERING  ",16.6
2344,iir,iir-2344,16.7 Exercises," 16.7 Exercises ?Exercise 16.7 Let Ωbe a clustering that exactly reproduces a class structure Cand Ω′a clustering that further subdivides some clusters in Ω. Show that I(Ω;C) = I(Ω′;C). Exercise 16.8 Show that I(Ω;C)≤[H(Ω) + H(C)]/2. Exercise 16.9 Mutual information is symmetric in the sense that its value does not change if the roles of clusters and classes are switched: I(Ω;C) = I(C;Ω). Which of the other three evaluation measures are symmetric in this sense? Exercise 16.10 Compute RSS for the two clusterings in Figure 16.7. Exercise 16.11 (i) Give an example of a set of points and three initial centroids (which need not be members of the set of points) for which 3-means converges to a clustering with an empty cluster. (ii) Can a clustering with an empty cluster be the global optimum with respect to RSS? Exercise 16.12 Download Reuters-21578. Discard documents that do not occur in one of the 10 classes acquisitions,corn,crude,earn,grain,interest,money-fx,ship,trade, and wheat. Discard documents that occur in two of these 10 classes. (i) Compute a K-means clustering of this subset into 10 clusters. There are a number of software packages that implement K-means, such as WEKA (Witten and Frank 2005) and R (R Development Core Team 2005). (ii) Compute purity, normalized mutual information, F1and RI for   16.7 Exercises 375 the clustering with respect to the 10 classes. (iii) Compile a confusion matrix (Table 14.5, page 308) for the 10 classes and 10 clusters. Identify classes that give rise to false positives and false negatives. Exercise 16.13 Prove that RSSmin(K)is monotonically decreasing in K. Exercise 16.14 There is a soft version of K-means that computes the fractional membership of a document in a cluster as a monotonically decreasing function of the distance ∆from its centroid, e.g., as e−∆. Modify reassignment and recomputation steps of hard K-means for this soft version. Exercise 16.15 In the last iteration in Table 16.3, document 6 is in cluster 2 even though it was the initial seed for cluster 1. Why does the document change membership? Exercise 16.16 The values of the parameters qmk in iteration 25 in Table 16.3 are rounded. What are the exact values that EM will converge to? Exercise 16.17 Perform a K-means clustering for the documents in Table 16.3. After how many iterations does K-means converge? Compare the result with the EM clustering in Table 16.3 and discuss the differences. Exercise 16.18 [⋆ ⋆ ⋆] Modify the expectation and maximization steps of EM for a Gaussian mixture. The maximization step computes the maximum likelihood parameter estimates αk,~µk, and Σkfor each of the clusters. The expectation step computes for each vector a soft assignment to clusters (Gaussians) based on their current parameters. Write down the equations for Gaussian mixtures corresponding to Equations (16.16) and (16.17). Exercise 16.19 [⋆ ⋆ ⋆] Show that K-means can be viewed as the limiting case of EM for Gaussian mixtures if variance is very small and all covariances are 0. Exercise 16.20 [⋆ ⋆ ⋆] The within-point scatter of a clustering is defined as ∑k1 2∑~xi∈ωk∑~xj∈ωk|~xi−~xj|2. Show that minimizing RSS and minimizing within-point scatter are equivalent. Exercise 16.21 [⋆ ⋆ ⋆] Derive an AIC criterion for the multivariate Bernoulli mixture model from Equation (16.12).  ",16.7
2345,iir,iir-2345,17 Hierarchical clustering," 17 Hierarchical clustering Flat clustering is efficient and conceptually simple, but as we saw in Chapter 16 it has a number of drawbacks. The algorithms introduced in Chapter 16 return a flat unstructured set of clusters, require a prespecified number of clusters as input and are nondeterministic. Hierarchical clustering (or  hierarchic clustering) outputs a hierarchy, a structure that is more informative than the unstructured set of clusters returned by flat clustering.1Hierarchical clustering does not require us to prespecify the number of clusters and most hierarchical algorithms that have been used in IR are deterministic. These advantages of hierarchical clustering come at the cost of lower efficiency. The most common hierarchical clustering algorithms have a complexity that is at least quadratic in the number of documents compared to the linear complexity of K-means and EM (cf. Section 16.4, page 364). This chapter first introduces agglomerative hierarchical clustering (Section 17.1) and presents four different agglomerative algorithms, in Sections 17.2–17.4, which differ in the similarity measures they employ: single-link, completelink, group-average, and centroid similarity. We then discuss the optimality conditions of hierarchical clustering in Section 17.5. Section 17.6 introduces top-down (or divisive) hierarchical clustering. Section 17.7 looks at labeling clusters automatically, a problem that must be solved whenever humans interact with the output of clustering. We discuss implementation issues in Section 17.8. Section 17.9 provides pointers to further reading, including references to soft hierarchical clustering, which we do not cover in this book. There are few differences between the applications of flat and hierarchical clustering in information retrieval. In particular, hierarchical clustering is appropriate for any of the applications shown in Table 16.1 (page 351; see also Section 16.6, page 372). In fact, the example we gave for collection clustering is hierarchical. In general, we select flat clustering when efficiency is important and hierarchical clustering when one of the potential problems 1. In this chapter, we only consider hierarchies that are binary trees like the one shown in Figure 17.1 – but hierarchical clustering can be easily extended to other types of trees.   17 Hierarchical clustering of flat clustering (not enough structure, predetermined number of clusters, non-determinism) is a concern. In addition, many researchers believe that hierarchical clustering produces better clusters than flat clustering. However, there is no consensus on this issue (see references in Section 17.9). 17.1 Hierarchical agglomerative clusterin ",17.1
2346,iir,iir-2346,17.1 Hierarchical agglomerative clustering," 17.1 Hierarchical agglomerative clustering Hierarchical clustering algorithms are either top-down or bottom-up. Bottomup algorithms treat each document as a singleton cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all documents. Bottomup hierarchical clustering is therefore called hierarchical agglomerative cluster-HIERARCHICAL AGGLOMERATIVE CLUSTERING ing or HAC. Top-down clustering requires a method for splitting a cluster. HAC It proceeds by splitting clusters recursively until individual documents are reached. See Section 17.6. HAC is more frequently used in IR than top-down clustering and is the main subject of this chapter. Before looking at specific similarity measures used in HAC in Sections 17.2–17.4, we first introduce a method for depicting hierarchical clusterings graphically, discuss a few key properties of HACs and present a simple algorithm for computing an HAC. An HAC clustering is typically visualized as a dendrogram as shown in Figure 17.1. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where documents are viewed as singleton clusters. We call this similarity the combination similarity of the merged cluster. For example, the combination similarity of the cluster consisting of Lloyd’s CEO questioned and Lloyd’s chief / U.S. grilling in Figure 17.1 is ≈0.56. We define the combination similarity of a singleton cluster as its document’s self-similarity (which is 1.0 for cosine similarity). By moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. For example, we see that the two documents entitled War hero Colin Powell were merged first in Figure 17.1 and that the last merge added Ag trade reform to a cluster consisting of the other 29 documents. A fundamental assumption in HAC is that the merge operation is mono- MONOTONICITY tonic. Monotonic means that if s1,s2, . . . , sK−1are the combination similarities of the successive merges of an HAC, then s1≥s2≥...≥sK−1holds. A nonmonotonic hierarchical clustering contains at least one inversion si&lt;si+1 INVERSION and contradicts the fundamental assumption that we chose the best merge available at each step. We will see an example of an inversion in Figure 17.12. Hierarchical clustering does not require a prespecified number of clusters. However, in some applications we want a partition of disjoint clusters just as   17.1 Hierarchical agglomerative clustering 379 1.0 0.8 0.6 0.4 0.2 0.0 Ag trade reform. Back−to−school spending is up Lloyd’s CEO questioned Lloyd’s chief / U.S. grilling Viag stays positive Chrysler / Latin America Ohio Blue Cross Japanese prime minister / Mexico CompuServe reports loss Sprint / Internet access service Planet Hollywood Trocadero: tripling of revenues German unions split War hero Colin Powell War hero Colin Powell Oil prices slip Chains may raise prices Clinton signs law Lawsuit against tobacco companies suits against tobacco firms Indiana tobacco lawsuit Most active stocks Mexican markets Hog prices tumble NYSE closing averages British FTSE index Fed holds interest rates steady Fed to keep interest rates steady Fed keeps interest rates steady Fed keeps interest rates steady ◮Figure 17.1 A dendrogram of a single-link clustering of 30 documents from Reuters-RCV1. Two possible cuts of the dendrogram are shown: at 0.4 into 24 clusters and at 0.1 into 12 clusters.      380 17 Hierarchical clustering in flat clustering. In those cases, the hierarchy needs to be cut at some point. A number of criteria can be used to determine the cutting point: •Cut at a prespecified level of similarity. For example, we cut the dendrogram at 0.4 if we want clusters with a minimum combination similarity of 0.4. In Figure 17.1, cutting the diagram at y=0.4 yields 24 clusters (grouping only documents with high similarity together) and cutting it at y=0.1 yields 12 clusters (one large financial news cluster and 11 smaller clusters). •Cut the dendrogram where the gap between two successive combination similarities is largest. Such large gaps arguably indicate “natural” clusterings. Adding one more cluster decreases the quality of the clustering significantly, so cutting before this steep decrease occurs is desirable. This strategy is analogous to looking for the knee in the K-means graph in Figure 16.8 (page 366). •Apply Equation (16.11) (page 366): K=arg min K′ [RSS(K′) + λK′] where K′refers to the cut of the hierarchy that results in K′clusters, RSS is the residual sum of squares and λis a penalty for each additional cluster. Instead of RSS, another measure of distortion can be used. •As in flat clustering, we can also prespecify the number of clusters Kand select the cutting point that produces Kclusters. A simple, naive HAC algorithm is shown in Figure 17.2. We first compute the N×Nsimilarity matrix C. The algorithm then executes N−1 steps of merging the currently most similar clusters. In each iteration, the two most similar clusters are merged and the rows and columns of the merged cluster iin Care updated.2The clustering is stored as a list of merges in A.Iindicates which clusters are still available to be merged. The function SIM(i,m,j)computes the similarity of cluster jwith the merge of clusters i and m. For some HAC algorithms, SIM(i,m,j)is simply a function of C[j][i] and C[j][m], for example, the maximum of these two values for single-link. We will now refine this algorithm for the different similarity measures of single-link and complete-link clustering (Section 17.2) and group-average and centroid clustering (Sections 17.3 and 17.4). The merge criteria of these four variants of HAC are shown in Figure 17.3. 2. We assume that we use a deterministic method for breaking ties, such as always choose the merge that is the first cluster with respect to a total ordering of the subsets of the document set D.   17.1 Hierarchical agglomerative clustering 381 SIMPLEHAC(d1, . . . , dN) 1for n←1to N 2do for i←1to N 3do C[n][i]←SIM(dn,di) 4I[n]←1(keeps track of active clusters) 5A←[] (assembles clustering as a sequence of merges) 6for k←1to N−1 7do hi,mi ← arg max{hi,mi:i6=m∧I[i]=1∧I[m]=1}C[i][m] 8A.APPEND(hi,mi)(store merge) 9for j←1to N 10 do C[i][j]←SIM(i,m,j) 11 C[j][i]←SIM(i,m,j) 12 I[m]←0(deactivate cluster) 13 return A ◮Figure 17.2 A simple, but inefficient HAC algorithm. (a) single-link: maximum similarity (b) complete-link: minimum similarity (c) centroid: average inter-similarity (d) group-average: average of all similarities ◮Figure 17.3 The different notions of cluster similarity used by the four HAC algorithms. An inter-similarity is a similarity between two documents from different clusters.      382 17 Hierarchical clustering 01234 0 1 2 3 × d5 × d6 × d7 × d8 × d1 × d2 × d3 × d4 01234 0 1 2 3 × d5 × d6 × d7 × d8 × d1 × d2 × d3 × d4 ◮Figure 17.4 A single-link (left) and complete-link (right) clustering of eight documents. The ellipses correspond to successive clustering stages. Left: The single-link similarity of the two upper two-point clusters is the similarity of d2and d3(solid line), which is greater than the single-link similarity of the two left two- point clusters (dashed line). Right: The complete-link similarity of the two upper two-point clusters is the similarity of d1and d4(dashed line), which is smaller than the complete-link similarity of the two left two-point clusters (solid line).  ",17.1
2356,iir,iir-2356,17.10 Exercises," 17.10 Exercises 401 17.10 Exercises ?Exercise 17.5 A single-link clustering can also be computed from the minimum spanning tree of a graph. The minimum spanning tree connects the vertices of a graph at the smallest possible cost, where cost is defined as the sum over all edges of the graph. In our case the cost of an edge is the distance between two documents. Show that if ∆k−1&gt; ∆k&gt;... &gt;∆1are the costs of the edges of a minimum spanning tree, then these edges correspond to the k−1 merges in constructing a single-link clustering. Exercise 17.6 Show that single-link clustering is best-merge persistent and that GAAC and centroid clustering are not best-merge persistent. Exercise 17.7 a. Consider running 2-means clustering on a collection with documents from two different languages. What result would you expect? b. Would you expect the same result when running an HAC algorithm? Exercise 17.8 Download Reuters-21578. Keep only documents that are in the classes crude,interest, and grain. Discard documents that are members of more than one of these three classes. Compute a (i) single-link, (ii) complete-link, (iii) GAAC, (iv) centroid clustering of the documents. (v) Cut each dendrogram at the second branch from the top to obtain K=3 clusters. Compute the Rand index for each of the 4 clusterings. Which clustering method performs best? Exercise 17.9 Suppose a run of HAC finds the clustering with K=7 to have the highest value on some prechosen goodness measure of clustering. Have we found the highest-value clustering among all clusterings with K=7? Exercise 17.10 Consider the task of producing a single-link clustering of Npoints on a line: × × × × × × × × × × Show that we only need to compute a total of about Nsimilarities. What is the overall complexity of single-link clustering for a set of points on a line? Exercise 17.11 Prove that single-link, complete-link, and group-average clustering are monotonic in the sense defined on page 378. Exercise 17.12 For Npoints, there are ≤NKdifferent flat clusterings into Kclusters (Section 16.2, page 356). What is the number of different hierarchical clusterings (or dendrograms) of Ndocuments? Are there more flat clusterings or more hierarchical clusterings for given Kand N?  ",17.1
2347,iir,iir-2347,17.2 Single-link and complete-link clustering," 17.2 Single-link and complete-link clustering In single-link clustering or single-linkage clustering, the similarity of two clus-SINGLE-LINK CLUSTERING ters is the similarity of their most similar members (see Figure 17.3, (a))3. This single-link merge criterion is local. We pay attention solely to the area where the two clusters come closest to each other. Other, more distant parts of the cluster and the clusters’ overall structure are not taken into account. In complete-link clustering or complete-linkage clustering, the similarity of two clusters is the similarity of their most dissimilar members (see Figure 17.3, (b)). This is equivalent to choosing the cluster pair whose merge has the smallest diameter. This complete-link merge criterion is non-local; the entire structure of the clustering can influence merge decisions. This results in a preference for compact clusters with small diameters over long, straggly clusters, but also causes sensitivity to outliers. A single document far from the center can increase diameters of candidate merge clusters dramatically and completely change the final clustering. Figure 17.4 depicts a single-link and a complete-link clustering of eight documents. The first four steps, each producing a cluster consisting of a pair of two documents, are identical. Then single-link clustering joins the upper two pairs (and after that the lower two pairs) because on the maximumsimilarity definition of cluster similarity, those two clusters are closest. Complete3. Throughout this chapter, we equate similarity with proximity in 2D depictions of clustering.   17.2 Single-link and complete-link clustering 383 1.0 0.8 0.6 0.4 0.2 0.0 NYSE closing averages Hog prices tumble Oil prices slip Ag trade reform. Chrysler / Latin America Japanese prime minister / Mexico Fed holds interest rates steady Fed to keep interest rates steady Fed keeps interest rates steady Fed keeps interest rates steady Mexican markets British FTSE index War hero Colin Powell War hero Colin Powell Lloyd’s CEO questioned Lloyd’s chief / U.S. grilling Ohio Blue Cross Lawsuit against tobacco companies suits against tobacco firms Indiana tobacco lawsuit Viag stays positive Most active stocks CompuServe reports loss Sprint / Internet access service Planet Hollywood Trocadero: tripling of revenues Back−to−school spending is up German unions split Chains may raise prices Clinton signs law ◮Figure 17.5 A dendrogram of a complete-link clustering. The same 30 documents were clustered with single-link clustering in Figure 17.1.      384 17 Hierarchical clustering × × × × × × × × × × × × ◮Figure 17.6 Chaining in single-link clustering. The local criterion in single-link clustering can cause undesirable elongated clusters. link clustering joins the left two pairs (and then the right two pairs) because those are the closest pairs according to the minimum-similarity definition of cluster similarity.4 Figure 17.1 is an example of a single-link clustering of a set of documents and Figure 17.5 is the complete-link clustering of the same set. When cutting the last merge in Figure 17.5, we obtain two clusters of similar size (documents 1–16, from NYSE closing averages to Lloyd’s chief / U.S. grilling, and documents 17–30, from Ohio Blue Cross to Clinton signs law). There is no cut of the dendrogram in Figure 17.1 that would give us an equally balanced clustering. Both single-link and complete-link clustering have graph-theoretic interpretations. Define skto be the combination similarity of the two clusters merged in step k, and G(sk)the graph that links all data points with a similarity of at least sk. Then the clusters after step kin single-link clustering are the connected components of G(sk)and the clusters after step kin complete-link clustering are maximal cliques of G(sk). A connected component is a maximal set of connected points such that there is a path connecting each pair. A clique CLIQUE is a set of points that are completely linked with each other. These graph-theoretic interpretations motivate the terms single-link and complete-link clustering. Single-link clusters at step kare maximal sets of points that are linked via at least one link (a single link) of similarity s≥sk; complete-link clusters at step kare maximal sets of points that are completely linked with each other via links of similarity s≥sk. Single-link and complete-link clustering reduce the assessment of cluster quality to a single similarity between a pair of documents: the two most similar documents in single-link clustering and the two most dissimilar documents in complete-link clustering. A measurement based on one pair cannot fully reflect the distribution of documents in a cluster. It is therefore not surprising that both algorithms often produce undesirable clusters. Single-link clustering can produce straggling clusters as shown in Figure 17.6. Since the merge criterion is strictly local, a chain of points can be extended for long 4. If you are bothered by the possibility of ties, assume that d1has coordinates (1+ǫ, 3 −ǫ)and that all other points have integer coordinates.   17.2 Single-link and complete-link clustering 385 01234567 0 1× d1 × d2 × d3 × d4 × d5 ◮Figure 17.7 Outliers in complete-link clustering. The five documents have the x-coordinates 1 +2ǫ, 4, 5 +2ǫ, 6 and 7 −ǫ. Complete-link clustering creates the two clusters shown as ellipses. The most intuitive two-cluster clustering is {{d1},{d2,d3,d4,d5}}, but in complete-link clustering, the outlier d1splits {d2,d3,d4,d5}as shown. distances without regard to the overall shape of the emerging cluster. This effect is called chaining.CHAINING The chaining effect is also apparent in Figure 17.1. The last eleven merges of the single-link clustering (those above the 0.1 line) add on single documents or pairs of documents, corresponding to a chain. The complete-link clustering in Figure 17.5 avoids this problem. Documents are split into two groups of roughly equal size when we cut the dendrogram at the last merge. In general, this is a more useful organization of the data than a clustering with chains. However, complete-link clustering suffers from a different problem. It pays too much attention to outliers, points that do not fit well into the global structure of the cluster. In the example in Figure 17.7 the four documents d2,d3,d4,d5are split because of the outlier d1at the left edge (Exercise 17.1). Complete-link clustering does not find the most intuitive cluster structure in this example. 17.2.1 Time complexity of HAC The comp ",17.2
2348,iir,iir-2348,17.2.1 Time complexity of HAC," 17.2.1 Time complexity of HAC The complexity of the naive HAC algorithm in Figure 17.2 is Θ(N3)because we exhaustively scan the N×Nmatrix Cfor the largest similarity in each of N−1 iterations. For the four HAC methods discussed in this chapter a more efficient algorithm is the priority-queue algorithm shown in Figure 17.8. Its time complexity is Θ(N2log N). The rows C[k]of the N×Nsimilarity matrix Care sorted in decreasing order of similarity in the priority queues P.P[k].MAX() then returns the cluster in P[k]that currently has the highest similarity with ωk, where we use ωkto denote the kth cluster as in Chapter 16. After creating the merged cluster of ωk1and ωk2,ωk1is used as its representative. The function SIM computes the similarity function for potential merge pairs: largest similarity for single-link, smallest similarity for complete-link, average similarity for GAAC (Section 17.3), and centroid similarity for centroid clustering (Sec      386 17 Hierarchical clustering EFFICIENTHAC(~ d1, . . . , ~ dN) 1for n←1to N 2do for i←1to N 3do C[n][i].sim ←~ dn·~ di 4C[n][i].index ←i 5I[n]←1 6P[n]←priority queue for C[n]sorted on sim 7P[n].DELETE(C[n][n]) (don’t want self-similarities) 8A←[] 9for k←1to N−1 10 do k1←arg max{k:I[k]=1}P[k].MAX().sim 11 k2←P[k1].MAX().index 12 A.APPEND(hk1,k2i) 13 I[k2]←0 14 P[k1]←[] 15 for each iwith I[i] = 1∧i6=k1 16 do P[i].DELETE(C[i][k1]) 17 P[i].DELETE(C[i][k2]) 18 C[i][k1].sim ←SIM(i,k1,k2) 19 P[i].INSERT(C[i][k1]) 20 C[k1][i].sim ←SIM(i,k1,k2) 21 P[k1].INSERT(C[k1][i]) 22 return A clustering algorithm SIM(i,k1,k2) single-link max(SIM(i,k1),SIM(i,k2)) complete-link min(SIM(i,k1),SIM(i,k2)) centroid (1 Nm~vm)·(1 Ni ~vi) group-average 1 (Nm+Ni)(Nm+Ni−1)[(~vm+~vi)2−(Nm+Ni)] compute C[5]12345 0.2 0.8 0.6 0.4 1.0 create P[5](by sorting) 2 3 4 1 0.8 0.6 0.4 0.2 merge 2 and 3, update similarity of 2, delete 3 2 4 1 0.3 0.4 0.2 delete and reinsert 2 4 2 1 0.4 0.3 0.2 ◮Figure 17.8 The priority-queue algorithm for HAC. Top: The algorithm. Center: Four different similarity measures. Bottom: An example for processing steps 6 and 16–19. This is a made up example showing P[5]for a 5 ×5 matrix C.      17.2 Single-link and complete-link clustering 387 SINGLELINKCLUSTERING(d1, . . . , dN) 1for n←1to N 2do for i←1to N 3do C[n][i].sim ←SIM(dn,di) 4C[n][i].index ←i 5I[n]←n 6NBM[n]←arg maxX∈{C[n][i]:n6=i}X.sim 7A←[] 8for n←1to N−1 9do i1←arg max{i:I[i]=i}NBM[i].sim 10 i2←I[NBM[i1].index] 11 A.APPEND(hi1,i2i) 12 for i←1to N 13 do if I[i] = i∧i6=i1∧i6=i2 14 then C[i1][i].sim ←C[i][i1].sim ←max(C[i1][i].sim, C[i2][i].sim) 15 if I[i] = i2 16 then I[i]←i1 17 NBM[i1]←arg maxX∈{C[i1][i]:I[i]=i∧i6=i1}X.sim 18 return A ◮Figure 17.9 Single-link clustering algorithm using an NBM array. After merging two clusters i1and i2, the first one (i1) represents the merged cluster. If I[i] = i, then i is the representative of its current cluster. If I[i]6=i, then ihas been merged into the cluster represented by I[i]and will therefore be ignored when updating NBM[i1]. tion 17.4). We give an example of how a row of Cis processed (Figure 17.8, bottom panel). The loop in lines 1–7 is Θ(N2)and the loop in lines 9–21 is Θ(N2log N)for an implementation of priority queues that supports deletion and insertion in Θ(log N). The overall complexity of the algorithm is therefore Θ(N2log N). In the definition of the function SIM,~vmand ~viare the vector sums of ωk1∪ωk2and ωi, respectively, and Nmand Niare the number of documents in ωk1∪ωk2and ωi, respectively. The argument of EFFICIENTHAC in Figure 17.8 is a set of vectors (as opposed to a set of generic documents) because GAAC and centroid clustering (Sections 17.3 and 17.4) require vectors as input. The complete-link version of EFFICIENTHAC can also be applied to documents that are not represented as vectors. For single-link, we can introduce a next-best-merge array (NBM) as a further optimization as shown in Figure 17.9. NBM keeps track of what the best merge is for each cluster. Each of the two top level for-loops in Figure 17.9 are Θ(N2), thus the overall complexity of single-link clustering is Θ(N2).      388 17 Hierarchical clustering 012345678910 0 1× d1 × d2 × d3 × d4 ◮Figure 17.10 Complete-link clustering is not best-merge persistent. At first, d2is the best-merge cluster for d3. But after merging d1and d2,d4becomes d3’s best- merge candidate. In a best-merge persistent algorithm like single-link, d3’s best- merge cluster would be {d1,d2}. Can we also speed up the other three HAC algorithms with an NBM array? We cannot because only single-link clustering is best-merge persistent .BEST-MERGE PERSISTENCE Suppose that the best merge cluster for ωkis ωjin single-link clustering. Then after merging ωjwith a third cluster ωi6=ωk, the merge of ωiand ωj will be ωk’s best merge cluster (Exercise 17.6). In other words, the best- merge candidate for the merged cluster is one of the two best-merge candidates of its components in single-link clustering. This means that Ccan be updated in Θ(N)in each iteration – by taking a simple max of two values on line 14 in Figure 17.9 for each of the remaining ≤Nclusters. Figure 17.10 demonstrates that best-merge persistence does not hold for complete-link clustering, which means that we cannot use an NBM array to speed up clustering. After merging d3’s best merge candidate d2with cluster d1, an unrelated cluster d4becomes the best merge candidate for d3. This is because the complete-link merge criterion is non-local and can be affected by points at a great distance from the area where two merge candidates meet. In practice, the efficiency penalty of the Θ(N2log N)algorithm is small compared with the Θ(N2)single-link algorithm since computing the similarity between two documents (e.g., as a dot product) is an order of magnitude slower than comparing two scalars in sorting. All four HAC algorithms in this chapter are Θ(N2)with respect to similarity computations. So the difference in complexity is rarely a concern in practice when choosing one of the algorithms. ?Exercise 17.1 Show that complete-link clustering creates the two-cluster clustering depicted in Figure 17.7.  ",17.2
2349,iir,iir-2349,17.3 Group-average agglomerative clustering," 17.3 Group-average agglomerative clustering Group-average agglomerative clustering or GAAC (see Figure 17.3, (d)) evaluates cluster quality based on all similarities between documents, thus avoiding the pitfalls of the single-link and complete-link criteria, which equate cluster   17.3 Group-average agglomerative clustering 389 similarity with the similarity of a single pair of documents. GAAC is also called group-average clustering and average-link clustering. GAAC computes the average similarity SIM-GA of all pairs of documents, including pairs from the same cluster. But self-similarities are not included in the average: SIM-GA(ωi,ωj) = 1 (Ni+Nj)(Ni+Nj−1)∑ dm∈ωi∪ωj ∑ dn∈ωi∪ωj,dn6=dm ~ dm·~ dn (17.1) where ~ dis the length-normalized vector of document d,·denotes the dot product, and Niand Njare the number of documents in ωiand ωj, respectively. The motivation for GAAC is that our goal in selecting two clusters ωi and ωjas the next merge in HAC is that the resulting merge cluster ωk= ωi∪ωjshould be coherent. To judge the coherence of ωk, we need to look at all document-document similarities within ωk, including those that occur within ωiand those that occur within ωj. We can compute the measure SIM-GA efficiently because the sum of individual vector similarities is equal to the similarities of their sums: ∑ dm∈ωi ∑ dn∈ωj (~ dm·~ dn) = ( ∑ dm∈ωi ~ dm)·(∑ dn∈ωj ~ dn) (17.2) With (17.2), we have: SIM-GA(ωi,ωj) = 1 (Ni+Nj)(Ni+Nj−1)[( ∑ dm∈ωi∪ωj ~ dm)2−(Ni+Nj)] (17.3) The term (Ni+Nj)on the right is the sum of Ni+Njself-similarities of value 1.0. With this trick we can compute cluster similarity in constant time (assuming we have available the two vector sums ∑dm∈ωi ~ dmand ∑dm∈ωj ~ dm) instead of in Θ(NiNj). This is important because we need to be able to compute the function SIM on lines 18 and 20 in EFFICIENTHAC (Figure 17.8) in constant time for efficient implementations of GAAC. Note that for two singleton clusters, Equation (17.3) is equivalent to the dot product. Equation (17.2) relies on the distributivity of the dot product with respect to vector addition. Since this is crucial for the efficient computation of a GAAC clustering, the method cannot be easily applied to representations of documents that are not real-valued vectors. Also, Equation (17.2) only holds for the dot product. While many algorithms introduced in this book have near-equivalent descriptions in terms of dot product, cosine similarity and Euclidean distance (cf. Section 14.1, page 291), Equation (17.2) can only be expressed using the dot product. This is a fundamental difference between single-link/complete-link clustering and GAAC. The first two only require a      390 17 Hierarchical clustering square matrix of similarities as input and do not care how these similarities were computed. To summarize, GAAC requires (i) documents represented as vectors, (ii) length normalization of vectors, so that self-similarities are 1.0, and (iii) the dot product as the measure of similarity between vectors and sums of vectors. The merge algorithms for GAAC and complete-link clustering are the same except that we use Equation (17.3) as similarity function in Figure 17.8. Therefore, the overall time complexity of GAAC is the same as for complete-link clustering: Θ(N2log N). Like complete-link clustering, GAAC is not bestmerge persistent (Exercise 17.6). This means that there is no Θ(N2)algorithm for GAAC that would be analogous to the Θ(N2)algorithm for single-link in Figure 17.9. We can also define group-average similarity as including self-similarities: SIM-GA′(ωi,ωj) = 1 (Ni+Nj)2(∑ dm∈ωi∪ωj ~ dm)2=1 Ni+Nj∑ dm∈ωi∪ωj [~ dm·~µ(ωi∪ωj)] (17.4) where the centroid ~µ(ω)is defined as in Equation (14.1) (page 292). This definition is equivalent to the intuitive definition of cluster quality as average similarity of documents ~ dmto the cluster’s centroid ~µ. Self-similarities are always equal to 1.0, the maximum possible value for length-normalized vectors. The proportion of self-similarities in Equation (17.4) is i/i2=1/ifor a cluster of size i. This gives an unfair advantage to small clusters since they will have proportionally more self-similarities. For two documents d1,d2with a similarity s, we have SIM-GA′(d1,d2) = (1+s)/2. In contrast, SIM-GA(d1,d2) = s≤(1+s)/2. This similarity SIM-GA(d1,d2) of two documents is the same as in single-link, complete-link and centroid clustering. We prefer the definition in Equation (17.3), which excludes selfsimilarities from the average, because we do not want to penalize large clusters for their smaller proportion of self-similarities and because we want a consistent similarity value sfor document pairs in all four HAC algorithms. ?Exercise 17.2 Apply group-average clustering to the points in Figures 17.6 and 17.7. Map them onto the surface of the unit sphere in a three-dimensional space to get length- normalized vectors. Is the group-average clustering different from the single-link and completelink clusterings?       ",17.3
2350,iir,iir-2350,17.4 Centroid clustering," 17.4 Centroid clustering 391 01234567 0 1 2 3 4 5×d1 ×d2 ×d3 ×d4 × d5×d6 µ1 µ3 µ2 ◮Figure 17.11 Three iterations of centroid clustering. Each iteration merges the two clusters whose centroids are closest. 17.4 Centroid clustering In centroid clustering, the similarity of two clusters is defined as the similarity of their centroids: SIM-CENT(ωi,ωj) = ~µ(ωi)·~µ(ωj) (17.5) = ( 1 Ni∑ dm∈ωi ~ dm)·(1 Nj∑ dn∈ωj ~ dn) =1 NiNj∑ dm∈ωi ∑ dn∈ωj ~ dm·~ dn (17.6) Equation (17.5) is centroid similarity. Equation (17.6) shows that centroid similarity is equivalent to average similarity of all pairs of documents from different clusters. Thus, the difference between GAAC and centroid clustering is that GAAC considers all pairs of documents in computing average pairwise similarity (Figure 17.3, (d)) whereas centroid clustering excludes pairs from the same cluster (Figure 17.3, (c)). Figure 17.11 shows the first three steps of a centroid clustering. The first two iterations form the clusters {d5,d6}with centroid µ1and {d1,d2}with centroid µ2because the pairs hd5,d6iand hd1,d2ihave the highest centroid similarities. In the third iteration, the highest centroid similarity is between µ1and d4producing the cluster {d4,d5,d6}with centroid µ3. Like GAAC, centroid clustering is not best-merge persistent and therefore Θ(N2log N)(Exercise 17.6). In contrast to the other three HAC algorithms, centroid clustering is not monotonic. So-called inversions can occur: Similarity can increase during      392 17 Hierarchical clustering 012345 0 1 2 3 4 5 × × × d1d2 d3 −4 −3 −2 −1 0d1d2d3 ◮Figure 17.12 Centroid clustering is not monotonic. The documents d1at (1+ǫ, 1), d2at (5, 1), and d3at (3, 1 +2√3)are almost equidistant, with d1and d2closer to each other than to d3. The non-monotonic inversion in the hierarchical clustering of the three points appears as an intersecting merge line in the dendrogram. The intersection is circled. clustering as in the example in Figure 17.12, where we define similarity as negative distance. In the first merge, the similarity of d1and d2is −(4−ǫ). In the second merge, the similarity of the centroid of d1and d2(the circle) and d3 is ≈ −cos(π/6)×4=−√3/2 ×4≈ −3.46 &gt;−(4−ǫ). This is an example of an inversion: similarity increases in this sequence of two clustering steps. In a monotonic HAC algorithm, similarity is monotonically decreasing from iteration to iteration. Increasing similarity in a series of HAC clustering steps contradicts the fundamental assumption that small clusters are more coherent than large clusters. An inversion in a dendrogram shows up as a horizontal merge line that is lower than the previous merge line. All merge lines in Figures 17.1 and 17.5 are higher than their predecessors because single-link and completelink clustering are monotonic clustering algorithms. Despite its non-monotonicity, centroid clustering is often used because its similarity measure – the similarity of two centroids – is conceptually simpler than the average of all pairwise similarities in GAAC. Figure 17.11 is all one needs to understand centroid clustering. There is no equally simple graph that would explain how GAAC works. ?Exercise 17.3 For a fixed set of Ndocuments there are up to N2distinct similarities between clusters in single-link and complete-link clustering. How many distinct cluster similarities are there in GAAC and centroid clustering?      ",17.4
2351,iir,iir-2351,17.5 Optimality of HAC," 17.5 Optimality of HAC 393 ✄17.5 Optimality of HAC To state the optimality conditions of hierarchical clustering precisely, we first define the combination similarity COMB-SIM of a clustering Ω={ω1, . . . , ωK} as the smallest combination similarity of any of its Kclusters: COMB-SIM({ω1, . . . , ωK}) = min kCOMB-SIM(ωk) Recall that the combination similarity of a cluster ωthat was created as the merge of ω1and ω2is the similarity of ω1and ω2(page 378). We then define Ω={ω1, . . . , ωK}to be optimal if all clusterings Ω′with k clusters, k≤K, have lower combination similarities: |Ω′| ≤ |Ω| ⇒ COMB-SIM(Ω′)≤COMB-SIM(Ω) Figure 17.12 shows that centroid clustering is not optimal. The clustering {{d1,d2},{d3}} (for K=2) has combination similarity −(4−ǫ)and {{d1,d2,d3}} (for K=1) has combination similarity -3.46. So the clustering {{d1,d2},{d3}} produced in the first merge is not optimal since there is a clustering with fewer clusters ({{d1,d2,d3}}) that has higher combination similarity. Centroid clustering is not optimal because inversions can occur. The above definition of optimality would be of limited use if it was only applicable to a clustering together with its merge history. However, we can show (Exercise 17.4) that combination similarity for the three non- inversion algorithms can be read off from the cluster without knowing its history. These direct definitions of combination similarity are as follows. single-link The combination similarity of a cluster ωis the smallest similarity of any bipartition of the cluster, where the similarity of a bipartition is the largest similarity between any two documents from the two parts: COMB-SIM(ω) = min {ω′:ω′⊂ω}max di∈ω′max dj∈ω−ω′SIM(di,dj) where each hω′,ω−ω′iis a bipartition of ω. complete-link The combination similarity of a cluster ωis the smallest similarity of any two points in ω: mindi∈ωmindj∈ωSIM(di,dj). GAAC The combination similarity of a cluster ωis the average of all pairwise similarities in ω(where self-similarities are not included in the average): Equation (17.3). If we use these definitions of combination similarity, then optimality is a property of a set of clusters and not of a process that produces a set of clusters.      394 17 Hierarchical clustering We can now prove the optimality of single-link clustering by induction over the number of clusters K. We will give a proof for the case where no two pairs of documents have the same similarity, but it can easily be extended to the case with ties. The inductive basis of the proof is that a clustering with K=Nclusters has combination similarity 1.0, which is the largest value possible. The induction hypothesis is that a single-link clustering ΩKwith Kclusters is optimal: COMB-SIM(ΩK)≥COMB-SIM(Ω′ K)for all Ω′ K. Assume for contradiction that the clustering ΩK−1we obtain by merging the two most similar clusters in ΩKis not optimal and that instead a different sequence of merges Ω′ K,Ω′ K−1 leads to the optimal clustering with K−1 clusters. We can write the assumption that Ω′ K−1is optimal and that ΩK−1is not as COMB-SIM(Ω′ K−1)&gt; COMB-SIM(ΩK−1). Case 1: The two documents linked by s=COMB-SIM(Ω′ K−1)are in the same cluster in ΩK. They can only be in the same cluster if a merge with similarity smaller than shas occurred in the merge sequence producing ΩK. This implies s&gt;COMB-SIM(ΩK). Thus, COMB-SIM(Ω′ K−1) = s&gt;COMB-SIM(ΩK)&gt; COMB-SIM(Ω′ K)&gt;COMB-SIM(Ω′ K−1). Contradiction. Case 2: The two documents linked by s=COMB-SIM(Ω′ K−1)are not in the same cluster in ΩK. But s=COMB-SIM(Ω′ K−1)&gt;COMB-SIM(ΩK−1), so the single-link merging rule should have merged these two clusters when processing ΩK. Contradiction. Thus, ΩK−1is optimal. In contrast to single-link clustering, complete-link clustering and GAAC are not optimal as this example shows: × × × × 1 3 3 d1d2d3d4 Both algorithms merge the two points with distance 1 (d2and d3) first and thus cannot find the two-cluster clustering {{d1,d2},{d3,d4}}. But {{d1,d2},{d3,d4}} is optimal on the optimality criteria of complete-link clustering and GAAC. However, the merge criteria of complete-link clustering and GAAC approximate the desideratum of approximate sphericity better than the merge criterion of single-link clustering. In many applications, we want spherical clusters. Thus, even though single-link clustering may seem preferable at first because of its optimality, it is optimal with respect to the wrong criterion in many document clustering applications. Table 17.1 summarizes the properties of the four HAC algorithms introduced in this chapter. We recommend GAAC for document clustering because it is generally the method that produces the clustering with the best       ",17.5
2352,iir,iir-2352,17.6 Divisive clustering," 17.6 Divisive clustering 395 method combination similarity time compl. optimal? comment single-link max inter-similarity of any 2 docs Θ(N2)yes chaining effect complete-link min inter-similarity of any 2 docs Θ(N2log N)no sensitive to outliers group-average average of all sims Θ(N2log N)no best choice for most applications centroid average inter-similarity Θ(N2log N)no inversions can occur ◮Table 17.1 Comparison of HAC algorithms. properties for applications. It does not suffer from chaining, from sensitivity to outliers and from inversions. There are two exceptions to this recommendation. First, for non-vector representations, GAAC is not applicable and clustering should typically be performed with the complete-link method. Second, in some applications the purpose of clustering is not to create a complete hierarchy or exhaustive partition of the entire document set. For instance, first story detection or novelty detection is the task of detecting the first occurrence of an event in a stream of news stories. One approach to this task is to find a tight cluster within the documents that were sent across the wire in a short period of time and are dissimilar from all previous documents. For example, the documents sent over the wire in the minutes after the World Trade Center attack on September 11, 2001 form such a cluster. Variations of single-link clustering can do well on this task since it is the structure of small parts of the vector space – and not global structure – that is important in this case. Similarly, we will describe an approach to duplicate detection on the web in Section 19.6 (page 440) where single-link clustering is used in the guise of the union-find algorithm. Again, the decision whether a group of documents are duplicates of each other is not influenced by documents that are located far away and single-link clustering is a good choice for duplicate detection. ?Exercise 17.4 Show the equivalence of the two definitions of combination similarity: the process definition on page 378 and the static definition on page 393. 17.6 Divisive clustering So far we have only looked at agglomerative clustering, but a cluster hierarchy can also be generated top-down. This variant of hierarchical clustering is called top-down clustering or divisive clustering. We start at the top with all documents in one cluster. The cluster is split using a flat clustering algo      396 17 Hierarchical clustering rithm. This procedure is applied recursively until each document is in its own singleton cluster. Top-down clustering is conceptually more complex than bottom-up clustering since we need a second, flat clustering algorithm as a “subroutine”. It has the advantage of being more efficient if we do not generate a complete hierarchy all the way down to individual document leaves. For a fixed number of top levels, using an efficient flat algorithm like K-means, top-down algorithms are linear in the number of documents and clusters. So they run much faster than HAC algorithms, which are at least quadratic. There is evidence that divisive algorithms produce more accurate hierarchies than bottom-up algorithms in some circumstances. See the references on bisecting K-means in Section 17.9. Bottom-up methods make clustering decisions based on local patterns without initially taking into account the global distribution. These early decisions cannot be undone. Top-down clustering benefits from complete information about the global distribution when making top-level partitioning decisions.  ",17.6
2353,iir,iir-2353,17.7 Cluster labeling," 17.7 Cluster labeling In many applications of flat clustering and hierarchical clustering, particularly in analysis tasks and in user interfaces (see applications in Table 16.1, page 351), human users interact with clusters. In such settings, we must label clusters, so that users can see what a cluster is about. Differential cluster labeling selects cluster labels by comparing the distribution of terms in one cluster with that of other clusters. The feature selection methods we introduced in Section 13.5 (page 271) can all be used for differential cluster labeling.5In particular, mutual information (MI) (Section 13.5.1, page 272) or, equivalently, information gain and the χ2-test (Section 13.5.2, page 275) will identify cluster labels that characterize one cluster in contrast to other clusters. A combination of a differential test with a penalty for rare terms often gives the best labeling results because rare terms are not necessarily representative of the cluster as a whole. We apply three labeling methods to a K-means clustering in Table 17.2. In this example, there is almost no difference between MI and χ2. We therefore omit the latter. Cluster-internal labeling computes a label that solely depends on the cluster itself, not on other clusters. Labeling a cluster with the title of the document closest to the centroid is one cluster-internal method. Titles are easier to read than a list of terms. A full title can also contain important context that didn’t make it into the top 10 terms selected by MI. On the web, anchor text can 5. Selecting the most frequent terms is a non-differential feature selection technique we discussed in Section 13.5. It can also be used for labeling clusters.   17.7 Cluster labeling 397 labeling method # docs centroid mutual information title 4 622 oil plant mexico production crude power 000 refinery gas bpd plant oil production barrels crude bpd mexico dolly capacity petroleum MEXICO: Hurricane Dolly heads for Mexico coast 9 1017 police security russian people military peace killed told grozny court police killed military security peace told troops forces rebels people RUSSIA: Russia’s Lebed meets rebel chief in Chechnya 10 1259 00 000 tonnes traders futures wheat prices cents september tonne delivery traders futures tonne tonnes desk wheat prices 000 00 USA: Export Business \- Grain/oilseeds complex ◮Table 17.2 Automatically computed cluster labels. This is for three of ten clusters (4, 9, and 10) in a K-means clustering of the first 10,000 documents in Reuters-RCV1. The last three columns show cluster summaries computed by three labeling methods: most highly weighted terms in centroid (centroid), mutual information, and the title of the document closest to the centroid of the cluster (title). Terms selected by only one of the first two methods are in bold. play a role similar to a title since the anchor text pointing to a page can serve as a concise summary of its contents. In Table 17.2, the title for cluster 9 suggests that many of its documents are about the Chechnya conflict, a fact the MI terms do not reveal. However, a single document is unlikely to be representative of all documents in a cluster. An example is cluster 4, whose selected title is misleading. The main topic of the cluster is oil. Articles about hurricane Dolly only ended up in this cluster because of its effect on oil prices. We can also use a list of terms with high weights in the centroid of the cluster as a label. Such highly weighted terms (or, even better, phrases, especially noun phrases) are often more representative of the cluster than a few titles can be, even if they are not filtered for distinctiveness as in the differential methods. However, a list of phrases takes more time to digest for users than a well crafted title. Cluster-internal methods are efficient, but they fail to distinguish terms that are frequent in the collection as a whole from those that are frequent only in the cluster. Terms like year or Tuesday may be among the most frequent in a cluster, but they are not helpful in understanding the contents of a cluster with a specific topic like oil. In Table 17.2, the centroid method selects a few more uninformative terms (000,court,cents,september) than MI (forces,desk), but most of the terms se      398 17 Hierarchical clustering lected by either method are good descriptors. We get a good sense of the documents in a cluster from scanning the selected terms. For hierarchical clustering, additional complications arise in cluster labeling. Not only do we need to distinguish an internal node in the tree from its siblings, but also from its parent and its children. Documents in child nodes are by definition also members of their parent node, so we cannot use a naive differential method to find labels that distinguish the parent from its children. However, more complex criteria, based on a combination of overall collection frequency and prevalence in a given cluster, can determine whether a term is a more informative label for a child node or a parent node (see Section 17.9).  ",17.7
2354,iir,iir-2354,17.8 Implementation notes," 17.8 Implementation notes Most problems that require the computation of a large number of dot products benefit from an inverted index. This is also the case for HAC clustering. Computational savings due to the inverted index are large if there are many zero similarities – either because many documents do not share any terms or because an aggressive stop list is used. In low dimensions, more aggressive optimizations are possible that make the computation of most pairwise similarities unnecessary (Exercise 17.10). However, no such algorithms are known in higher dimensions. We encountered the same problem in kNN classification (see Section 14.7, page 314). When using GAAC on a large document set in high dimensions, we have to take care to avoid dense centroids. For dense centroids, clustering can take time Θ(MN2log N)where Mis the size of the vocabulary, whereas complete-link clustering is Θ(Mave N2log N)where Mave is the average size of the vocabulary of a document. So for large vocabularies complete-link clustering can be more efficient than an unoptimized implementation of GAAC. We discussed this problem in the context of K-means clustering in Chapter 16 (page 365) and suggested two solutions: truncating centroids (keeping only highly weighted terms) and representing clusters by means of sparse medoids instead of dense centroids. These optimizations can also be applied to GAAC and centroid clustering. Even with these optimizations, HAC algorithms are all Θ(N2)or Θ(N2log N) and therefore infeasible for large sets of 1,000,000 or more documents. For such large sets, HAC can only be used in combination with a flat clustering algorithm like K-means. Recall that K-means requires a set of seeds as initialization (Figure 16.5, page 361). If these seeds are badly chosen, then the resulting clustering will be of poor quality. We can employ an HAC algorithm to compute seeds of high quality. If the HAC algorithm is applied to a document subset of size √N, then the overall runtime of K-means cum HAC seed       ",17.8
2355,iir,iir-2355,17.9 References and further reading," 17.9 References and further reading 399 generation is Θ(N). This is because the application of a quadratic algorithm to a sample of size √Nhas an overall complexity of Θ(N). An appropriate adjustment can be made for an Θ(N2log N)algorithm to guarantee linearity. This algorithm is referred to as the Buckshot algorithm. It combines the determinism and higher reliability of HAC with the efficiency of K-means. 17.9 References and further reading An excellent general review of clustering is (Jain et al. 1999). Early references for specific HAC algorithms are (King 1967) (single-link), (Sneath and Sokal 1973) (complete-link, GAAC) and (Lance and Williams 1967) (discussing a large variety of hierarchical clustering algorithms). The single-link algorithm in Figure 17.9 is similar to Kruskal’s algorithm for constructing a minimum spanning tree. A graph-theoretical proof of the correctness of Kruskal’s algorithm (which is analogous to the proof in Section 17.5) is provided by Cormen et al. (1990, Theorem 23.1). See Exercise 17.5 for the connection between minimum spanning trees and single-link clusterings. It is often claimed that hierarchical clustering algorithms produce better clusterings than flat algorithms (Jain and Dubes (1988, p. 140), Cutting et al. (1992), Larsen and Aone (1999)) although more recently there have been experimental results suggesting the opposite (Zhao and Karypis 2002). Even without a consensus on average behavior, there is no doubt that results of EM and K-means are highly variable since they will often converge to a local optimum of poor quality. The HAC algorithms we have presented here are deterministic and thus more predictable. The complexity of complete-link, group-average and centroid clustering is sometimes given as Θ(N2)(Day and Edelsbrunner 1984,Voorhees 1985b, Murtagh 1983) because a document similarity computation is an order of magnitude more expensive than a simple comparison, the main operation executed in the merging steps after the N×Nsimilarity matrix has been computed. The centroid algorithm described here is due to Voorhees (1985b). Voorhees recommends complete-link and centroid clustering over single-link for a retrieval application. The Buckshot algorithm was originally published by Cutting et al. (1993). Allan et al. (1998) apply single-link clustering to first story detection. An important HAC technique not discussed here is Ward’s method (Ward Jr. 1963,El-Hamdouchi and Willett 1986), also called minimum variance clustering. In each step, it selects the merge with the smallest RSS (Chapter 16, page 360). The merge criterion in Ward’s method (a function of all individual distances from the centroid) is closely related to the merge criterion in GAAC (a function of all individual similarities to the centroid).     400 17 Hierarchical clustering Despite its importance for making the results of clustering useful, comparatively little work has been done on labeling clusters. Popescul and Ungar (2000) obtain good results with a combination of χ2and collection frequency of a term. Glover et al. (2002b) use information gain for labeling clusters of web pages. Stein and zu Eissen’s approach is ontology-based (2004). The more complex problem of labeling nodes in a hierarchy (which requires distinguishing more general labels for parents from more specific labels for children) is tackled by Glover et al. (2002a) and Treeratpituk and Callan (2006). Some clustering algorithms attempt to find a set of labels first and then build (often overlapping) clusters around the labels, thereby avoiding the problem of labeling altogether (Zamir and Etzioni 1999,Käki 2005,Osi´nski and Weiss 2005). We know of no comprehensive study that compares the quality of such “label-based” clustering to the clustering algorithms discussed in this chapter and in Chapter 16. In principle, work on multi-document summarization (McKeown and Radev 1995) is also applicable to cluster labeling, but multi-document summaries are usually longer than the short text fragments needed when labeling clusters (cf. Section 8.7, page 170). Presenting clusters in a way that users can understand is a UI problem. We recommend reading (Baeza-Yates and Ribeiro-Neto 1999, ch. 10) for an introduction to user interfaces in IR. An example of an efficient divisive algorithm is bisecting K-means (Steinbach et al. 2000). Spectral clustering algorithms (Kannan et al. 2000,Dhillon 2001,Zha et al. 2001,Ng et al. 2001a), including principal direction divisive partitioning (PDDP) (whose bisecting decisions are based on SVD, see Chapter 18) (Boley 1998,Savaresi and Boley 2004), are computationally more expensive than bisecting K-means, but have the advantage of being deterministic. Unlike K-means and EM, most hierarchical clustering algorithms do not have a probabilistic interpretation. Model-based hierarchical clustering (Vaithyanathan and Dom 2000,Kamvar et al. 2002,Castro et al. 2004) is an exception. The evaluation methodology described in Section 16.3 (page 356) is also applicable to hierarchical clustering. Specialized evaluation measures for hierarchies are discussed by Fowlkes and Mallows (1983), Larsen and Aone (1999) and Sahoo et al. (2006). The R environment (R Development Core Team 2005) offers good support for hierarchical clustering. The R function hclust implements single-link, complete-link, group-average, and centroid clustering; and Ward’s method. Another option provided is medianclustering which represents each cluster by its medoid (cf. k-medoids in Chapter 16, page 365). Support for clustering vectors in high-dimensional spaces is provided by the software package CLUTO (http://glaros.dtc.umn.edu/gkhome/views/cluto).       ",17.9
2262,iir,iir-2262,19 Web search basics," 19 Web search basics In this and the following two chapters, we consider web search engines. Sections 19.1–19.4 provide some background and history to help the reader appreciate the forces that conspire to make the Web chaotic, fast-changing and (from the standpoint of information retrieval) very different from the “traditional” collections studied thus far in this book. Sections 19.5–19.6 deal with estimating the number of documents indexed by web search engines, and the elimination of duplicate documents in web indexes, respectively. These two latter sections serve as background material for the following two chapters. 19.1 Background and history The Web is unprecedented in many ways: unprecedented in scale, unprecedented in t ",19.1
2263,iir,iir-2263,19.1 Background and history," 19.1 Background and history The Web is unprecedented in many ways: unprecedented in scale, unprecedented in the almost-complete lack of coordination in its creation, and unprecedented in the diversity of backgrounds and motives of its participants. Each of these contributes to making web search different – and generally far harder – than searching “traditional” documents. The invention of hypertext, envisioned by Vannevar Bush in the 1940’s and first realized in working systems in the 1970’s, significantly precedes the formation of the World Wide Web (which we will simply refer to as the Web), in the 1990’s. Web usage has shown tremendous growth to the point where it now claims a good fraction of humanity as participants, by relying on a simple, open client-server design: (1) the server communicates with the client via a protocol (the http or hypertext transfer protocol) that is lightweight and simple, asynchronously carrying a variety of payloads (text, images and – over time – richer media such as audio and video files) encoded in a simple markup language called HTML (for hypertext markup language); (2) the client – generally a browser, an application within a graphical user environment – can ignore what it does not understand. Each of these seemingly innocuous features has contributed enormously to the growth of the Web, so it is worthwhile to examine them further.     422 19 Web search basics The basic operation is as follows: a client (such as a browser) sends an http request to a web server. The browser specifies a URL (for Universal Resource Lo-URL cator) such as http://www.stanford.edu/home/atoz/contact.html. In this example URL, the string http refers to the protocol to be used for transmitting the data. The string www.stanford.edu is known as the domain and specifies the root of a hierarchy of web pages (typically mirroring a filesystem hierarchy underlying the web server). In this example, /home/atoz/contact.html is a path in this hierarchy with a file contact.html that contains the information to be returned by the web server at www.stanford.eduin response to this request. The HTML-encoded file contact.html holds the hyperlinks and the content (in this instance, contact information for Stanford University), as well as formatting rules for rendering this content in a browser. Such an http request thus allows us to fetch the content of a page, something that will prove to be useful to us for crawling and indexing documents (Chapter 20). The designers of the first browsers made it easy to view the HTML markup tags on the content of a URL. This simple convenience allowed new users to create their own HTML content without extensive training or experience; rather, they learned from example content that they liked. As they did so, a second feature of browsers supported the rapid proliferation of web content creation and usage: browsers ignored what they did not understand. This did not, as one might fear, lead to the creation of numerous incompatible dialects of HTML. What it did promote was amateur content creators who could freely experiment with and learn from their newly created web pages without fear that a simple syntax error would “bring the system down.” Publishing on the Web became a mass activity that was not limited to a few trained programmers, but rather open to tens and eventually hundreds of millions of individuals. For most users and for most information needs, the Web quickly became the best way to supply and consume information on everything from rare ailments to subway schedules. The mass publishing of information on the Web is essentially useless unless this wealth of information can be discovered and consumed by other users. Early attempts at making web information “discoverable” fell into two broad categories: (1) full-text index search engines such as Altavista, Excite and Infoseek and (2) taxonomies populated with web pages in categories, such as Yahoo! The former presented the user with a keyword search interface supported by inverted indexes and ranking mechanisms building on those introduced in earlier chapters. The latter allowed the user to browse through a hierarchical tree of category labels. While this is at first blush a convenient and intuitive metaphor for finding web pages, it has a number of drawbacks: first, accurately classifying web pages into taxonomy tree nodes is for the most part a manual editorial process, which is difficult to scale with the size of the Web. Arguably, we only need to have “high-quality”      ",19.1
2264,iir,iir-2264,19.2 Web characteristics," 19.2 Web characteristics 423 web pages in the taxonomy, with only the best web pages for each category. However, just discovering these and classifying them accurately and consistently into the taxonomy entails significant human effort. Furthermore, in order for a user to effectively discover web pages classified into the nodes of the taxonomy tree, the user’s idea of what sub-tree(s) to seek for a particular topic should match that of the editors performing the classification. This quickly becomes challenging as the size of the taxonomy grows; the Yahoo! taxonomy tree surpassed 1000 distinct nodes fairly early on. Given these challenges, the popularity of taxonomies declined over time, even though variants (such as About.com and the Open Directory Project) sprang up with subject-matter experts collecting and annotating web pages for each category. The first generation of web search engines transported classical search techniques such as those in the preceding chapters to the web domain, focusing on the challenge of scale. The earliest web search engines had to contend with indexes containing tens of millions of documents, which was a few orders of magnitude larger than any prior information retrieval system in the public domain. Indexing, query serving and ranking at this scale required the harnessing together of tens of machines to create highly available systems, again at scales not witnessed hitherto in a consumer-facing search application. The first generation of web search engines was largely successful at solving these challenges while continually indexing a significant fraction of the Web, all the while serving queries with sub-second response times. However, the quality and relevance of web search results left much to be desired owing to the idiosyncrasies of content creation on the Web that we discuss in Section 19.2. This necessitated the invention of new ranking and spam-fighting techniques in order to ensure the quality of the search results. While classical information retrieval techniques (such as those covered earlier in this book) continue to be necessary for web search, they are not by any means sufficient. A key aspect (developed further in Chapter 21) is that whereas classical techniques measure the relevance of a document to a query, there remains a need to gauge the authoritativeness of a document based on cues such as which website hosts it. 19.2 Web characteristics The essential feature that led to the explosive growth of the web – decentralized content publishing with essentially no central control of authorship – turned out to be the biggest challenge for web search engines in their quest to index and retrieve this content. Web page authors created content in dozens of (natural) languages and thousands of dialects, thus demanding many different forms of stemming and other linguistic operations. Because publish     424 19 Web search basics ing was now open to tens of millions, web pages exhibited heterogeneity at a daunting scale, in many crucial aspects. First, content-creation was no longer the privy of editorially-trained writers; while this represented a tremendous democratization of content creation, it also resulted in a tremendous variation in grammar and style (and in many cases, no recognizable grammar or style). Indeed, web publishing in a sense unleashed the best and worst of desktop publishing on a planetary scale, so that pages quickly became riddled with wild variations in colors, fonts and structure. Some web pages, including the professionally created home pages of some large corporations, consisted entirely of images (which, when clicked, led to richer textual content) – and therefore, no indexable text. What about the substance of the text in web pages? The democratization of content creation on the web meant a new level of granularity in opinion on virtually any subject. This meant that the web contained truth, lies, contradictions and suppositions on a grand scale. This gives rise to the question: which web pages does one trust? In a simplistic approach, one might argue that some publishers are trustworthy and others not – begging the question of how a search engine is to assign such a measure of trust to each website or web page. In Chapter 21 we will examine approaches to understanding this question. More subtly, there may be no universal, user-independent notion of trust; a web page whose contents are trustworthy to one user may not be so to another. In traditional (non-web) publishing this is not an issue: users self-select sources they find trustworthy. Thus one reader may find the reporting of The New York Times to be reliable, while another may prefer The Wall Street Journal. But when a search engine is the only viable means for a user to become aware of (let alone select) most content, this challenge becomes significant. While the question “how big is the Web?” has no easy answer (see Section 19.5), the question “how many web pages are in a search engine’s index” is more precise, although, even this question has issues. By the end of 1995, Altavista reported that it had crawled and indexed approximately 30 million static web pages. Static web pages are those whose content does not vary from PAGES one request for that page to the next. For this purpose, a professor who manually updates his home page every week is considered to have a static web page, but an airport’s flight status page is considered to be dynamic. Dynamic pages are typically mechanically generated by an application server in response to a query to a database, as show in Figure 19.1. One sign of such a page is that the URL has the character ""?"" in it. Since the number of static web pages was believed to be doubling every few months in 1995, early web search engines such as Altavista had to constantly add hardware and bandwidth for crawling and indexing web pages.   19.2 Web characteristics 425 ◮Figure 19.1 A dynamically generated web page. The browser sends a request for flight information on flight AA129 to the web application, that fetches the information from back-end databases then creates a dynamic web page that it returns to the browser. &amp;% '$ &amp;% '$ anchor ◮Figure 19.2 Two nodes of the web graph joined by a link. 19.2.1 The web graph We can view the s ",19.2
2265,iir,iir-2265,19.2.1 The web graph," 19.2.1 The web graph We can view the static Web consisting of static HTML pages together with the hyperlinks between them as a directed graph in which each web page is a node and each hyperlink a directed edge. Figure 19.2 shows two nodes A and B from the web graph, each corresponding to a web page, with a hyperlink from A to B. We refer to the set of all such nodes and directed edges as the web graph. Figure 19.2 also shows that (as is the case with most links on web pages) there is some text surrounding the origin of the hyperlink on page A. This text is generally encapsulated in the href attribute of the &lt;a&gt; (for anchor) tag that encodes the hyperlink in the HTML code of page A, and is referred to as anchor text. As one might suspect, this directed graph is not strongly connected: there are pairs of pages such that one cannot proceed from one page of the pair to the other by following hyperlinks. We refer to the hyperlinks into a page as in-links and thoseIN-LINKS out of a page as out-links. The number of in-links to a page (also known as its in-degree) has averaged from roughly 8 to 15, in a range of studies. We similarly define the out-degree of a web page to be the number of links out      426 19 Web search basics ◮Figure 19.3 A sample small web graph. In this example we have six pages labeled A-F. Page B has in-degree 3 and out-degree 1. This example graph is not strongly connected: there is no path from any of pages B-F to page A. of it. These notions are represented in Figure 19.3. There is ample evidence that these links are not randomly distributed; for one thing, the distribution of the number of links into a web page does not follow the Poisson distribution one would expect if every web page were to pick the destinations of its links uniformly at random. Rather, this distribution is widely reported to be a power law, in which the total number of web pages with in-degree iis proportional to 1/iα; the value of αtypically reported by studies is 2.1.1Furthermore, several studies have suggested that the directed graph connecting web pages has a bowtie shape: there are three major categories of web pages that are sometimes referred to as IN, OUT and SCC. A web surfer can pass from any page in IN to any page in SCC, by following hyperlinks. Likewise, a surfer can pass from page in SCC to any page in OUT. Finally, the surfer can surf from any page in SCC to any other page in SCC. However, it is not possible to pass from a page in SCC to any page in IN, or from a page in OUT to a page in SCC (or, consequently, IN). Notably, in several studies IN and OUT are roughly equal in size, whereas 1. Cf. Zipf’s law of the distribution of words in text in Chapter 5(page 90), which is a power law with α=1.      19.2 Web characteristics 427 ◮Figure 19.4 The bowtie structure of the Web. Here we show one tube and three tendrils. SCC is somewhat larger; most web pages fall into one of these three sets. The remaining pages form into tubes that are small sets of pages outside SCC that lead directly from IN to OUT, and tendrils that either lead nowhere from IN, or from nowhere to OUT. Figure 19.4 illustrates this structure of the Web.  ",19.2
2266,iir,iir-2266,19.2.2 Spam," 19.2.2 Spam Early in the history of web search, it became clear that web search engines were an important means for connecting advertisers to prospective buyers. A user searching for maui golf real estate is not merely seeking news or entertainment on the subject of housing on golf courses on the island of Maui, but instead likely to be seeking to purchase such a property. Sellers of such property and their agents, therefore, have a strong incentive to create web pages that rank highly on this query. In a search engine whose scoring was based on term frequencies, a web page with numerous repetitions of maui golf real estate would rank highly. This led to the first generation of spam, which (in the context of web search) is the manipulation of web page content for the purpose of appearing high up in search results for selected keywords. To avoid irritating users with these repetitions, sophisticated spammers resorted to such tricks as rendering these repeated terms in the same color as the background. Despite these words being consequently invisible to the human user, a search engine indexer would parse the invisible words out of      428 19 Web search basics ◮Figure 19.5 Cloaking as used by spammers. the HTML representation of the web page and index these words as being present in the page. At its root, spam stems from the heterogeneity of motives in content creation on the Web. In particular, many web content creators have commercial motives and therefore stand to gain from manipulating search engine results. You might argue that this is no different from a company that uses large fonts to list its phone numbers in the yellow pages; but this generally costs the company more and is thus a fairer mechanism. A more apt analogy, perhaps, is the use of company names beginning with a long string of A’s to be listed early in a yellow pages category. In fact, the yellow pages’ model of companies paying for larger/darker fonts has been replicated in web search: in many search engines, it is possible to pay to have one’s web page included in the search engine’s index – a model known as paid inclusion. Different search engines have different policies on whether to allow paid inclusion, and whether such a payment has any effect on ranking in search results. Search engines soon became sophisticated enough in their spam detection to screen out a large number of repetitions of particular keywords. Spammers responded with a richer set of spam techniques, the best known of which we now describe. The first of these techniques is cloaking, shown in Figure 19.5. Here, the spammer’s web server returns different pages depending on whether the http request comes from a web search engine’s crawler (the part of the search engine that gathers web pages, to be described in Chapter 20), or from a human user’s browser. The former causes the web page to be indexed by the search engine under misleading keywords. When the user searches for these keywords and elects to view the page, he receives a web page that has altogether different content than that indexed by the search engine. Such deception of search indexers is unknown in the traditional world of information retrieval; it stems from the fact that the relationship between page publishers and web search engines is not completely collaborative. Adoorway page contains text and metadata carefully chosen to rank highly      ",19.2
2267,iir,iir-2267,19.3 Advertising as the economic model," 19.3 Advertising as the economic model 429 on selected search keywords. When a browser requests the doorway page, it is redirected to a page containing content of a more commercial nature. More complex spamming techniques involve manipulation of the metadata related to a page including (for reasons we will see in Chapter 21) the links into a web page. Given that spamming is inherently an economically motivated activity, there has sprung around it an industry of Search Engine Optimizers,SEARCH ENGINE OPTIMIZERS or SEOs to provide consultancy services for clients who seek to have their web pages rank highly on selected keywords. Web search engines frown on this business of attempting to decipher and adapt to their proprietary ranking techniques and indeed announce policies on forms of SEO behavior they do not tolerate (and have been known to shut down search requests from certain SEOs for violation of these). Inevitably, the parrying between such SEOs (who gradually infer features of each web search engine’s ranking methods) and the web search engines (who adapt in response) is an unending struggle; indeed, the research sub-area of adversarial information retrieval has sprung up around this battle. To combat spammers who manipulate the text of their web pages is the exploitation of the link structure of the Web – a technique known as link analysis. The first web search engine known to apply link analysis on a large scale (to be detailed in Chapter 21) was Google, although all web search engines currently make use of it (and correspondingly, spammers now invest considerable effort in subverting it – this is known as link spam). ?Exercise 19.1 If the number of pages with in-degree iis proportional to 1/i2.1, what is the probability that a randomly chosen web page has in-degree 1? Exercise 19.2 If the number of pages with in-degree iis proportional to 1/i2.1, what is the average in-degree of a web page? Exercise 19.3 If the number of pages with in-degree iis proportional to 1/i2.1, then as the largest in-degree goes to infinity, does the fraction of pages with in-degree igrow, stay the same, or diminish? How would your answer change for values of the exponent other than 2.1? Exercise 19.4 The average in-degree of all nodes in a snapshot of the web graph is 9. What can we say about the average out-degree of all nodes in this snapshot? 19.3 Advertising as the economic model Early in the history of the Web, companies used graphical banner advertisements on web pages at popular websites (news and entertainment sites such as MSN, America Online, Yahoo! and CNN). The primary purpose of these advertisements was branding: to convey to the viewer a positive feeling about     430 19 Web search basics the brand of the company placing the advertisement. Typically these advertisements are priced on a cost per mil (CPM) basis: the cost to the company of having its banner advertisement displayed 1000 times. Some websites struck contracts with their advertisers in which an advertisement was priced not by the number of times it is displayed (also known as impressions), but rather by the number of times it was clicked on by the user. This pricing model is known as the cost per click (CPC) model. In such cases, clicking on the adver- CPC tisement leads the user to a web page set up by the advertiser, where the user is induced to make a purchase. Here the goal of the advertisement is not so much brand promotion as to induce a transaction. This distinction between brand and transaction-oriented advertising was already widely recognized in the context of conventional media such as broadcast and print. The interactivity of the web allowed the CPC billing model – clicks could be metered and monitored by the website and billed to the advertiser. The pioneer in this direction was a company named Goto, which changed its name to Overture prior to eventual acquisition by Yahoo! Goto was not, in the traditional sense, a search engine; rather, for every query term qit accepted bids from companies who wanted their web page shown on the query q. In response to the query q, Goto would return the pages of all advertisers who bid for q, ordered by their bids. Furthermore, when the user clicked on one of the returned results, the corresponding advertiser would make a payment to Goto (in the initial implementation, this payment equaled the advertiser’s bid for q). Several aspects of Goto’s model are worth highlighting. First, a user typing the query qinto Goto’s search interface was actively expressing an interest and intent related to the query q. For instance, a user typing golf clubs is more likely to be imminently purchasing a set than one who is simply browsing news on golf. Second, Goto only got compensated when a user actually expressed interest in an advertisement – as evinced by the user clicking the advertisement. Taken together, these created a powerful mechanism by which to connect advertisers to consumers, quickly raising the annual revenues of Goto/Overture into hundreds of millions of dollars. This style of search engine came to be known variously as sponsored search or search advertising.SPONSORED SEARCH SEARCH ADVERTISING Given these two kinds of search engines – the “pure” search engines such as Google and Altavista, versus the sponsored search engines – the logical next step was to combine them into a single user experience. Current search engines follow precisely this model: they provide pure search results (generally known as algorithmic search results) as the primary response to a  user’s search, together with sponsored search results displayed separately and distinctively to the right of the algorithmic results. This is shown in Figure 19.6. Retrieving sponsored search results and ranking them in response to a query has now become considerably more sophisticated than the simple Goto scheme; the process entails a blending of ideas from information   19.3 Advertising as the economic model 431 ◮Figure 19.6 Search advertising triggered by query keywords. Here the query A320 returns algorithmic search results about the Airbus aircraft, together with advertisements for various non-aircraft goods numbered A320, that advertisers seek to market to those querying on this query. The lack of advertisements for the aircraft reflects the fact that few marketers attempt to sell A320 aircraft on the web. retrieval and microeconomics, and is beyond the scope of this book. For advertisers, understanding how search engines do this ranking and how to allocate marketing campaign budgets to different keywords and to different sponsored search engines has become a profession known as search engine MARKETING marketing (SEM). The inherently economic motives underlying sponsored search give rise to attempts by some participants to subvert the system to their advantage. This can take many forms, one of which is known as click spam. There is currently no universally accepted definition of click spam. It refers (as the name suggests) to clicks on sponsored search results that are not from bona fide search users. For instance, a devious advertiser may attempt to exhaust the advertising budget of a competitor by clicking repeatedly (through the use of a robotic click generator) on that competitor’s sponsored search advertisements. Search engines face the challenge of discerning which of the clicks they observe are part of a pattern of click spam, to avoid charging their advertiser clients for such clicks. ?Exercise 19.5 The Goto method ranked advertisements matching a query by bid: the highest- bidding advertiser got the top position, the second-highest the next, and so on. What can go wrong with this when the highest-bidding advertiser places an advertisement that is irrelevant to the query? Why might an advertiser with an irrelevant advertisement bid high in this manner? Exercise 19.6 Suppose that, in addition to bids, we had for each advertiser their click- through rate: the ratio of the historical number of times users click on their advertisement to the number of times the advertisement was shown. Suggest a modification of the Goto scheme that exploits this data to avoid the problem in Exercise 19.5 above.     432 19 Web search basics  ",19.3
2268,iir,iir-2268,19.4 The search user experience," 19.4 The search user experience It is crucial that we understand the users of web search as well. This is again a significant change from traditional information retrieval, where users were typically professionals with at least some training in the art of phrasing queries over a well-authored collection whose style and structure they understood well. In contrast, web search users tend to not know (or care) about the heterogeneity of web content, the syntax of query languages and the art of phrasing queries; indeed, a mainstream tool (as web search has come to become) should not place such onerous demands on billions of people. A range of studies has concluded that the average number of keywords in a web search is somewhere between 2 and 3. Syntax operators (Boolean connectives, wildcards, etc.) are seldom used, again a result of the composition of the audience – “normal” people, not information scientists. It is clear that the more user traffic a web search engine can attract, the more revenue it stands to earn from sponsored search. How do search engines differentiate themselves and grow their traffic? Here Google identified two principles that helped it grow at the expense of its competitors: (1) a focus on relevance, specifically precision rather than recall in the first few results; (2) a user experience that is lightweight, meaning that both the search query page and the search results page are uncluttered and almost entirely textual, with very few graphical elements. The effect of the first was simply to save users time in locating the information they sought. The effect of the second is to provide a user experience that is extremely responsive, or at any rate not bottlenecked by the time to load the search query or results page. 19.4.1 User query needs There appear to be three broad categor ",19.4
2269,iir,iir-2269,19.4.1 User query needs," 19.4.1 User query needs There appear to be three broad categories into which common web search queries can be grouped: (i) informational, (ii) navigational and (iii) transactional. We now explain these categories; it should be clear that some queries will fall in more than one of these categories, while others will fall outside them. Informational queries seek general information on a broad topic, such as leukemia or Provence. There is typically not a single web page that contains all the information sought; indeed, users with informational queries typically try to assimilate information from multiple web pages. Navigational queries seek the website or home page of a single entity that the user has in mind, say Lufthansa airlines. In such cases, the user’s expectation is that the very first search result should be the home page of Lufthansa. The user is not interested in a plethora of documents containing the term Lufthansa; for such a user, the best measure of user satisfaction is precision at 1.      ",19.4
2270,iir,iir-2270,19.5 Index size and estimation," 19.5 Index size and estimation 433 Atransactional query is one that is a prelude to the user performing a trans- TRANSACTIONAL QUERY action on the Web – such as purchasing a product, downloading a file or making a reservation. In such cases, the search engine should return results listing services that provide form interfaces for such transactions. Discerning which of these categories a query falls into can be challenging. The category not only governs the algorithmic search results, but the suitability of the query for sponsored search results (since the query may reveal an intent to purchase). For navigational queries, some have argued that the search engine should return only a single result or even the target web page directly. Nevertheless, web search engines have historically engaged in a battle of bragging rights over which one indexes more web pages. Does the user really care? Perhaps not, but the media does highlight estimates (often statistically indefensible) of the sizes of various search engines. Users are influenced by these reports and thus, search engines do have to pay attention to how their index sizes compare to competitors’. For informational (and to a lesser extent, transactional) queries, the user does care about the comprehensiveness of the search engine. Figure 19.7 shows a composite picture of a web search engine including the crawler, as well as both the web page and advertisement indexes. The portion of the figure under the curved dashed line is internal to the search engine. 19.5 Index size and estimation To a first approximation, comprehensiveness grows with index size, although it does matter which specific pages a search engine indexes – some pages are more informative than others. It is also difficult to reason about the fraction of the Web indexed by a search engine, because there is an infinite number of dynamic web pages; for instance, http://www.yahoo.com/any_string returns a valid HTML page rather than an error, politely informing the user that there is no such page at Yahoo! Such a ""soft 404 error"" is only one example of many ways in which web servers can generate an infinite number of valid web pages. Indeed, some of these are malicious spider traps devised to cause a search engine’s crawler (the component that systematically gathers web pages for the search engine’s index, described in Chapter 20) to stay within a spammer’s website and index many pages from that site. We could ask the following better-defined question: given two search engines, what are the relative sizes of their indexes? Even this question turns out to be imprecise, because: 1\. In response to queries a search engine can return web pages whose contents it has not (fully or even partially) indexed. For one thing, search engines generally index only the first few thousand words in a web page.      434 19 Web search basics ◮Figure 19.7 The various components of a web search engine. In some cases, a search engine is aware of a page pthat is linked to by pages it has indexed, but has not indexed pitself. As we will see in Chapter 21, it is still possible to meaningfully return pin search results. 2\. Search engines generally organize their indexes in various tiers and partitions, not all of which are examined on every search (recall tiered indexes from Section 7.2.1). For instance, a web page deep inside a website may be indexed but not retrieved on general web searches; it is however retrieved as a result on a search that a user has explicitly restricted to that website (such site-specific search is offered by most web search engines). Thus, search engine indexes include multiple classes of indexed pages, so that there is no single measure of index size. These issues notwithstanding, a number of techniques have been devised for crude estimates of the ratio of the index sizes of two search engines, E1and E2. The basic hypothesis underlying these techniques is that each search engine indexes a fraction of the Web chosen independently and uniformly at random. This involves some questionable assumptions: first, that there is a finite size for the Web from which each search engine chooses a subset, and second, that each engine chooses an independent, uniformly chosen subset. As will be clear from the discussion of crawling in Chapter 20, this is far from true. However, if we begin   19.5 Index size and estimation 435 with these assumptions, then we can invoke a classical estimation technique known as the capture-recapture method.CAPTURE-RECAPTURE METHOD Suppose that we could pick a random page from the index of E1and test whether it is in E2’s index and symmetrically, test whether a random page from E2is in E1. These experiments give us fractions xand ysuch that our estimate is that a fraction xof the pages in E1are in E2, while a fraction yof the pages in E2are in E1. Then, letting |Ei|denote the size of the index of search engine Ei, we have x|E1| ≈ y|E2|, from which we have the form we will use |E1| |E2|≈y x. (19.1) If our assumption about E1and E2being independent and uniform random subsets of the Web were true, and our sampling process unbiased, then Equation (19.1) should give us an unbiased estimator for |E1|/|E2|. We distinguish between two scenarios here. Either the measurement is performed by someone with access to the index of one of the search engines (say an employee of E1), or the measurement is performed by an independent party with no access to the innards of either search engine. In the former case, we can simply pick a random document from one index. The latter case is more challenging; by picking a random page from one search engine from outside the search engine, then verify whether the random page is present in the other search engine. To implement the sampling phase, we might generate a random page from the entire (idealized, finite) Web and test it for presence in each search engine. Unfortunately, picking a web page uniformly at random is a difficult problem. We briefly outline several attempts to achieve such a sample, pointing out the biases inherent to each; following this we describe in some detail one technique that much research has built on. 1\. Random searches: Begin with a search log of web searches; send a random search from this log to E1and a random page from the results. Since such logs are not widely available outside a search engine, one implementation is to trap all search queries going out of a work group (say scientists in a research center) that agrees to have all its searches logged. This approach has a number of issues, including the bias from the types of searches made by the work group. Further, a random document from the results of such a random search to E1is not the same as a random document from E1. 2\. Random IP addresses: A second approach is to generate random IP addresses and send a request to a web server residing at the random address, collecting all pages at that server. The biases here include the fact     436 19 Web search basics that many hosts might share one IP (due to a practice known as virtual hosting) or not accept http requests from the host where the experiment is conducted. Furthermore, this technique is more likely to hit one of the many sites with few pages, skewing the document probabilities; we may be able to correct for this effect if we understand the distribution of the number of pages on websites. 3\. Random walks: If the web graph were a strongly connected directed graph, we could run a random walk starting at an arbitrary web page. This walk would converge to a steady state distribution (see Chapter 21, Section 21.2.1 for more background material on this), from which we could in principle pick a web page with a fixed probability. This method, too has a number of biases. First, the Web is not strongly connected so that, even with various corrective rules, it is difficult to argue that we can reach a steady state distribution starting from any page. Second, the time it takes for the random walk to settle into this steady state is unknown and could exceed the length of the experiment. Clearly each of these approaches is far from perfect. We now describe a fourth sampling approach, random queries. This approach is noteworthy for two reasons: it has been successfully built upon for a series of increasingly refined estimates, and conversely it has turned out to be the approach most likely to be misinterpreted and carelessly implemented, leading to misleading measurements. The idea is to pick a page (almost) uniformly at random from a search engine’s index by posing a random query to it. It should be clear that picking a set of random terms from (say) Webster’s dictionary is not a good way of implementing this idea. For one thing, not all vocabulary terms occur equally often, so this approach will not result in documents being chosen uniformly at random from the search engine. For another, there are a great many terms in web documents that do not occur in a standard dictionary such as Webster’s. To address the problem of vocabulary terms not in a standard dictionary, we begin by amassing a sample web dictionary. This could be done by crawling a limited portion of the Web, or by crawling a manually-assembled representative subset of the Web such as Yahoo! (as was done in the earliest experiments with this method). Consider a conjunctive query with two or more randomly chosen words from this dictionary. Operationally, we proceed as follows: we use a random conjunctive query on E1and pick from the top 100 returned results a page pat random. We then test pfor presence in E2by choosing 6-8 low-frequency terms in pand using them in a conjunctive query for E2. We can improve the estimate by repeating the experiment a large number of times. Both the sampling process and the testing process have a number of issues. 1\. Our sample is biased towards longer documents.     19.6 Near-duplicates and shingling 437 2\. Picking from the top 100 results of E1induces a bias from the ranking algorithm of E1. Picking from all the results of E1makes the experiment slower. This is particularly so because most web search engines put up defenses against excessive robotic querying. 3\. During the checking phase, a number of additional biases are introduced: for instance, E2may not handle 8-word conjunctive queries properly. 4\. Either E1or E2may refuse to respond to the test queries, treating them as robotic spam rather than as bona fide queries. 5\. There could be operational problems like connection time-outs. A sequence of research has built on this basic paradigm to eliminate some of these issues; there is no perfect solution yet, but the level of sophistication in statistics for understanding the biases is increasing. The main idea is to address biases by estimating, for each document, the magnitude of the bias. From this, standard statistical sampling methods can generate unbiased samples. In the checking phase, the newer work moves away from conjunctive queries to phrase and other queries that appear to be betterbehaved. Finally, newer experiments use other sampling methods besides random queries. The best known of these is document random walk sampling, in which a document is chosen by a random walk on a virtual graph derived from documents. In this graph, nodes are documents; two documents are connected by an edge if they share two or more words in common. The graph is never instantiated; rather, a random walk on it can be performed by moving from a document dto another by picking a pair of keywords in d, running a query on a search engine and picking a random document from the results. Details may be found in the references in Section 19.7. ?Exercise 19.7 Two web search engines A and B each generate a large number of pages uniformly at random from their indexes. 30% of A’s pages are present in B’s index, while 50% of B’s pages are present in A’s index. What is the number of pages in A’s index relative to B’s? 19.6 Near-duplicates and shingling One aspect we have ignored in the discussion of index size in Section 19.5 is duplication: the Web contains multiple copies of the same content. By some estimates, as many as 40% of the pages on the Web are duplicates of other pages. Many of these are legitimate copies; for instance, certain information repositories are mirrored simply to provide redundancy and access reliability. Search engines try to avoid indexing multiple copies of the same content, to keep down storage and processing overheads.  ",19.5
2271,iir,iir-2271,19.6 Near-duplicatesand shingling,"   19.6 Near-duplicates and shingling 437 2\. Picking from the top 100 results of E1induces a bias from the ranking algorithm of E1. Picking from all the results of E1makes the experiment slower. This is particularly so because most web search engines put up defenses against excessive robotic querying. 3\. During the checking phase, a number of additional biases are introduced: for instance, E2may not handle 8-word conjunctive queries properly. 4\. Either E1or E2may refuse to respond to the test queries, treating them as robotic spam rather than as bona fide queries. 5\. There could be operational problems like connection time-outs. A sequence of research has built on this basic paradigm to eliminate some of these issues; there is no perfect solution yet, but the level of sophistication in statistics for understanding the biases is increasing. The main idea is to address biases by estimating, for each document, the magnitude of the bias. From this, standard statistical sampling methods can generate unbiased samples. In the checking phase, the newer work moves away from conjunctive queries to phrase and other queries that appear to be betterbehaved. Finally, newer experiments use other sampling methods besides random queries. The best known of these is document random walk sampling, in which a document is chosen by a random walk on a virtual graph derived from documents. In this graph, nodes are documents; two documents are connected by an edge if they share two or more words in common. The graph is never instantiated; rather, a random walk on it can be performed by moving from a document dto another by picking a pair of keywords in d, running a query on a search engine and picking a random document from the results. Details may be found in the references in Section 19.7. ?Exercise 19.7 Two web search engines A and B each generate a large number of pages uniformly at random from their indexes. 30% of A’s pages are present in B’s index, while 50% of B’s pages are present in A’s index. What is the number of pages in A’s index relative to B’s? 19.6 Near-duplicates and shingling One aspect we have ignored in the discussion of index size in Section 19.5 is duplication: the Web contains multiple copies of the same content. By some estimates, as many as 40% of the pages on the Web are duplicates of other pages. Many of these are legitimate copies; for instance, certain information repositories are mirrored simply to provide redundancy and access reliability. Search engines try to avoid indexing multiple copies of the same content, to keep down storage and processing overheads.     438 19 Web search basics The simplest approach to detecting duplicates is to compute, for each web page, a fingerprint that is a succinct (say 64-bit) digest of the characters on that page. Then, whenever the fingerprints of two web pages are equal, we test whether the pages themselves are equal and if so declare one of them to be a duplicate copy of the other. This simplistic approach fails to capture a crucial and widespread phenomenon on the Web: near duplication. In many cases, the contents of one web page are identical to those of another except for a few characters – say, a notation showing the date and time at which the page was last modified. Even in such cases, we want to be able to declare the two pages to be close enough that we only index one copy. Short of exhaustively comparing all pairs of web pages, an infeasible task at the scale of billions of pages, how can we detect and filter out such near duplicates? We now describe a solution to the problem of detecting near-duplicate web pages. The answer lies in a technique known as shingling. Given a positive integer kand a sequence of terms in a document d, define the k-shingles of dto be the set of all consecutive sequences of kterms in d. As an example, consider the following text: a rose is a rose is a rose. The 4-shingles for this text (k=4 is a typical value used in the detection of near-duplicate web pages) are a rose is a,rose is a rose and is a rose is. The first two of these shingles each occur twice in the text. Intuitively, two documents are near duplicates if the sets of shingles generated from them are nearly the same. We now make this intuition precise, then develop a method for efficiently computing and comparing the sets of shingles for all web pages. Let S(dj)denote the set of shingles of document dj. Recall the Jaccard coefficient from page 61, which measures the degree of overlap between the sets S(d1)and S(d2)as |S(d1)∩S(d2)|/|S(d1)∪S(d2)|; denote this by J(S(d1),S(d2)). Our test for near duplication between d1and d2is to compute this Jaccard coefficient; if it exceeds a preset threshold (say, 0.9), we declare them near duplicates and eliminate one from indexing. However, this does not appear to have simplified matters: we still have to compute Jaccard coefficients pairwise. To avoid this, we use a form of hashing. First, we map every shingle into a hash value over a large space, say 64 bits. For j=1, 2, let H(dj)be the corresponding set of 64-bit hash values derived from S(dj). We now invoke the following trick to detect document pairs whose sets H() have large Jaccard overlaps. Let πbe a random permutation from the 64-bit integers to the 64-bit integers. Denote by Π(dj)the set of permuted hash values in H(dj); thus for each h∈H(dj), there is a corresponding value π(h)∈Π(dj). Let xπ jbe the smallest integer in Π(dj). Then Theorem 19.1. J(S(d1),S(d2)) = P(xπ 1=xπ 2).      19.6 Near-duplicates and shingling 439 0 0 0 0 0 0 0 0 264 −1 264 −1 264 −1 264 −1 264 −1 264 −1 264 −1 264 −1 Document 1 Document 2 H(d1)H(d2) u 1u 1 u 2u 2 u 3u 3 u 4u 4 H(d1)and Π(d1)H(d2)and Π(d2) u uu uu uu u 3 31 14 42 2 3 31 14 42 2 3 3 Π(d1)Π(d2) xπ 1xπ 2 ◮Figure 19.8 Illustration of shingle sketches. We see two documents going through four stages of shingle sketch computation. In the first step (top row), we apply a 64-bit hash to each shingle from each document to obtain H(d1)and H(d2)(circles). Next, we apply a random permutation Πto permute H(d1)and H(d2), obtaining Π(d1) and Π(d2)(squares). The third row shows only Π(d1)and Π(d2), while the bottom row shows the minimum values xπ 1and xπ 2for each document. Proof. We give the proof in a slightly more general setting: consider a family of sets whose elements are drawn from a common universe. View the sets as columns of a matrix A, with one row for each element in the universe. The element aij =1 if element iis present in the set Sjthat the jth column represents. Let Πbe a random permutation of the rows of A; denote by Π(Sj)the column that results from applying Πto the jth column. Finally, let xπ jbe the index of the first row in which the column Π(Sj)has a 1. We then prove that for any two columns j1,j2, P(xπ j1=xπ j2) = J(Sj1,Sj2). If we can prove this, the theorem follows. Consider two columns j1,j2as shown in Figure 19.9. The ordered pairs of entries of Sj1and Sj2partition the rows into four types: those with 0’s in both of these columns, those with a 0 in Sj1and a 1 in Sj2, those with a 1 in Sj1 and a 0 in Sj2, and finally those with 1’s in both of these columns. Indeed, the first four rows of Figure 19.9 exemplify all of these four types of rows.      440 19 Web search basics Sj1Sj2 0 1 1 0 1 1 0 0 1 1 0 1 ◮Figure 19.9 Two sets Sj1and Sj2; their Jaccard coefficient is 2/5. Denote by C00 the number of rows with 0’s in both columns, C01 the second, C10 the third and C11 the fourth. Then, J(Sj1,Sj2) = C11 C01 +C10 +C11 . (19.2) To complete the proof by showing that the right-hand side of Equation (19.2) equals P(xπ j1=xπ j2), consider scanning columns j1,j2in increasing row index until the first non-zero entry is found in either column. Because Πis a random permutation, the probability that this smallest row has a 1 in both columns is exactly the right-hand side of Equation (19.2). Thus, our test for the Jaccard coefficient of the shingle sets is probabilistic: we compare the computed values xπ ifrom different documents. If a pair coincides, we have candidate near duplicates. Repeat the process independently for 200 random permutations π(a choice suggested in the literature). Call the set of the 200 resulting values of xπ ithe sketch ψ(di)of di. We can then estimate the Jaccard coefficient for any pair of documents di,djto be |ψi∩ψj|/200; if this exceeds a preset threshold, we declare that diand djare similar. How can we quickly compute |ψi∩ψj|/200 for all pairs i,j? Indeed, how do we represent all pairs of documents that are similar, without incurring a blowup that is quadratic in the number of documents? First, we use fingerprints to remove all but one copy of identical documents. We may also remove common HTML tags and integers from the shingle computation, to eliminate shingles that occur very commonly in documents without telling us anything about duplication. Next we use a union-find algorithm to create clusters that contain documents that are similar. To do this, we must accomplish a crucial step: going from the set of sketches to the set of pairs i,jsuch that diand djare similar. To this end, we compute the number of shingles in common for any pair of documents whose sketches have any members in common. We begin with the list &lt;xπ i,di&gt;sorted by xπ ipairs. For each xπ i, we can now generate      ",19.6
2272,iir,iir-2272,19.7 References and further reading," 19.7 References and further reading 441 all pairs i,jfor which xπ iis present in both their sketches. From these we can compute, for each pair i,jwith non-zero sketch overlap, a count of the number of xπ ivalues they have in common. By applying a preset threshold, we know which pairs i,jhave heavily overlapping sketches. For instance, if the threshold were 80%, we would need the count to be at least 160 for any i,j. As we identify such pairs, we run the union-find to group documents into near-duplicate “syntactic clusters”. This is essentially a variant of the single-link clustering algorithm introduced in Section 17.2 (page 382). One final trick cuts down the space needed in the computation of |ψi∩ ψj|/200 for pairs i,j, which in principle could still demand space quadratic in the number of documents. To remove from consideration those pairs i,j whose sketches have few shingles in common, we preprocess the sketch for each document as follows: sort the xπ iin the sketch, then shingle this sorted sequence to generate a set of super-shingles for each document. If two documents have a super-shingle in common, we proceed to compute the precise value of |ψi∩ψj|/200. This again is a heuristic but can be highly effective in cutting down the number of i,jpairs for which we accumulate the sketch overlap counts. ?Exercise 19.8 Web search engines A and B each crawl a random subset of the same size of the Web. Some of the pages crawled are duplicates – exact textual copies of each other at different URLs. Assume that duplicates are distributed uniformly amongst the pages crawled by A and B. Further, assume that a duplicate is a page that has exactly two copies – no pages have more than two copies. A indexes pages without duplicate elimination whereas B indexes only one copy of each duplicate page. The two random subsets have the same size before duplicate elimination. If, 45% of A’s indexed URLs are present in B’s index, while 50% of B’s indexed URLs are present in A’s index, what fraction of the Web consists of pages that do not have a duplicate? Exercise 19.9 Instead of using the process depicted in Figure 19.8, consider instead the following process for estimating the Jaccard coefficient of the overlap between two sets S1and S2. We pick a random subset of the elements of the universe from which S1and S2 are drawn; this corresponds to picking a random subset of the rows of the matrix Ain the proof. We exhaustively compute the Jaccard coefficient of these random subsets. Why is this estimate an unbiased estimator of the Jaccard coefficient for S1and S2? Exercise 19.10 Explain why this estimator would be very difficult to use in practice. 19.7 References and further reading Bush (1945) foreshadowed the Web when he described an information management system that he called memex.Berners-Lee et al. (1992) describes one of the earliest incarnations of the Web. Kumar et al. (2000) and Broder     442 19 Web search basics et al. (2000) provide comprehensive studies of the Web as a graph. The use of anchor text was first described in McBryan (1994). The taxonomy of web queries in Section 19.4 is due to Broder (2002). The observation of the power law with exponent 2.1 in Section 19.2.1 appeared in Kumar et al. (1999). Chakrabarti (2002) is a good reference for many aspects of web search and analysis. The estimation of web search index sizes has a long history of development covered by Bharat and Broder (1998), Lawrence and Giles (1998), Rusmevichientong et al. (2001), Lawrence and Giles (1999), Henzinger et al. (2000), Bar-Yossef and Gurevich (2006). The state of the art is Bar-Yossef and Gurevich (2006), including several of the bias-removal techniques mentioned at the end of Section 19.5. Shingling was introduced by Broder et al. (1997) and used for detecting websites (rather than simply pages) that are identical by Bharat et al. (2000).  ",19.7
2386,iir,iir-2386,2 The term vocabulary and postings lists,"       19 2The term vocabulary and postings lists Recall the major steps in inverted index construction: 1\. Collect the documents to be indexed. 2\. Tokenize the text. 3\. Do linguistic preprocessing of tokens. 4\. Index the documents that each term occurs in. In this chapter we first briefly mention how the basic unit of a document can be defined and how the character sequence that it comprises is determined (Section 2.1). We then examine in detail some of the substantive linguistic issues of tokenization and linguistic preprocessing, which determine the vocabulary of terms which a system uses (Section 2.2). Tokenization is the process of chopping character streams into tokens, while linguistic preprocessing then deals with building equivalence classes of tokens which are the set of terms that are indexed. Indexing itself is covered in Chapters 1and 4. Then we return to the implementation of postings lists. In Section 2.3, we examine an extended postings list data structure that supports faster querying, while Section 2.4 covers building postings data structures suitable for handling phrase and proximity queries, of the sort that commonly appear in both extended Boolean models and on the web.  ",2.1
2387,iir,iir-2387,2.1 Document delineation and character sequence decoding," 2.1 Document delineation and character sequence decoding 2.1.1 Obtaining the character sequence in a document Digital documents that are the input to an indexing process are typically bytes in a file or on a web server. The first step of processing is to convert this byte sequence into a linear sequence of characters. For the case of plain English text in ASCII encoding, this is trivial. But often things get much more  ",2.1
2388,iir,iir-2388,2.1.1 Obtaining the character sequence in a document," 2.1.1 Obtaining the character sequence in a document Digital documents that are the input to an indexing process are typically bytes in a file or on a web server. The first step of processing is to convert this byte sequence into a linear sequence of characters. For the case of plain English text in ASCII encoding, this is trivial. But often things get much more      20 2 The term vocabulary and postings lists complex. The sequence of characters may be encoded by one of various single byte or multibyte encoding schemes, such as Unicode UTF-8, or various national or vendor-specific standards. We need to determine the correct encoding. This can be regarded as a machine learning classification problem, as discussed in Chapter 13,1but is often handled by heuristic methods, user selection, or by using provided document metadata. Once the encoding is determined, we decode the byte sequence to a character sequence. We might save the choice of encoding because it gives some evidence about what language the document is written in. The characters may have to be decoded out of some binary representation like Microsoft Word DOC files and/or a compressed format such as zip files. Again, we must determine the document format, and then an appropriate decoder has to be used. Even for plain text documents, additional decoding may need to be done. In XML documents (Section 10.1, page 197), character entities, such as &amp;amp;, need to be decoded to give the correct character, namely &amp;for &amp;amp;. Finally, the textual part of the document may need to be extracted out of other material that will not be processed. This might be the desired handling for XML files, if the markup is going to be ignored; we would almost certainly want to do this with postscript or PDF files. We will not deal further with these issues in this book, and will assume henceforth that our documents are a list of characters. Commercial products usually need to support a broad range of document types and encodings, since users want things to just work with their data as is. Often, they just think of documents as text inside applications and are not even aware of how it is encoded on disk. This problem is usually solved by licensing a software library that handles decoding document formats and character encodings. The idea that text is a linear sequence of characters is also called into question by some writing systems, such as Arabic, where text takes on some two dimensional and mixed order characteristics, as shown in Figures 2.1 and 2.2. But, despite some complicated writing system conventions, there is an underlying sequence of sounds being represented and hence an essentially linear structure remains, and this is what is represented in the digital representation of Arabic, as shown in Figure 2.1.  ",2.1
2389,iir,iir-2389,2.1.2 Choosing a document unit,"    2.1 Document delineation and character sequence decoding 21 ٌبَِآ ⇐ ٌ ب ا ت ِ ك un b ā t i k /kitābun/ ‘a book’ ◮Figure 2.1 An example of a vocalized Modern Standard Arabic word. The writing is from right to left and letters undergo complex mutations as they are combined. The representation of short vowels (here, /i/ and /u/) and the final /n/ (nunation) departs from strict linearity by being represented as diacritics above and below letters. Nevertheless, the represented text is still clearly a linear ordering of characters representing sounds. Full vocalization, as here, normally appears only in the Koran and children’s books. Day-to-day text is unvocalized (short vowels are not represented but the letter for ¯ a would still appear) or partially vocalized, with short vowels inserted in places where the writer perceives ambiguities. These choices add further complexities to indexing.   اا ا1962  132ا لا   . ← → ← → ← START ‘Algeria achieved its independence in 1962 after 132 years of French occupation.’ ◮Figure 2.2 The conceptual linear order of characters is not necessarily the order that you see on the page. In languages that are written right-to-left, such as Hebrew and Arabic, it is quite common to also have left-to-right text interspersed, such as numbers and dollar amounts. With modern Unicode representation concepts, the order of characters in files matches the conceptual order, and the reversal of displayed characters is handled by the rendering system, but this may not be true for documents in older encodings. are many cases in which you might want to do something different. A traditional Unix (mbox-format) email file stores a sequence of email messages (an email folder) in one file, but you might wish to regard each email message as a separate document. Many email messages now contain attached documents, and you might then want to regard the email message and each contained attachment as separate documents. If an email message has an attached zip file, you might want to decode the zip file and regard each file it contains as a separate document. Going in the opposite direction, various pieces of web software (such as latex2html) take things that you might regard as a single document (e.g., a Powerpoint file or a L A T EX document) and split them into separate HTML pages for each slide or subsection, stored as separate files. In these cases, you might want to combine multiple files into a single document. More generally, for very long documents, the issue of indexing granularity arises. For a collection of books, it would usually be a bad idea to index an      22 2 The term vocabulary and postings lists entire book as a document. A search for Chinese toys might bring up a book that mentions China in the first chapter and toys in the last chapter, but this does not make it relevant to the query. Instead, we may well wish to index each chapter or paragraph as a mini-document. Matches are then more likely to be relevant, and since the documents are smaller it will be much easier for the user to find the relevant passages in the document. But why stop there? We could treat individual sentences as mini-documents. It becomes clear that there is a precision/recall tradeoff here. If the units get too small, we are likely to miss important passages because terms were distributed over several mini-documents, while if units are too large we tend to get spurious matches and the relevant information is hard for the user to find. The problems with large document units can be alleviated by use of explicit or implicit proximity search (Sections 2.4.2 and 7.2.2), and the tradeoffs in resulting system performance that we are hinting at are discussed in Chapter 8. The issue of index granularity, and in particular a need to simultaneously index documents at multiple levels of granularity, appears prominently in XML retrieval, and is taken up again in Chapter 10. An IR system should be designed to offer choices of granularity. For this choice to be made well, the person who is deploying the system must have a good understanding of the document collection, the users, and their likely information needs and usage patterns. For now, we will henceforth assume that a suitable size document unit has been chosen, together with an appropriate way of dividing or aggregating files, if needed.  ",2.1
2390,iir,iir-2390,2.2 Determining the vocabulary of terms," 2.2 Determining the vocabulary of terms 2.2.1 Tokenization Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation. Here is an example of tokenization: Input: Friends, Romans, Countrymen, lend me your ears; Output: Friends Romans Countrymen lend me your ears These tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction. A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence. A term is a (perhaps nor-TERM malized) type that is included in the IR system’s dictionary. The set of index terms could be entirely distinct from the tokens, for instance, they could be  ",2.2
2391,iir,iir-2391,2.2.1 Tokenization," 2.2.1 Tokenization Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation. Here is an example of tokenization: Input: Friends, Romans, Countrymen, lend me your ears; Output: Friends Romans Countrymen lend me your ears These tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction. A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence. A term is a (perhaps nor-TERM malized) type that is included in the IR system’s dictionary. The set of index terms could be entirely distinct from the tokens, for instance, they could be      2.2 Determining the vocabulary of terms 23 semantic identifiers in a taxonomy, but in practice in modern IR systems they are strongly related to the tokens in the document. However, rather than being exactly the tokens that appear in the document, they are usually derived from them by various normalization processes which are discussed in Section 2.2.3.2For example, if the document to be indexed is to sleep perchance to dream, then there are 5 tokens, but only 4 types (since there are 2 instances of to). However, if to is omitted from the index (as a stop word, see Section 2.2.2 (page 27)), then there will be only 3 terms: sleep,perchance, and dream. The major question of the tokenization phase is what are the correct tokens to use? In this example, it looks fairly trivial: you chop on whitespace and throw away punctuation characters. This is a starting point, but even for English there are a number of tricky cases. For example, what do you do about the various uses of the apostrophe for possession and contractions? Mr. O’Neill thinks that the boys’ stories about Chile’s capital aren’t amusing. For O’Neill, which of the following is the desired tokenization? neill oneill o’neill o’ neill o neill ? And for aren’t, is it: aren’t arent are n’t aren t ? A simple strategy is to just split on all non-alphanumeric characters, but while o neill looks okay, aren t looks intuitively bad. For all of them, the choices determine which Boolean queries will match. A query of neill AND capital will match in three cases but not the other two. In how many cases would a query of o’neill AND capital match? If no preprocessing of a query is done, then it would match in only one of the five cases. For either 2. That is, as defined here, tokens that are not indexed (stop words) are not terms, and if multiple tokens are collapsed together via normalization, they are indexed as one term, under the normalized form. However, we later relax this definition when discussing classification and clustering in Chapters 13–18, where there is no index. In these chapters, we drop the requirement of inclusion in the dictionary. A term means a normalized word.      24 2 The term vocabulary and postings lists Boolean or free text queries, you always want to do the exact same tokenization of document and query words, generally by processing queries with the same tokenizer. This guarantees that a sequence of characters in a text will always match the same sequence typed in a query.3 These issues of tokenization are language-specific. It thus requires the language of the document to be known. Language identification based on classifiers that use short character subsequences as features is highly effective; most languages have distinctive signature patterns (see page 46 for references). For most languages and particular domains within them there are unusual specific tokens that we wish to recognize as terms, such as the programming languages C++ and C#, aircraft names like B-52, or a T.V. show name such as M*A*S*H – which is sufficiently integrated into popular culture that you find usages such as M*A*S*H-style hospitals. Computer technology has introduced new types of character sequences that a tokenizer should probably tokenize as a single token, including email addresses (jblack@mail.yahoo.com), web URLs (http://stuff.big.com/new/specials.html),numeric IP addresses (142.32.48.231), package tracking numbers (1Z9999W99845399981), and more. One possible solution is to omit from indexing tokens such as monetary amounts, numbers, and URLs, since their presence greatly expands the size of the vocabulary. However, this comes at a large cost in restricting what people can search for. For instance, people might want to search in a bug database for the line number where an error occurs. Items such as the date of an email, which have a clear semantic type, are often indexed separately as document metadata (see Section 6.1, page 110). In English, hyphenation is used for various purposes ranging from split- HYPHENS ting up vowels in words (co-education) to joining nouns as names (HewlettPackard) to a copyediting device to show word grouping (the hold-him-backand-drag-him-away maneuver). It is easy to feel that the first example should be regarded as one token (and is indeed more commonly written as just coeducation), the last should be separated into words, and that the middle case is unclear. Handling hyphens automatically can thus be complex: it can either be done as a classification problem, or more commonly by some heuristic rules, such as allowing short hyphenated prefixes on words, but not longer hyphenated forms. Conceptually, splitting on white space can also split what should be regarded as a single token. This occurs most commonly with names (San Francisco, Los Angeles) but also with borrowed foreign phrases (au fait) and com3. For the free text case, this is straightforward. The Boolean case is more complex: this tokenization may produce multiple terms from one query word. This can be handled by combining the terms with an AND or as a phrase query (see Section 2.4, page 39). It is harder for a system to handle the opposite case where the user entered as two terms something that was tokenized together in the document processing.     2.2 Determining the vocabulary of terms 25 pounds that are sometimes written as a single word and sometimes space separated (such as white space vs. whitespace). Other cases with internal spaces that we might wish to regard as a single token include phone numbers ((800) 2342333) and dates (Mar 11, 1983). Splitting tokens on spaces can cause bad retrieval results, for example, if a search for York University mainly returns documents containing New York University. The problems of hyphens and non-separating whitespace can even interact. Advertisements for air fares frequently contain items like San Francisco-Los Angeles, where simply doing whitespace splitting would give unfortunate results. In such cases, issues of tokenization interact with handling phrase queries (which we discuss in Section 2.4 (page 39)), particularly if we would like queries for all of lowercase, lower-case and lower case to return the same results. The last two can be handled by splitting on hyphens and using a phrase index. Getting the first case right would depend on knowing that it is sometimes written as two words and also indexing it in this way. One effective strategy in practice, which is used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis (Example 1.1), is to encourage users to enter hyphens wherever they may be possible, and whenever there is a hyphenated form, the system will generalize the query to cover all three of the one word, hyphenated, and two word forms, so that a query for over-eager will search for over-eager OR “over eager” OR overeager. However, this strategy depends on user training, since if you query using either of the other two forms, you get no generalization. Each new language presents some new issues. For instance, French has a variant use of the apostrophe for a reduced definite article ‘the’ before a word beginning with a vowel (e.g., l’ensemble) and has some uses of the hyphen with postposed clitic pronouns in imperatives and questions (e.g., donnemoi ‘give me’). Getting the first case correct will affect the correct indexing of a fair percentage of nouns and adjectives: you would want documents mentioning both l’ensemble and un ensemble to be indexed under ensemble. Other languages make the problem harder in new ways. German writes compound nouns without spaces (e.g., Computerlinguistik ‘computational lin- COMPOUNDS guistics’; Lebensversicherungsgesellschaftsangestellter ‘life insurance company employee’). Retrieval systems for German greatly benefit from the use of a compound-splitter module, which is usually implemented by seeing if a word can be subdivided into multiple words that appear in a vocabulary. This phenomenon reaches its limit case with major East Asian Languages (e.g., Chinese, Japanese, Korean, and Thai), where text is written without any spaces between words. An example is shown in Figure 2.3. One approach here is to perform word segmentation as prior linguistic processing. Methods of word segmentation vary from having a large vocabulary and taking the longest vocabulary match with some heuristics for unknown words to the use of machine learning sequence models, such as hidden Markov models or conditional random fields, trained over hand-segmented words (see the references      26 2 The term vocabulary and postings lists ◮Figure 2.3 The standard unsegmented form of Chinese text using the simplified characters of mainland China. There is no whitespace between words, not even between sentences – the apparent space after the Chinese period (◦) is just a typographical illusion caused by placing the character on the left side of its square box. The first sentence is just words in Chinese characters with no spaces between them. The second and third sentences include Arabic numerals and punctuation breaking up the Chinese characters. ◮Figure 2.4 Ambiguities in Chinese word segmentation. The two characters can be treated as one word meaning ‘monk’ or as a sequence of two words meaning ‘and’ and ‘still’. a an and are as at be by for from has he in is it its of on that the to was were will with ◮Figure 2.5 A stop list of 25 semantically non-selective words which are common in Reuters-RCV1. in Section 2.5). Since there are multiple possible segmentations of character sequences (see Figure 2.4), all such methods make mistakes sometimes, and so you are never guaranteed a consistent unique tokenization. The other approach is to abandon word-based indexing and to do all indexing via just short subsequences of characters (character k-grams), regardless of whether particular sequences cross word boundaries or not. Three reasons why this approach is appealing are that an individual Chinese character is more like a syllable than a letter and usually has some semantic content, that most words are short (the commonest length is 2 characters), and that, given the lack of standardization of word breaking in the writing system, it is not always clear where word boundaries should be placed anyway. Even in English, some cases of where to put word boundaries are just orthographic conventions – think of notwithstanding vs. not to mention or into vs. on to – but people are educated to write the words with consistent use of spaces.  ",2.2
2392,iir,iir-2392,2.2.2 Dropping common terms: stop words," 2.2.2 Dropping common terms: stop words Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words. The general strategy for determining a stop list is to sort the terms by collection frequency  (the total number of times each term appears in the document collection), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents being indexed, as astop list, the members of which are then discarded during indexing. An example of a stop list is shown in Figure 2.5. Using a stop list significantly reduces the number of postings that a system has to store; we will present some statistics on this in Chapter 5(see Table 5.1, page 87). And a lot of the time not indexing stop words does little harm: keyword searches with terms like the and by don’t seem very useful. However, this is not true for phrase searches. The phrase query “President of the United States”, which contains two stop words, is more precise than President AND “United States”. The meaning of flights to London is likely to be lost if the word to is stopped out. A search for Vannevar Bush’s article As we may think will be difficult if the first three words are stopped out, and the system searches simply for documents containing the word think. Some special query types are disproportionately affected. Some song titles and well known pieces of verse consist entirely of words that are commonly on stop lists (To be or not to be,Let It Be,I don’t want to be, ... ). The general trend in IR systems over time has been from standard use of quite large stop lists (200–300 terms) to very small stop lists (7–12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists. Some of the design of modern IR systems has focused precisely on how we can exploit the statistics of language so as to be able to cope with common words in better ways. We will show in Section 5.3 (page 95) how good compression techniques greatly reduce the cost of storing the postings for common words. Section 6.2.1 (page 117) then discusses how standard term weighting leads to very common words having little impact on document rankings. Finally, Section 7.1.5 (page 140) shows how an IR system with impact-sorted indexes can terminate scanning a postings list early when weights get small, and hence common words do not cause a large additional processing cost for the average query, even though postings lists for stop words are very long. So for most modern IR systems, the additional cost of including stop words is not that big – neither in terms of index size nor in terms of query processing time.  ",2.2
2393,iir,iir-2393,2.2.3 Normalization (equivalence classing of terms)," 2.2.3 Normalization (equivalence classing of terms) Having broken up our documents (and also our query) into tokens, the easy case is if tokens in the query just match tokens in the token list of the document. However, there are many cases when two character sequences are not quite the same but you would like a match to occur. For instance, if you search for USA, you might hope to also match documents containing U.S.A. Token normalization is the process of canonicalizing tokens so that matches occur despite superficial differences in the character sequences of the tokens.4The most standard way to normalize is to implicitly create equivalence CLASSES classes, which are normally named after one member of the set. For instance, if the tokens anti-discriminatory and antidiscriminatory are both mapped onto the term antidiscriminatory, in both the document text and queries, then searches for one term will retrieve documents that contain either. The advantage of just using mapping rules that remove characters like hyphens is that the equivalence classing to be done is implicit, rather than being fully calculated in advance: the terms that happen to become identical as the result of these rules are the equivalence classes. It is only easy to write rules of this sort that remove characters. Since the equivalence classes are implicit, it is not obvious when you might want to add characters. For instance, it would be hard to know to turn antidiscriminatory into anti-discriminatory. An alternative to creating equivalence classes is to maintain relations between unnormalized tokens. This method can be extended to hand-constructed lists of synonyms such as car and automobile, a topic we discuss further in Chapter 9. These term relationships can be achieved in two ways. The usual way is to index unnormalized tokens and to maintain a query expansion list of multiple vocabulary entries to consider for a certain query term. A query term is then effectively a disjunction of several postings lists. The alternative is to perform the expansion during index construction. When the document contains automobile, we index it under car as well (and, usually, also vice-versa). Use of either of these methods is considerably less efficient than equivalence classing, as there are more postings to store and merge. The first 4. It is also often referred to as term normalization, but we prefer to reserve the name term for the output of the normalization process.      2.2 Determining the vocabulary of terms 29 method adds a query expansion dictionary and requires more processing at query time, while the second method requires more space for storing postings. Traditionally, expanding the space required for the postings lists was seen as more disadvantageous, but with modern storage costs, the increased flexibility that comes from distinct postings lists is appealing. These approaches are more flexible than equivalence classes because the expansion lists can overlap while not being identical. This means there can be an asymmetry in expansion. An example of how such an asymmetry can be exploited is shown in Figure 2.6: if the user enters windows, we wish to allow matches with the capitalized Windows operating system, but this is not plausible if the user enters window, even though it is plausible for this query to also match lowercase windows. The best amount of equivalence classing or query expansion to do is a fairly open question. Doing some definitely seems a good idea. But doing a lot can easily have unexpected consequences of broadening queries in unintended ways. For instance, equivalence-classing U.S.A. and USA to the latter by deleting periods from tokens might at first seem very reasonable, given the prevalent pattern of optional use of periods in acronyms. However, if I put in as my query term C.A.T., I might be rather upset if it matches every appearance of the word cat in documents.5 Below we present some of the forms of normalization that are commonly employed and how they are implemented. In many cases they seem helpful, but they can also do harm. In fact, you can worry about many details of equivalence classing, but it often turns out that providing processing is done consistently to the query and to documents, the fine details may not have much aggregate effect on performance. Accents and diacritics. Diacritics on characters in English have a fairly marginal status, and we might well want cliché and cliche to match, or naive and naïve. This can be done by normalizing tokens to remove diacritics. In many other languages, diacritics are a regular part of the writing system and distinguish different sounds. Occasionally words are distinguished only by their accents. For instance, in Spanish, peña is ‘a cliff’, while pena is ‘sorrow’. Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems. In these cases, it might be best to equate all words to a form without diacritics. 5. At the time we wrote this chapter (Aug. 2005), this was actually the case on Google: the top result for the query C.A.T. was a site about cats, the Cat Fanciers Web Site http://www.fanciers.com/.     30 2 The term vocabulary and postings lists Capitalization/case-folding. A common strategy is to do case-folding by re- CASE-FOLDING ducing all letters to lower case. Often this is a good idea: it will allow instances of Automobile at the beginning of a sentence to match with a query of automobile. It will also help on a web search engine when most of your users type in ferrari when they are interested in a Ferrari car. On the other hand, such case folding can equate words that might better be kept apart. Many proper nouns are derived from common nouns and so are distinguished only by case, including companies (General Motors,The Associated Press), government organizations (the Fed vs. fed) and person names (Bush,Black). We already mentioned an example of unintended query expansion with acronyms, which involved not only acronym normalization (C.A.T. →CAT) but also case-folding (CAT →cat). For English, an alternative to making every token lowercase is to just make some tokens lowercase. The simplest heuristic is to convert to lowercase words at the beginning of a sentence and all words occurring in a title that is all uppercase or in which most or all words are capitalized. These words are usually ordinary words that have been capitalized. Mid-sentence capitalized words are left as capitalized (which is usually correct). This will mostly avoid case-folding in cases where distinctions should be kept apart. The same task can be done more accurately by a machine learning sequence model which uses more features to make the decision of when to case-fold. This is known as truecasing. However, trying to get capitalization right in this way probably doesn’t help if your users usually use lowercase regardless of the correct case of words. Thus, lowercasing everything often remains the most practical solution. Other issues in English. Other possible normalizations are quite idiosyncratic and particular to English. For instance, you might wish to equate ne’er and never or the British spelling colour and the American spelling color. Dates, times and similar items come in multiple formats, presenting additional challenges. You might wish to collapse together 3/12/91 and Mar. 12, 1991. However, correct processing here is complicated by the fact that in the U.S., 3/12/91 is Mar. 12, 1991, whereas in Europe it is 3 Dec 1991. Other languages. English has maintained a dominant position on the WWW; approximately 60% of web pages are in English (Gerrand 2007). But that still leaves 40% of the web, and the non-English portion might be expected to grow over time, since less than one third of Internet users and less than 10% of the world’s population primarily speak English. And there are signs of change: Sifry (2007) reports that only about one third of blog posts are in English. Other languages again present distinctive issues in equivalence classing.      2.2 Determining the vocabulary of terms 31 ◮Figure 2.7 Japanese makes use of multiple intermingled writing systems and, like Chinese, does not segment words. The text is mainly Chinese characters with the hiragana syllabary for inflectional endings and function words. The part in latin letters is actually a Japanese expression, but has been taken up as the name of an environmental campaign by 2004 Nobel Peace Prize winner Wangari Maathai. His name is written using the katakana syllabary in the middle of the first line. The first four characters of the final line express a monetary amount that we would want to match with ¥500,000 (500,000 Japanese yen). The French word for the has distinctive forms based not only on the gender (masculine or feminine) and number of the following noun, but also depending on whether the following word begins with a vowel: le,la,l’,les. We may well wish to equivalence class these various forms of the. German has a convention whereby vowels with an umlaut can be rendered instead as a two vowel digraph. We would want to treat Schütze and Schuetze as equivalent. Japanese is a well-known difficult writing system, as illustrated in Figure 2.7. Modern Japanese is standardly an intermingling of multiple alphabets, principally Chinese characters, two syllabaries (hiragana and katakana) and western characters (Latin letters, Arabic numerals, and various symbols). While there are strong conventions and standardization through the education system over the choice of writing system, in many cases the same word can be written with multiple writing systems. For example, a word may be written in katakana for emphasis (somewhat like italics). Or a word may sometimes be written in hiragana and sometimes in Chinese characters. Successful retrieval thus requires complex equivalence classing across the writing systems. In particular, an end user might commonly present a query entirely in hiragana, because it is easier to type, just as Western end users commonly use all lowercase. Document collections being indexed can include documents from many different languages. Or a single document can easily contain text from multiple languages. For instance, a French email might quote clauses from a contract document written in English. Most commonly, the language is detected and language-particular tokenization and normalization rules are applied at a predetermined granularity, such as whole documents or individual paragraphs, but this still will not correctly deal with cases where language changes occur for brief quotations. When document collections contain mul     32 2 The term vocabulary and postings lists tiple languages, a single index may have to contain terms of several languages. One option is to run a language identification classifier on documents and then to tag terms in the vocabulary for their language. Or this tagging can simply be omitted, since it is relatively rare for the exact same character sequence to be a word in different languages. When dealing with foreign or complex words, particularly foreign names, the spelling may be unclear or there may be variant transliteration standards giving different spellings (for example, Chebyshev and Tchebycheff or Beijing and Peking). One way of dealing with this is to use heuristics to equivalence class or expand terms with phonetic equivalents. The traditional and best known such algorithm is the Soundex algorithm, which we cover in Section 3.4 (page 63).  ",2.2
2394,iir,iir-2394,2.2.4 Stemming and lemmatization," 2.2.4 Stemming and lemmatization For grammatical reasons, documents are going to use different forms of a word, such as organize,organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance: am, are, is ⇒be car, cars, car’s, cars’ ⇒car The result of this mapping of text will be something like: the boy’s cars are different colors ⇒ the boy car be differ color However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.     2.2 Determining the vocabulary of terms 33 Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is Porter’s algorithm (Porter 1980). The entire algorithm is too long and intricate to present here, but we will indicate its general nature. Porter’s algorithm consists of 5 phases of word reductions, applied sequentially. Within each phase there are various conventions to select rules, such as selecting the rule from each rule group that applies to the longest suffix. In the first phase, this convention is used with the following rule group: (2.1) Rule Example SSES →SS caresses →caress IES →I ponies →poni SS →SS caress →caress S→cats →cat Many of the later rules use a concept of the measure of a word, which loosely checks the number of syllables to see whether a word is long enough that it is reasonable to regard the matching portion of a rule as a suffix rather than as part of the stem of a word. For example, the rule: (m&gt;1) EMENT → would map replacement to replac, but not cement to c. The official site for the Porter Stemmer is: http://www.tartarus.org/˜martin/PorterStemmer/ Other stemmers exist, including the older, one-pass Lovins stemmer (Lovins 1968), and newer entrants like the Paice/Husk stemmer (Paice 1990); see: http://www.cs.waikato.ac.nz/˜eibe/stemmers/ http://www.comp.lancs.ac.uk/computing/research/stemming/ Figure 2.8 presents an informal comparison of the different behaviors of these stemmers. Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words. Particular domains may also require special stemming rules. However, the exact stemmed form does not matter, only the equivalence classes it forms. Rather than using a stemmer, you can use a lemmatizer, a tool from Nat- LEMMATIZER ural Language Processing which does full morphological analysis to accurately identify the lemma for each word. Doing full morphological analysis produces at most very modest benefits for retrieval. It is hard to say more,      34 2 The term vocabulary and postings lists Sample text: Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation Lovins stemmer: such an analys can reve featur that ar not eas vis from th vari in th individu gen and can lead to a pictur of expres that is mor biolog transpar and acces to interpres Porter stemmer: such an analysi can reveal featur that ar not easili visibl from the variat in the individu gene and can lead to a pictur of express that is more biolog transpar and access to interpret Paice stemmer: such an analys can rev feat that are not easy vis from the vary in the individ gen and can lead to a pict of express that is mor biolog transp and access to interpret ◮Figure 2.8 A comparison of three stemming algorithms on a sample text. because either form of normalization tends not to improve English information retrieval performance in aggregate – at least not by very much. While it helps a lot for some queries, it equally hurts performance a lot for others. Stemming increases recall while harming precision. As an example of what can go wrong, note that the Porter stemmer stems all of the following words: operate operating operates operation operative operatives operational to oper. However, since operate in its various forms is a common verb, we would expect to lose considerable precision on queries such as the following with Porter stemming: operational AND research operating AND system operative AND dentistry For a case like this, moving to using a lemmatizer would not completely fix the problem because particular inflectional forms are used in particular collocations: a sentence with the words operate and system is not a good match for the query operating AND system. Getting better value from term normalization depends more on pragmatic issues of word use than on formal issues of linguistic morphology. The situation is different for languages with much more morphology (such as Spanish, German, and Finnish). Results in the European CLEF evaluations have repeatedly shown quite large gains from the use of stemmers (and compound splitting for languages like German); see the references in Section 2.5.  ",2.2
2395,iir,iir-2395,2.3 Faster postings list intersection via skip pointers," 2.3 Faster postings list intersection via skip pointers In the remainder of this chapter, we will discuss extensions to postings list data structures and ways to increase the efficiency of using postings lists. Recall the basic postings list intersection operation from Section 1.3 (page 10): we walk through the two postings lists simultaneously, in time linear in the total number of postings entries. If the list lengths are mand n, the intersection takes O(m+n)operations. Can we do better than this? That is, empirically, can we usually process postings list intersection in sublinear time? We can, if the index isn’t changing too fast. One way to do this is to use a skip list by augmenting postings lists with LIST skip pointers (at indexing time), as shown in Figure 2.9. Skip pointers are effectively shortcuts that allow us to avoid processing parts of the postings list that will not figure in the search results. The two questions are then where to place skip pointers and how to do efficient merging using skip pointers. Consider first efficient merging, with Figure 2.9 as an example. Suppose we’ve stepped through the lists in the figure until we have matched 8 on each list and moved it to the results list. We advance both pointers, giving us 16 on the upper list and 41 on the lower list. The smallest item is then the element 16 on the top list. Rather than simply advancing the upper pointer, we first check the skip list pointer and note that 28 is also less than 41. Hence we can follow the skip list pointer, and then we advance the upper pointer to 28 . We thus avoid stepping to 19 and 23 on the upper list. A number of variant versions of postings list intersection with skip pointers is possible depending on when exactly you check the skip pointer. One version is shown   2.3 Faster postings list intersection via skip pointers 37 INTERSECTWITHSKIPS(p1,p2) 1answer ← h i 2while p16=NIL and p26=NIL 3do if docID(p1) = docID(p2) 4then ADD(answer,docID(p1)) 5p1←next(p1) 6p2←next(p2) 7else if docID(p1)&lt;docID(p2) 8then if hasSkip(p1)and (docID(skip(p1)) ≤docID(p2)) 9then while hasSkip(p1)and (docID(skip(p1)) ≤docID(p2)) 10 do p1←skip(p1) 11 else p1←next(p1) 12 else if hasSkip(p2)and (docID(skip(p2)) ≤docID(p1)) 13 then while hasSkip(p2)and (docID(skip(p2)) ≤docID(p1)) 14 do p2←skip(p2) 15 else p2←next(p2) 16 return answer ◮Figure 2.10 Postings lists intersection with skip pointers. in Figure 2.10. Skip pointers will only be available for the original postings lists. For an intermediate result in a complex query, the call hasSkip(p)will always return false. Finally, note that the presence of skip pointers only helps for AND queries, not for OR queries. Where do we place skips? There is a tradeoff. More skips means shorter skip spans, and that we are more likely to skip. But it also means lots of comparisons to skip pointers, and lots of space storing skip pointers. Fewer skips means few pointer comparisons, but then long skip spans which means that there will be fewer opportunities to skip. A simple heuristic for placing skips, which has been found to work well in practice, is that for a postings list of length P, use √Pevenly-spaced skip pointers. This heuristic can be improved upon; it ignores any details of the distribution of query terms. Building effective skip pointers is easy if an index is relatively static; it is harder if a postings list keeps changing because of updates. A malicious deletion strategy can render skip lists ineffective. Choosing the optimal encoding for an inverted index is an ever-changing game for the system builder, because it is strongly dependent on underlying computer technologies and their relative speeds and sizes. Traditionally, CPUs were slow, and so highly compressed techniques were not optimal. Now CPUs are fast and disk is slow, so reducing disk postings list size dominates. However, if you’re running a search engine with everything in mem      38 2 The term vocabulary and postings lists ory then the equation changes again. We discuss the impact of hardware parameters on index construction time in Section 4.1 (page 68) and the impact of index size on system speed in Chapter 5. ?Exercise 2.5 [⋆] Why are skip pointers not useful for queries of the form xOR y? Exercise 2.6 [⋆] We have a two-word query. For one term the postings list consists of the following 16 entries: [4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180] and for the other it is the one entry postings list: [47]. Work out how many comparisons would be done to intersect the two postings lists with the following two strategies. Briefly justify your answers: a. Using standard postings lists b. Using postings lists stored with skip pointers, with a skip length of √P, as suggested in Section 2.3. Exercise 2.7 [⋆] Consider a postings intersection between this postings list, with skip pointers: 3 5 9 15 24 39 60 68 75 81 84 89 92 96 97 100 115 and the following intermediate result postings list (which hence has no skip pointers): 3 5 89 95 97 99 100 101 Trace through the postings intersection algorithm in Figure 2.10 (page 37). a. How often is a skip pointer followed (i.e., p1is advanced to skip(p1))? b. How many postings comparisons will be made by this algorithm while intersecting the two lists? c. How many postings comparisons would be made if the postings lists are intersected without the use of skip pointers?  ",2.3
2396,iir,iir-2396,2.4 Positional postings and phrase queries," 2.4 Positional postings and phrase queries 39 2.4 Positional postings and phrase queries Many complex or technical concepts and many organization and product names are multiword compounds or phrases. We would like to be able to pose a query such as Stanford University by treating it as a phrase so that a sentence in a document like The inventor Stanford Ovshinsky never went to university. is not a match. Most recent search engines support a double quotes syntax (“stanford university”) for phrase queries, which has proven to be very easily understood and successfully used by users. As many as 10% of web queries are phrase queries, and many more are implicit phrase queries (such as person names), entered without use of double quotes. To be able to support such queries, it is no longer sufficient for postings lists to be simply lists of documents that contain individual terms. In this section we consider two approaches to supporting phrase queries and their combination. A search engine should not only support phrase queries, but implement them efficiently. A related but distinct concept is term proximity weighting, where a document is preferred to the extent that the query terms appear close to each other in the text. This technique is covered in Section 7.2.2 (page 144) in the context of ranked retrieval. 2.4.1 Biword indexes One approach to  ",2.4
2397,iir,iir-2397,2.4.1 Biword indexes," 2.4.1 Biword indexes One approach to handling phrases is to consider every pair of consecutive terms in a document as a phrase. For example, the text Friends, Romans, Countrymen would generate the biwords: friends romans romans countrymen In this model, we treat each of these biwords as a vocabulary term. Being able to process two-word phrase queries is immediate. Longer phrases can be processed by breaking them down. The query stanford university palo alto can be broken into the Boolean query on biwords: “stanford university” AND “university palo” AND “palo alto” This query could be expected to work fairly well in practice, but there can and will be occasional false positives. Without examining the documents, we cannot verify that the documents matching the above Boolean query do actually contain the original 4 word phrase. Among possible queries, nouns and noun phrases have a special status in describing the concepts people are interested in searching for. But related nouns can often be divided from each other by various function words, in phrases such as the abolition of slavery or renegotiation of the constitution. These needs can be incorporated into the biword indexing model in the following      40 2 The term vocabulary and postings lists way. First, we tokenize the text and perform part-of-speech-tagging.6We can then group terms into nouns, including proper nouns, (N) and function words, including articles and prepositions, (X), among other classes. Now deem any string of terms of the form NX*N to be an extended biword. Each such extended biword is made a term in the vocabulary. For example: renegotiation of the constitution N X X N To process a query using such an extended biword index, we need to also parse it into N’s and X’s, and then segment the query into extended biwords, which can be looked up in the index. This algorithm does not always work in an intuitively optimal manner when parsing longer queries into Boolean queries. Using the above algorithm, the query cost overruns on a power plant is parsed into “cost overruns” AND “overruns power” AND “power plant” whereas it might seem a better query to omit the middle biword. Better results can be obtained by using more precise part-of-speech patterns that define which extended biwords should be indexed. The concept of a biword index can be extended to longer sequences of words, and if the index includes variable length word sequences, it is generally referred to as a phrase index. Indeed, searches for a single term are not naturally handled in a biword index (you would need to scan the dictionary for all biwords containing the term), and so we also need to have an index of single-word terms. While there is always a chance of false positive matches, the chance of a false positive match on indexed phrases of length 3 or more becomes very small indeed. But on the other hand, storing longer phrases has the potential to greatly expand the vocabulary size. Maintaining exhaustive phrase indexes for phrases of length greater than two is a daunting prospect, and even use of an exhaustive biword dictionary greatly expands the size of the vocabulary. However, towards the end of this section we discuss the utility of the strategy of using a partial phrase index in a compound indexing scheme. 6. Part of speech taggers classify words as nouns, verbs, etc. – or, in practice, often as finergrained classes like “plural proper noun”. Many fairly accurate (c. 96% per- tag accuracy) partof-speech taggers now exist, usually trained by machine learning methods on hand-tagged text. See, for instance, Manning and Schütze (1999, ch. 10).      2.4 Positional postings and phrase queries 41 to, 993427: h1, 6: h7, 18, 33, 72, 86, 231i; 2, 5: h1, 17, 74, 222, 255i; 4, 5: h8, 16, 190, 429, 433i; 5, 2: h363, 367i; 7, 3: h13, 23, 191i; . .. i be, 178239: h1, 2: h17, 25i; 4, 5: h17, 191, 291, 430, 434i; 5, 3: h14, 19, 101i; . .. i ◮Figure 2.11 Positional index example. The word to has a document frequency 993,477, and occurs 6 times in document 1 at positions 7, 18, 33, etc.  ",2.4
2398,iir,iir-2398,2.4.2 Positional indexes," 2.4.2 Positional indexes For the reasons given, a biword index is not the standard solution. Rather, apositional index is most commonly employed. Here, for each term in the vocabulary, we store postings of the form docID: hposition1, position2, ... i, as shown in Figure 2.11, where each position is a token index in the document. Each posting will also usually record the term frequency, for reasons discussed in Chapter 6. To process a phrase query, you still need to access the inverted index entries for each distinct term. As before, you would start with the least frequent term and then work to further restrict the list of possible candidates. In the merge operation, the same general technique is used as before, but rather than simply checking that both terms are in a document, you also need to check that their positions of appearance in the document are compatible with the phrase query being evaluated. This requires working out offsets between the words. ✎Example 2.1: Satisfying phrase queries. Suppose the postings lists for to and be are as in Figure 2.11, and the query is “to be or not to be”. The postings lists to access are: to,be,or,not. We will examine intersecting the postings lists for to and be. We first look for documents that contain both terms. Then, we look for places in the lists where there is an occurrence of be with a token index one higher than a position of to, and then we look for another occurrence of each word with token index 4 higher than the first occurrence. In the above lists, the pattern of occurrences that is a possible match is: to:h. . . ; 4:h. . . ,429,433i; . . . i be:h. . . ; 4:h. . . ,430,434i; . . . i      42 2 The term vocabulary and postings lists POSITIONALINTERSECT(p1,p2,k) 1answer ← h i 2while p16=NIL and p26=NIL 3do if docID(p1) = docID(p2) 4then l← h i 5pp1←positions(p1) 6pp2←positions(p2) 7while pp16=NIL 8do while pp26=NIL 9do if |pos(pp1)−pos(pp2)| ≤ k 10 then ADD(l,pos(pp2)) 11 else if pos(pp2)&gt;pos(pp1) 12 then break 13 pp2←next(pp2) 14 while l6=h i and |l[0]−pos(pp1)|&gt;k 15 do DELETE(l[0]) 16 for each ps ∈l 17 do ADD(answer,hdocID(p1),pos(pp1),psi) 18 pp1←next(pp1) 19 p1←next(p1) 20 p2←next(p2) 21 else if docID(p1)&lt;docID(p2) 22 then p1←next(p1) 23 else p2←next(p2) 24 return answer ◮Figure 2.12 An algorithm for proximity intersection of postings lists p1and p2. The algorithm finds places where the two terms appear within kwords of each other and returns a list of triples giving docID and the term position in p1and p2. The same general method is applied for within kword proximity searches, of the sort we saw in Example 1.1 (page 15): employment /3 place Here, /kmeans “within kwords of (on either side)”. Clearly, positional indexes can be used for such queries; biword indexes cannot. We show in Figure 2.12 an algorithm for satisfying within kword proximity searches; it is further discussed in Exercise 2.12. Positional index size. Adopting a positional index expands required postings storage significantly, even if we compress position values/offsets as we     2.4 Positional postings and phrase queries 43 will discuss in Section 5.3 (page 95). Indeed, moving to a positional index also changes the asymptotic complexity of a postings intersection operation, because the number of items to check is now bounded not by the number of documents but by the total number of tokens in the document collection T. That is, the complexity of a Boolean query is Θ(T)rather than Θ(N). However, most applications have little choice but to accept this, since most users now expect to have the functionality of phrase and proximity searches. Let’s examine the space implications of having a positional index. A posting now needs an entry for each occurrence of a term. The index size thus depends on the average document size. The average web page has less than 1000 terms, but documents like SEC stock filings, books, and even some epic poems easily reach 100,000 terms. Consider a term with frequency 1 in 1000 terms on average. The result is that large documents cause an increase of two orders of magnitude in the space required to store the postings list: Expected Expected entries Document size postings in positional posting 1000 1 1 100,000 1 100 While the exact numbers depend on the type of documents and the language being indexed, some rough rules of thumb are to expect a positional index to be 2 to 4 times as large as a non-positional index, and to expect a compressed positional index to be about one third to one half the size of the raw text (after removal of markup, etc.) of the original uncompressed documents. Specific numbers for an example collection are given in Table 5.1 (page 87) and Table 5.6 (page 103).  ",2.4
2399,iir,iir-2399,2.4.3 Combination schemes," 2.4.3 Combination schemes The strategies of biword indexes and positional indexes can be fruitfully combined. If users commonly query on particular phrases, such as Michael Jackson, it is quite inefficient to keep merging positional postings lists. A combination strategy uses a phrase index, or just a biword index, for certain queries and uses a positional index for other phrase queries. Good queries to include in the phrase index are ones known to be common based on recent querying behavior. But this is not the only criterion: the most expensive phrase queries to evaluate are ones where the individual words are common but the desired phrase is comparatively rare. Adding Britney Spears as a phrase index entry may only give a speedup factor to that query of about 3, since most documents that mention either word are valid results, whereas adding The Who as a phrase index entry may speed up that query by a factor of 1000. Hence, having the latter is more desirable, even if it is a relatively less common query.     44 2 The term vocabulary and postings lists Williams et al. (2004) evaluate an even more sophisticated scheme which employs indexes of both these sorts and additionally a partial next word index as a halfway house between the first two strategies. For each term, a next word index records terms that follow it in a document. They conclude that such a strategy allows a typical mixture of web phrase queries to be completed in one quarter of the time taken by use of a positional index alone, while taking up 26% more space than use of a positional index alone. ?Exercise 2.8 [⋆] Assume a biword index. Give an example of a document which will be returned for a query of New York University but is actually a false positive which should not be returned. Exercise 2.9 [⋆] Shown below is a portion of a positional index in the format: term: doc1: hposition1, position2, . . . i; doc2: hposition1, position2, . . . i; etc. angels: 2: h36,174,252,651i; 4: h12,22,102,432i; 7: h17i; fools: 2: h1,17,74,222i; 4: h8,78,108,458i; 7: h3,13,23,193i; fear: 2: h87,704,722,901i; 4: h13,43,113,433i; 7: h18,328,528i; in: 2: h3,37,76,444,851i; 4: h10,20,110,470,500i; 7: h5,15,25,195i; rush: 2: h2,66,194,321,702i; 4: h9,69,149,429,569i; 7: h4,14,404i; to: 2: h47,86,234,999i; 4: h14,24,774,944i; 7: h199,319,599,709i; tread: 2: h57,94,333i; 4: h15,35,155i; 7: h20,320i; where: 2: h67,124,393,1001i; 4: h11,41,101,421,431i; 7: h16,36,736i; Which document(s) if any match each of the following queries, where each expression within quotes is a phrase query? a. “fools rush in” b. “fools rush in” AND “angels fear to tread” Exercise 2.10 [⋆] Consider the following fragment of a positional index with the format: word: document: hposition, position, . . .i; document: hposition, . . .i ... Gates: 1: h3i; 2: h6i; 3: h2,17i; 4: h1i; IBM: 4: h3i; 7: h14i; Microsoft: 1: h1i; 2: h1,21i; 3: h3i; 5: h16,22,51i; The /koperator, word1 /kword2 finds occurrences of word1 within kwords of word2 (on either side), where kis a positive integer argument. Thus k=1 demands that word1 be adjacent to word2. a. Describe the set of documents that satisfy the query Gates /2 Microsoft. b. Describe each set of values for kfor which the query Gates /kMicrosoft returns a different set of documents as the answer.  ",2.4
2400,iir,iir-2400,2.5 References and further reading," 2.5 References and further reading 45 Exercise 2.11 [⋆⋆] Consider the general procedure for merging two positional postings lists for a given document, to determine the document positions where a document satisfies a /k clause (in general there can be multiple positions at which each term occurs in a single document). We begin with a pointer to the position of occurrence of each term and move each pointer along the list of occurrences in the document, checking as we do so whether we have a hit for /k. Each move of either pointer counts as a step. Let Ldenote the total number of occurrences of the two terms in the document. What is the big-O complexity of the merge procedure, if we wish to have postings including positions in the result? Exercise 2.12 [⋆⋆] Consider the adaptation of the basic algorithm for intersection of two postings lists (Figure 1.6, page 11) to the one in Figure 2.12 (page 42), which handles proximity queries. A naive algorithm for this operation could be O(PLmax2), where Pis the sum of the lengths of the postings lists (i.e., the sum of document frequencies) and Lmax is the maximum length of a document (in tokens). a. Go through this algorithm carefully and explain how it works. b. What is the complexity of this algorithm? Justify your answer carefully. c. For certain queries and data distributions, would another algorithm be more efficient? What complexity does it have? Exercise 2.13 [⋆⋆] Suppose we wish to use a postings intersection procedure to determine simply the list of documents that satisfy a /kclause, rather than returning the list of positions, as in Figure 2.12 (page 42). For simplicity, assume k≥2. Let Ldenote the total number of occurrences of the two terms in the document collection (i.e., the sum of their collection frequencies). Which of the following is true? Justify your answer. a. The merge can be accomplished in a number of steps linear in Land independent of k, and we can ensure that each pointer moves only to the right. b. The merge can be accomplished in a number of steps linear in Land independent of k, but a pointer may be forced to move non-monotonically (i.e., to sometimes back up) c. The merge can require kL steps in some cases. Exercise 2.14 [⋆⋆] How could an IR system combine use of a positional index and use of stop words? What is the potential problem, and how could it be handled? 2.5 References and further reading Exhaustive discussion of the character-level processing of East Asian languages can be found in Lunde (1998). Character bigram indexes are perhaps the most standard approach to indexing Chinese, although some systems use word segmentation. Due to differences in the language and writing system, word segmentation is most usual for Japanese (Luk and Kwok 2002,Kishida     46 2 The term vocabulary and postings lists et al. 2005). The structure of a character k-gram index over unsegmented text differs from that in Section 3.2.2 (page 54): there the k-gram dictionary points to postings lists of entries in the regular dictionary, whereas here it points directly to document postings lists. For further discussion of Chinese word segmentation, see Sproat et al. (1996), Sproat and Emerson (2003), Tseng et al. (2005), and Gao et al. (2005). Lita et al. (2003) present a method for truecasing. Natural language processing work on computational morphology is presented in (Sproat 1992, Beesley and Karttunen 2003). Language identification was perhaps first explored in cryptography; for example, Konheim (1981) presents a character-level k-gram language identification algorithm. While other methods such as looking for particular distinctive function words and letter combinations have been used, with the advent of widespread digital text, many people have explored the character n-gram technique, and found it to be highly successful (Beesley 1998, Dunning 1994,Cavnar and Trenkle 1994). Written language identification is regarded as a fairly easy problem, while spoken language identification remains more difficult; see Hughes et al. (2006) for a recent survey. Experiments on and discussion of the positive and negative impact of stemming in English can be found in the following works: Salton (1989), Harman (1991), Krovetz (1995), Hull (1996). Hollink et al. (2004) provide detailed results for the effectiveness of language-specific methods on 8 European languages. In terms of percent change in mean average precision (see page 159) over a baseline system, diacritic removal gains up to 23% (being especially helpful for Finnish, French, and Swedish). Stemming helped markedly for Finnish (30% improvement) and Spanish (10% improvement), but for most languages, including English, the gain from stemming was in the range 0– 5%, and results from a lemmatizer were poorer still. Compound splitting gained 25% for Swedish and 15% for German, but only 4% for Dutch. Rather than language-particular methods, indexing character k-grams (as we suggested for Chinese) could often give as good or better results: using withinword character 4-grams rather than words gave gains of 37% in Finnish, 27% in Swedish, and 20% in German, while even being slightly positive for other languages, such as Dutch, Spanish, and English. Tomlinson (2003) presents broadly similar results. Bar-Ilan and Gutman (2005) suggest that, at the time of their study (2003), the major commercial web search engines suffered from lacking decent language-particular processing; for example, a query on www.google.fr for l’électricité did not separate off the article l’ but only matched pages with precisely this string of article+noun. The classic presentation of skip pointers for IR can be found in Moffat and Zobel (1996). Extended techniques are discussed in Boldi and Vigna (2005). The main paper in the algorithms literature is Pugh (1990), which uses multilevel skip pointers to give expected O(log P)list access (the same expected   2.5 References and further reading 47 efficiency as using a tree data structure) with less implementational complexity. In practice, the effectiveness of using skip pointers depends on various system parameters. Moffat and Zobel (1996) report conjunctive queries running about five times faster with the use of skip pointers, but Bahle et al. (2002, p. 217) report that, with modern CPUs, using skip lists instead slows down search because it expands the size of the postings list (i.e., disk I/O dominates performance). In contrast, Strohman and Croft (2007) again show good performance gains from skipping, in a system architecture designed to optimize for the large memory spaces and multiple cores of recent CPUs. Johnson et al. (2006) report that 11.7% of all queries in two 2002 web query logs contained phrase queries, though Kammenhuber et al. (2006) report only 3% phrase queries for a different data set. Silverstein et al. (1999) note that many queries without explicit phrase operators are actually implicit phrase searches.  ",2.4
2273,iir,iir-2273,21 Link analysis," 21 Link analysis The analysis of hyperlinks and the graph structure of the Web has been instrumental in the development of web search. In this chapter we focus on the use of hyperlinks for ranking web search results. Such link analysis is one of many factors considered by web search engines in computing a composite score for a web page on any given query. We begin by reviewing some basics of the Web as a graph in Section 21.1, then proceed to the technical development of the elements of link analysis for ranking. Link analysis for web search has intellectual antecedents in the field of citation analysis, aspects of which overlap with an area known as bibliometrics. These disciplines seek to quantify the influence of scholarly articles by analyzing the pattern of citations amongst them. Much as citations represent the conferral of authority from a scholarly article to others, link analysis on the Web treats hyperlinks from a web page to another as a conferral of authority. Clearly, not every citation or hyperlink implies such authority conferral; for this reason, simply measuring the quality of a web page by the number of in-links (citations from other pages) is not robust enough. For instance, one may contrive to set up multiple web pages pointing to a target web page, with the intent of artificially boosting the latter’s tally of in-links. This phenomenon is referred to as link spam. Nevertheless, the phenomenon of citation is prevalent and dependable enough that it is feasible for web search engines to derive useful signals for ranking from more sophisticated link analysis. Link analysis also proves to be a useful indicator of what page(s) to crawl next while crawling the web; this is done by using link analysis to guide the priority assignment in the front queues of Chapter 20. Section 21.1 develops the basic ideas underlying the use of the web graph in link analysis. Sections 21.2 and 21.3 then develop two distinct methods for link analysis, PageRank and HITS.   21 Link analysis 21.1 The Web as a graph Recall the notion ",21.1
2274,iir,iir-2274,21.1 The Web as a graph," 21.1 The Web as a graph Recall the notion of the web graph from Section 19.2.1 and particularly Figure 19.2. Our study of link analysis builds on two intuitions: 1\. The anchor text pointing to page B is a good description of page B. 2\. The hyperlink from A to B represents an endorsement of page B, by the creator of page A. This is not always the case; for instance, many links amongst pages within a single website stem from the user of a common template. For instance, most corporate websites have a pointer from every page to a page containing a copyright notice – this is clearly not an endorsement. Accordingly, implementations of link analysis algorithms will typical discount such “internal” links. 21.1.1 Anchor text and the web graph The following fragmen ",21.1
2275,iir,iir-2275,21.1.1 Anchor text and the web graph," 21.1.1 Anchor text and the web graph The following fragment of HTML code from a web page shows a hyperlink pointing to the home page of the Journal of the ACM: &lt;a href=""http://www.acm.org/jacm/""&gt;Journal of the ACM.&lt;/a&gt; In this case, the link points to the page http://www.acm.org/jacm/and the anchor text is Journal of the ACM. Clearly, in this example the anchor is descriptive of the targetpage. But then the target page (B = http://www.acm.org/jacm/) itself contains the same description as well as considerable additional information on the journal. So what use is the anchor text? The Web is full of instances where the page B does not provide an accurate description of itself. In many cases this is a matter of how the publishers of page B choose to present themselves; this is especially common with corporate web pages, where a web presence is a marketing statement. For example, at the time of the writing of this book the home page of the IBM corporation (http://www.ibm.com) did not contain the term computer anywhere in its HTML code, despite the fact that IBM is widely viewed as the world’s largest computer maker. Similarly, the HTML code for the home page of Yahoo! (http://www.yahoo.com) does not at this time contain the word portal. Thus, there is often a gap between the terms in a web page, and how web users would describe that web page. Consequently, web searchers need not use the terms in a page to query for it. In addition, many web pages are rich in graphics and images, and/or embed their text in these images; in such cases, the HTML parsing performed when crawling will not extract text that is useful for indexing these pages. The “standard IR” approach to this would be to use the methods outlined in Chapter 9and Section 12.4. The insight     21.1 The Web as a graph 463 behind anchor text is that such methods can be supplanted by anchor text, thereby tapping the power of the community of web page authors. The fact that the anchors of many hyperlinks pointing to http://www.ibm.com include the word computer can be exploited by web search engines. For instance, the anchor text terms can be included as terms under which to index the target web page. Thus, the postings for the term computer would include the document http://www.ibm.comand that for the term portal would include the document http://www.yahoo.com, using a special indicator to show that these terms occur as anchor (rather than in-page) text. As with in-page terms, anchor text terms are generally weighted based on frequency, with a penalty for terms that occur very often (the most common terms in anchor text across the Web are Click and here, using methods very similar to idf). The actual weighting of terms is determined by machine-learned scoring, as in Section 15.4.1; current web search engines appear to assign a substantial weighting to anchor text terms. The use of anchor text has some interesting side-effects. Searching for big blue on most web search engines returns the home page of the IBM corporation as the top hit; this is consistent with the popular nickname that many people use to refer to IBM. On the other hand, there have been (and continue to be) many instances where derogatory anchor text such as evil empire leads to somewhat unexpected results on querying for these terms on web search engines. This phenomenon has been exploited in orchestrated campaigns against specific sites. Such orchestrated anchor text may be a form of spamming, since a website can create misleading anchor text pointing to itself, to boost its ranking on selected query terms. Detecting and combating such systematic abuse of anchor text is another form of spam detection that web search engines perform. The window of text surrounding anchor text (sometimes referred to as extended anchor text) is often usable in the same manner as anchor text itself; consider for instance the fragment of web text there is good discussion of vedic scripture &lt;a&gt;here&lt;/a&gt;. This has been considered in a number of settings and the useful width of this window has been studied; see Section 21.4 for references. ?Exercise 21.1 Is it always possible to follow directed edges (hyperlinks) in the web graph from any node (web page) to any other? Why or why not? Exercise 21.2 Find an instance of misleading anchor-text on the Web. Exercise 21.3 Given the collection of anchor-text phrases for a web page x, suggest a heuristic for choosing one term or phrase from this collection that is most descriptive of x.      464 21 Link analysis   A  C   B   D  @@R ◮Figure 21.1 The random surfer at node A proceeds with probability 1/3 to each of B, C and D. Exercise 21.4 Does your heuristic in the previous exercise take into account a single domain D repeating anchor text for xfrom multiple pages in D?  ",21.1
2276,iir,iir-2276,21.2 PageRank," 21.2 PageRank We now focus on scoring and ranking measures derived from the link structure alone. Our first technique for link analysis assigns to every node in the web graph a numerical score between 0 and 1, known as its PageRank.PAGERANK The PageRank of a node will depend on the link structure of the web graph. Given a query, a web search engine computes a composite score for each web page that combines hundreds of features such as cosine similarity (Section 6.3) and term proximity (Section 7.2.2), together with the PageRank score. This composite score, developed using the methods of Section 15.4.1, is used to provide a ranked list of results for the query. Consider a random surfer who begins at a web page (a node of the web graph) and executes a random walk on the Web as follows. At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to. Figure 21.1 shows the surfer at a node A, out of which there are three hyperlinks to nodes B, C and D; the surfer proceeds at the next time step to one of these three nodes, with equal probabilities 1/3. As the surfer proceeds in this random walk from node to node, he visits some nodes more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. The idea behind PageRank is that pages visited more often in this walk are more important. What if the current location of the surfer, the node A, has no out-links? To address this we introduce an additional operation for our random surfer: the teleport operation. In the teleport operation the surfer jumps from a node to any other node in the web graph. This could happen because he types   21.2 PageRank 465 an address into the URL bar of his browser. The destination of a teleport operation is modeled as being chosen uniformly at random from all web pages. In other words, if Nis the total number of nodes in the web graph1, the teleport operation takes the surfer to each node with probability 1/N. The surfer would also teleport to his present position with probability 1/N. In assigning a PageRank score to each node of the web graph, we use the teleport operation in two ways: (1) When at a node with no out-links, the surfer invokes the teleport operation. (2) At any node that has outgoing links, the surfer invokes the teleport operation with probability 0 &lt;α&lt;1 and the standard random walk (follow an out-link chosen uniformly at random as in Figure 21.1) with probability 1 −α, where αis a fixed parameter chosen in advance. Typically, αmight be 0.1. In Section 21.2.1, we will use the theory of Markov chains to argue that when the surfer follows this combined process (random walk plus teleport) he visits each node vof the web graph a fixed fraction of the time π(v)that depends on (1) the structure of the web graph and (2) the value of α. We call this value π(v)the PageRank of vand will show how to compute this value in Section 21.2.2. 21.2.1 Markov chains A Markov chain is ",21.2
2277,iir,iir-2277,21.2.1 Markov chains," 21.2.1 Markov chains A Markov chain is a discrete-time stochastic process: a process that occurs in a series of time-steps in each of which a random choice is made. A Markov chain consists of N states. Each web page will correspond to a state in the Markov chain we will formulate. A Markov chain is characterized by an N×N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of Padd up to 1. The Markov chain can be in one of the Nstates at any given timestep; then, the entry Pij tells us the probability that the state at the next timestep is j, conditioned on the current state being i. Each entry Pij is known as a transition probability and depends only on the current state i; this is known as the Markov property. Thus, by the Markov property, ∀i,j,Pij ∈[0, 1] and ∀i,N ∑ j=1 Pij =1. (21.1) A matrix with non-negative entries that satisfies Equation (21.1) is known as a stochastic matrix. A key property of a stochastic matrix is that it has a principal left eigenvector corresponding to its largest eigenvalue, which is 1.PRINCIPAL LEFT EIGENVECTOR 1. This is consistent with our usage of Nfor the number of documents in the collection.      466 21 Link analysis   A  B   C10.5 0.5 1 ◮Figure 21.2 A simple Markov chain with three states; the numbers on the links indicate the transition probabilities. In a Markov chain, the probability distribution of next states for a Markov chain depends only on the current state, and not on how the Markov chain arrived at the current state. Figure 21.2 shows a simple Markov chain with three states. From the middle state A, we proceed with (equal) probabilities of 0.5 to either B or C. From either B or C, we proceed with probability 1 to A. The transition probability matrix of this Markov chain is then  0 0.5 0.5 1 0 0 1 0 0   A Markov chain’s probability distribution over its states may be viewed as aprobability vector: a vector all of whose entries are in the interval [0, 1], and the entries add up to 1. An N-dimensional probability vector each of whose components corresponds to one of the Nstates of a Markov chain can be viewed as a probability distribution over its states. For our simple Markov chain of Figure 21.2, the probability vector would have 3 components that sum to 1. We can view a random surfer on the web graph as a Markov chain, with one state for each web page, and each transition probability representing the probability of moving from one web page to another. The teleport operation contributes to these transition probabilities. The adjacency matrix Aof the web graph is defined as follows: if there is a hyperlink from page ito page j, then Aij =1, otherwise Aij =0. We can readily derive the transition probability matrix Pfor our Markov chain from the N×Nmatrix A: 1\. If a row of Ahas no 1’s, then replace each element by 1/N. For all other rows proceed as follows. 2\. Divide each 1 in Aby the number of 1’s in its row. Thus, if there is a row with three 1’s, then each of them is replaced by 1/3. 3\. Multiply the resulting matrix by 1 −α.      21.2 PageRank 467 4\. Add α/Nto every entry of the resulting matrix, to obtain P. We can depict the probability distribution of the surfer’s position at any time by a probability vector ~x. At t=0 the surfer may begin at a state whose corresponding entry in ~xis 1 while all others are zero. By definition, the surfer’s distribution at t=1 is given by the probability vector ~xP; at t=2 by (~xP)P=~xP2, and so on. We will detail this process in Section 21.2.2. We can thus compute the surfer’s distribution over the states at any time, given only the initial distribution and the transition probability matrix P. If a Markov chain is allowed to run for many time steps, each state is visited at a (different) frequency that depends on the structure of the Markov chain. In our running analogy, the surfer visits certain web pages (say, popular news home pages) more often than other pages. We now make this intuition precise, establishing conditions under which such the visit frequency converges to fixed, steady-state quantity. Following this, we set the PageRank of each node vto this steady-state visit frequency and show how it can be computed. Definition: A Markov chain is said to be ergodic if there exists a positive integer T0such that for all pairs of states i,jin the Markov chain, if it is started at time 0 in state ithen for all t&gt;T0, the probability of being in state jat time tis greater than 0. For a Markov chain to be ergodic, two technical conditions are required of its states and the non-zero transition probabilities; these conditions are known as irreducibility and aperiodicity. Informally, the first ensures that there is a sequence of transitions of non-zero probability from any state to any other, while the latter ensures that the states are not partitioned into sets such that all state transitions occur cyclically from one set to another. Theorem 21.1. For any ergodic Markov chain, there is a unique steady-state prob-STEADY-STATE ability vector ~ πthat is the principal left eigenvector of P, such that if η(i,t)is the number of visits to state i in t steps, then lim t→∞ η(i,t) t=π(i), where π(i)&gt;0is the steady-state probability for state i. It follows from Theorem 21.1 that the random walk with teleporting results in a unique distribution of steady-state probabilities over the states of the induced Markov chain. This steady-state probability for a state is the PageRank of the corresponding web page.      468 21 Link analysis  ",21.2
2278,iir,iir-2278,21.2.2 The PageRank computation," 21.2.2 The PageRank computation How do we compute PageRank values? Recall the definition of a left eigenvector from Equation 18.2; the left eigenvectors of the transition probability matrix Pare N-vectors ~ πsuch that ~ πP=λ~ π. (21.2) The Nentries in the principal eigenvector ~ πare the steady-state probabilities of the random walk with teleporting, and thus the PageRank values for the corresponding web pages. We may interpret Equation (21.2) as follows: if ~ πis the probability distribution of the surfer across the web pages, he remains in the steady-state distribution ~ π. Given that ~ πis the steady-state distribution, we have that πP=1π, so 1 is an eigenvalue of P. Thus if we were to compute the principal left eigenvector of the matrix P— the one with eigenvalue 1 — we would have computed the PageRank values. There are many algorithms available for computing left eigenvectors; the references at the end of Chapter 18 and the present chapter are a guide to these. We give here a rather elementary method, sometimes known as power iteration. If ~xis the initial distribution over the states, then the distribution at time tis ~xPt. As tgrows large, we would expect that the distribution ~xPt2 is very similar to the distribution ~xPt+1, since for large twe would expect the Markov chain to attain its steady state. By Theorem 21.1 this is independent of the initial distribution ~x. The power iteration method simulates the surfer’s walk: begin at a state and run the walk for a large number of steps t, keeping track of the visit frequencies for each of the states. After a large number of steps t, these frequencies “settle down” so that the variation in the computed frequencies is below some predetermined threshold. We declare these tabulated frequencies to be the PageRank values. We consider the web graph in Exercise 21.6 with α=0.5. The transition probability matrix of the surfer’s walk with teleportation is then P= 1/6 2/3 1/6 5/12 1/6 5/12 1/6 2/3 1/6  . (21.3) Imagine that the surfer starts in state 1, corresponding to the initial probability distribution vector ~x0= (1 0 0). Then, after one step the distribution is ~x0P=1/6 2/3 1/6 =~x1. (21.4) 2. Note that Ptrepresents Praised to the tth power, not the transpose of Pwhich is denoted PT.      21.2 PageRank 469 ~x01 0 0 ~x11/6 2/3 1/6 ~x21/3 1/3 1/3 ~x31/4 1/2 1/4 ~x47/24 5/12 7/24 ... ··· ··· ··· ~x5/18 4/9 5/18 ◮Figure 21.3 The sequence of probability vectors. After two steps it is ~x1P=1/6 2/3 1/6  1/6 2/3 1/6 5/12 1/6 5/12 1/6 2/3 1/6  =1/3 1/3 1/3 =~x2. (21.5) Continuing in this fashion gives a sequence of probability vectors as shown in Figure 21.3. Continuing for several steps, we see that the distribution converges to the steady state of ~x= (5/18 4/9 5/18). In this simple example, we may directly calculate this steady-state probability distribution by observing the symmetry of the Markov chain: states 1 and 3 are symmetric, as evident from the fact that the first and third rows of the transition probability matrix in Equation (21.3) are identical. Postulating, then, that they both have the same steady-state probability and denoting this probability by p, we know that the steady-state distribution is of the form ~ π= (p1−2p p). Now, using the identity ~ π=~ πP, we solve a simple linear equation to obtain p=5/18 and consequently, ~ π= (5/18 4/9 5/18). The PageRank values of pages (and the implicit ordering amongst them) are independent of any query a user might pose; PageRank is thus a queryindependent measure of the static quality of each web page (recall such static quality measures from Section 7.1.4). On the other hand, the relative ordering of pages should, intuitively, depend on the query being served. For this reason, search engines use static quality measures such as PageRank as just one of many factors in scoring a web page on a query. Indeed, the relative contribution of PageRank to the overall score may again be determined by machine-learned scoring as in Section 15.4.1.      470 21 Link analysis d0 d2d1 d5 d3d6 d4 car benz ford gm honda jaguar jag cat leopard tiger jaguar lion cheetah speed ◮Figure 21.4 A small web graph. Arcs are annotated with the word that occurs in the anchor text of the corresponding link. ✎Example 21.1: Consider the graph in Figure 21.4. For a teleportation rate of 0.14 its (stochastic) transition probability matrix is: 0.02 0.02 0.88 0.02 0.02 0.02 0.02 0.02 0.45 0.45 0.02 0.02 0.02 0.02 0.31 0.02 0.31 0.31 0.02 0.02 0.02 0.02 0.02 0.02 0.45 0.45 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.88 0.02 0.02 0.02 0.02 0.02 0.45 0.45 0.02 0.02 0.02 0.31 0.31 0.02 0.31 The PageRank vector of this matrix is: ~x= (0.05 0.04 0.11 0.25 0.21 0.04 0.31) (21.6) Observe that in Figure 21.4,q2,q3,q4and q6are the nodes with at least two in- links. Of these, q2has the lowest PageRank since the random walk tends to drift out of the top part of the graph – the walker can only return there through teleportation.     21.2 PageRank 471 21.2.3 Topic-specific PageRank Thus far we have discussed the PageRank computation with a teleport operation in which the surfer jumps to a random web page chosen uniformly at random. We now consider teleporting to a random web page chosen nonuniformly. In doing so, we are able to derive PageRank values tailored to particular interests. For instance, a sports aficionado might wish that pages on sports be ranked higher than non-sports pages. Suppose that web pages on sports are “near” one another in the web graph. Then, a random surfer who frequently finds himself on random sports pages is likely (in the course of the random walk) to spend most of his time at sports pages, so that the steady-state distribution of sports pages is boosted. Suppose our random surfer, endowed with a teleport operation as before, teleports to a random web page on the topic of sports instead of teleporting to a uniformly chosen random web page. We will not focus on how we collect all web pages on the topic of sports; in fact, we only need a non-zero subset Sof sports-related web pages, so that the teleport operation is feasible. This may be obtained, for instance, from a manually built directory of sports pages such as the open directory project (http://www.dmoz.org/) or that of Yahoo. Provided the set Sof sports-related pages is non-empty, it follows that there is a non-empty set of web pages Y⊇Sover which the random walk has a steady-state distribution; let us denote this sports PageRank distribution by ~ πs. For web pages not in Y, we set the PageRank values to zero. We call ~ πsthe topic-specific PageRank for sports.TOPIC-SPECIFIC PAGERANK We do not demand that teleporting takes the random surfer to a uniformly chosen sports page; the distribution over teleporting targets Scould in fact be arbitrary. In like manner we can envision topic-specific PageRank distributions for each of several topics such as science, religion, politics and so on. Each of these distributions assigns to each web page a PageRank value in the interval [0, 1). For a user interested in only a single topic from among these topics, we may invoke the corresponding PageRank distribution when scoring and ranking search results. This gives us the potential of considering settings in which the search engine knows what topic a user is interested in. This may happen because users either explicitly register their interests, or because the system learns by observing each user’s behavior over time. But what if a user is known to have a mixture of interests from multiple topics? For instance, a user may have an interest mixture (or profile) that is 60% sports and 40% politics; can we compute a personalized PageRank for this user? At first glance, this appears daunting: how could we possibly compute a different PageRank distribution for each user profile (with, potentially, infinitely many possible profiles)? We can in fact address this provided we assume that an individual’s interests can be well-approximated as a linear  ",21.2
2279,iir,iir-2279,21.2.3 Topic-speci?c PageRank,"   21.2 PageRank 471 21.2.3 Topic-specific PageRank Thus far we have discussed the PageRank computation with a teleport operation in which the surfer jumps to a random web page chosen uniformly at random. We now consider teleporting to a random web page chosen nonuniformly. In doing so, we are able to derive PageRank values tailored to particular interests. For instance, a sports aficionado might wish that pages on sports be ranked higher than non-sports pages. Suppose that web pages on sports are “near” one another in the web graph. Then, a random surfer who frequently finds himself on random sports pages is likely (in the course of the random walk) to spend most of his time at sports pages, so that the steady-state distribution of sports pages is boosted. Suppose our random surfer, endowed with a teleport operation as before, teleports to a random web page on the topic of sports instead of teleporting to a uniformly chosen random web page. We will not focus on how we collect all web pages on the topic of sports; in fact, we only need a non-zero subset Sof sports-related web pages, so that the teleport operation is feasible. This may be obtained, for instance, from a manually built directory of sports pages such as the open directory project (http://www.dmoz.org/) or that of Yahoo. Provided the set Sof sports-related pages is non-empty, it follows that there is a non-empty set of web pages Y⊇Sover which the random walk has a steady-state distribution; let us denote this sports PageRank distribution by ~ πs. For web pages not in Y, we set the PageRank values to zero. We call ~ πsthe topic-specific PageRank for sports.TOPIC-SPECIFIC PAGERANK We do not demand that teleporting takes the random surfer to a uniformly chosen sports page; the distribution over teleporting targets Scould in fact be arbitrary. In like manner we can envision topic-specific PageRank distributions for each of several topics such as science, religion, politics and so on. Each of these distributions assigns to each web page a PageRank value in the interval [0, 1). For a user interested in only a single topic from among these topics, we may invoke the corresponding PageRank distribution when scoring and ranking search results. This gives us the potential of considering settings in which the search engine knows what topic a user is interested in. This may happen because users either explicitly register their interests, or because the system learns by observing each user’s behavior over time. But what if a user is known to have a mixture of interests from multiple topics? For instance, a user may have an interest mixture (or profile) that is 60% sports and 40% politics; can we compute a personalized PageRank for this user? At first glance, this appears daunting: how could we possibly compute a different PageRank distribution for each user profile (with, potentially, infinitely many possible profiles)? We can in fact address this provided we assume that an individual’s interests can be well-approximated as a linear      472 21 Link analysis ◮Figure 21.5 Topic-specific PageRank. In this example we consider a user whose interests are 60% sports and 40% politics. If the teleportation probability is 10%, this user is modeled as teleporting 6% to sports pages and 4% to politics pages. combination of a small number of topic page distributions. A user with this mixture of interests could teleport as follows: determine first whether to teleport to the set Sof known sports pages, or to the set of known politics pages. This choice is made at random, choosing sports pages 60% of the time and politics pages 40% of the time. Once we choose that a particular teleport step is to (say) a random sports page, we choose a web page in Suniformly at random to teleport to. This in turn leads to an ergodic Markov chain with a steady-state distribution that is personalized to this user’s preferences over topics (see Exercise 21.16). While this idea has intuitive appeal, its implementation appears cumbersome: it seems to demand that for each user, we compute a transition prob     21.2 PageRank 473 ability matrix and compute its steady-state distribution. We are rescued by the fact that the evolution of the probability distribution over the states of a Markov chain can be viewed as a linear system. In Exercise 21.16 we will show that it is not necessary to compute a PageRank vector for every distinct combination of user interests over topics; the personalized PageRank vector for any user can be expressed as a linear combination of the underlying topicspecific PageRanks. For instance, the personalized PageRank vector for the user whose interests are 60% sports and 40% politics can be computed as 0.6~ πs+0.4~ πp, (21.7) where ~ πsand ~ πpare the topic-specific PageRank vectors for sports and for politics, respectively. ?Exercise 21.5 Write down the transition probability matrix for the example in Figure 21.2. Exercise 21.6 Consider a web graph with three nodes 1, 2 and 3. The links are as follows: 1 → 2, 3 →2, 2 →1, 2 →3. Write down the transition probability matrices for the surfer’s walk with teleporting, for the following three values of the teleport probability: (a) α=0; (b) α=0.5 and (c) α=1. Exercise 21.7 A user of a browser can, in addition to clicking a hyperlink on the page xhe is currently browsing, use the back button to go back to the page from which he arrived at x. Can such a user of back buttons be modeled as a Markov chain? How would we model repeated invocations of the back button? Exercise 21.8 Consider a Markov chain with three states A, B and C, and transition probabilities as follows. From state A, the next state is B with probability 1. From B, the next state is either A with probability pA, or state C with probability 1 −pA. From C the next state is A with probability 1. For what values of pA∈[0, 1]is this Markov chain ergodic? Exercise 21.9 Show that for any directed graph, the Markov chain induced by a random walk with the teleport operation is ergodic. Exercise 21.10 Show that the PageRank of every page is at least α/N. What does this imply about the difference in PageRank values (over the various pages) as αbecomes close to 1? Exercise 21.11 For the data in Example 21.1, write a small routine or use a scientific calculator to compute the PageRank values stated in Equation (21.6).     474 21 Link analysis Exercise 21.12 Suppose that the web graph is stored on disk as an adjacency list, in such a way that you may only query for the out-neighbors of pages in the order in which they are stored. You cannot load the graph in main memory but you may do multiple reads over the full graph. Write the algorithm for computing the PageRank in this setting. Exercise 21.13 Recall the sets Sand Yintroduced near the beginning of Section 21.2.3. How does the set Yrelate to S? Exercise 21.14 Is the set Yalways the set of all web pages? Why or why not? Exercise 21.15 [⋆⋆⋆] Is the sports PageRank of any page in Sat least as large as its PageRank? Exercise 21.16 [⋆⋆⋆] Consider a setting where we have two topic-specific PageRank values for each web page: a sports PageRank ~ πs, and a politics PageRank ~ πp. Let αbe the (common) teleportation probability used in computing both sets of topic-specific PageRanks. For q∈[0, 1], consider a user whose interest profile is divided between a fraction qin sports and a fraction 1 −qin politics. Show that the user’s personalized PageRank is the steady-state distribution of a random walk in which – on a teleport step – the walk teleports to a sports page with probability qand to a politics page with probability 1−q. Exercise 21.17 Show that the Markov chain corresponding to the walk in Exercise 21.16 is ergodic and hence the user’s personalized PageRank can be obtained by computing the steadystate distribution of this Markov chain. Exercise 21.18 Show that in the steady-state distribution of Exercise 21.17, the steady-state probability for any web page iequals qπs(i) + (1−q)πp(i).  ",21.2
2280,iir,iir-2280,21.3 Hubs and Authorities," 21.3 Hubs and Authorities We now develop a scheme in which, given a query, every web page is assigned two scores. One is called its hub score and the other its authority score.HUB SCORE AUTHORITY SCORE For any query, we compute two ranked lists of results rather than one. The ranking of one list is induced by the hub scores and that of the other by the authority scores. This approach stems from a particular insight into the creation of web pages, that there are two primary kinds of web pages useful as results for broad-topic searches. By a broad topic search we mean an informational query such as ""I wish to learn about leukemia"". There are authoritative sources of information on the topic; in this case, the National Cancer Institute’s page on   21.3 Hubs and Authorities 475 leukemia would be such a page. We will call such pages authorities; in the computation we are about to describe, they are the pages that will emerge with high authority scores. On the other hand, there are many pages on the Web that are hand-compiled lists of links to authoritative web pages on a specific topic. These hub pages are not in themselves authoritative sources of topic-specific information, but rather compilations that someone with an interest in the topic has spent time putting together. The approach we will take, then, is to use these hub pages to discover the authority pages. In the computation we now develop, these hub pages are the pages that will emerge with high hub scores. A good hub page is one that points to many good authorities; a good authority page is one that is pointed to by many good hub pages. We thus appear to have a circular definition of hubs and authorities; we will turn this into an iterative computation. Suppose that we have a subset of the web containing good hub and authority pages, together with the hyperlinks amongst them. We will iteratively compute a hub score and an authority score for every web page in this subset, deferring the discussion of how we pick this subset until Section 21.3.1. For a web page vin our subset of the web, we use h(v)to denote its hub score and a(v)its authority score. Initially, we set h(v) = a(v) = 1 for all nodes v. We also denote by v7→ ythe existence of a hyperlink from vto y. The core of the iterative algorithm is a pair of updates to the hub and authority scores of all pages given by Equation 21.8, which capture the intuitive notions that good hubs point to good authorities and that good authorities are pointed to by good hubs. h(v)←∑ v7→y a(y) (21.8) a(v)←∑ y7→v h(y). Thus, the first line of Equation (21.8) sets the hub score of page vto the sum of the authority scores of the pages it links to. In other words, if vlinks to pages with high authority scores, its hub score increases. The second line plays the reverse role; if page vis linked to by good hubs, its authority score increases. What happens as we perform these updates iteratively, recomputing hub scores, then new authority scores based on the recomputed hub scores, and so on? Let us recast the equations Equation (21.8) into matrix-vector form. Let~ hand~adenote the vectors of all hub and all authority scores respectively, for the pages in our subset of the web graph. Let Adenote the adjacency matrix of the subset of the web graph that we are dealing with: Ais a square matrix with one row and one column for each page in the subset. The entry     476 21 Link analysis Aij is 1 if there is a hyperlink from page ito page j, and 0 otherwise. Then, we may write Equation (21.8) ~ h←A~a (21.9) ~a←AT~ h, where ATdenotes the transpose of the matrix A. Now the right hand side of each line of Equation (21.9) is a vector that is the left hand side of the other line of Equation (21.9). Substituting these into one another, we may rewrite Equation (21.9) as ~ h←AAT~ h (21.10) ~a←ATA~a. Now, Equation (21.10) bears an uncanny resemblance to a pair of eigenvector equations (Section 18.1); indeed, if we replace the ←symbols by =symbols and introduce the (unknown) eigenvalue, the first line of Equation (21.10) becomes the equation for the eigenvectors of AAT, while the second becomes the equation for the eigenvectors of ATA: ~ h= (1/λh)AAT~ h ~a= (1/λa)ATA~a. (21.11) Here we have used λhto denote the eigenvalue of AATand λato denote the eigenvalue of ATA. This leads to some key consequences: 1\. The iterative updates in Equation (21.8) (or equivalently, Equation (21.9)), if scaled by the appropriate eigenvalues, are equivalent to the power iteration method for computing the eigenvectors of AATand ATA. Provided that the principal eigenvalue of AATis unique, the iteratively computed entries of~ hand~asettle into unique steady-state values determined by the entries of Aand hence the link structure of the graph. 2\. In computing these eigenvector entries, we are not restricted to using the power iteration method; indeed, we could use any fast method for computing the principal eigenvector of a stochastic matrix. The resulting computation thus takes the following form: 1\. Assemble the target subset of web pages, form the graph induced by their hyperlinks and compute AATand ATA. 2\. Compute the principal eigenvectors of AATand ATAto form the vector of hub scores~ hand authority scores~a.   21.3 Hubs and Authorities 477 3\. Output the top-scoring hubs and the top-scoring authorities. This method of link analysis is known as HITS, which is an acronym for Hyperlink-Induced Topic Search. ✎Example 21.2: Assuming the query jaguar and double-weighting of links whose anchors contain the query word, the matrix Afor Figure 21.4 is as follows: 0010000 0110000 1012000 0001100 0000001 0000011 0002101 The hub and authority vectors are: ~ h= (0.03 0.04 0.33 0.18 0.04 0.04 0.35) ~a= (0.10 0.01 0.12 0.47 0.16 0.01 0.13) Here, q3is the main authority – two hubs (q2and q6) are pointing to it via highly weighted jaguar links. Since the iterative updates captured the intuition of good hubs and good authorities, the high-scoring pages we output would give us good hubs and authorities from the target subset of web pages. In Section 21.3.1 we describe the remaining detail: how do we gather a target subset of web pages around a topic such as leukemia? 21.3.1 Choosing the subset of the Web ",21.3
2281,iir,iir-2281,21.3.1 Choosing the subset of the Web," 21.3.1 Choosing the subset of the Web In assembling a subset of web pages around a topic such as leukemia, we must cope with the fact that good authority pages may not contain the specific query term leukemia. This is especially true, as we noted in Section 21.1.1, when an authority page uses its web presence to project a certain marketing image. For instance, many pages on the IBM website are authoritative sources of information on computer hardware, even though these pages may not contain the term computer or hardware. However, a hub compiling computer hardware resources is likely to use these terms and also link to the relevant pages on the IBM website. Building on these observations, the following procedure has been suggested for compiling the subset of the Web for which to compute hub and authority scores.     478 21 Link analysis 1\. Given a query (say leukemia), use a text index to get all pages containing leukemia. Call this the root set of pages. 2\. Build the base set of pages, to include the root set as well as any page that either links to a page in the root set, or is linked to by a page in the root set. We then use the base set for computing hub and authority scores. The base set is constructed in this manner for three reasons: 1\. A good authority page may not contain the query text (such as computer hardware). 2\. If the text query manages to capture a good hub page vhin the root set, then the inclusion of all pages linked to by any page in the root set will capture all the good authorities linked to by vhin the base set. 3\. Conversely, if the text query manages to capture a good authority page vain the root set, then the inclusion of pages which point to vawill bring other good hubs into the base set. In other words, the “expansion” of the root set into the base set enriches the common pool of good hubs and authorities. Running HITS across a variety of queries reveals some interesting insights about link analysis. Frequently, the documents that emerge as top hubs and authorities include languages other than the language of the query. These pages were presumably drawn into the base set, following the assembly of the root set. Thus, some elements of cross-language retrieval (where a query in one language retrieves documents in another) are evident here; interestingly, this cross-language effect resulted purely from link analysis, with no linguistic translation taking place. We conclude this section with some notes on implementing this algorithm. The root set consists of all pages matching the text query; in fact, implementations (see the references in Section 21.4) suggest that it suffices to use 200 or so web pages for the root set, rather than all pages matching the text query. Any algorithm for computing eigenvectors may be used for computing the hub/authority score vector. In fact, we need not compute the exact values of these scores; it suffices to know the relative values of the scores so that we may identify the top hubs and authorities. To this end, it is possible that a small number of iterations of the power iteration method yields the relative ordering of the top hubs and authorities. Experiments have suggested that in practice, about five iterations of Equation (21.8) yield fairly good results. Moreover, since the link structure of the web graph is fairly sparse (the average web page links to about ten others), we do not perform these as matrix-vector products but rather as additive updates as in Equation (21.8).      21.3 Hubs and Authorities 479 ◮Figure 21.6 A sample run of HITS on the query japan elementary schools. Figure 21.6 shows the results of running HITS on the query japan elementary schools. The figure shows the top hubs and authorities; each row lists the title tag from the corresponding HTML page. Because the resulting string is not necessarily in Latin characters, the resulting print is (in many cases) a string of gibberish. Each of these corresponds to a web page that does not use Latin characters, in this case very likely pages in Japanese. There also appear to be pages in other non-English languages, which seems surprising given that the query string is in English. In fact, this result is emblematic of the functioning of HITS – following the assembly of the root set, the (English) query string is ignored. The base set is likely to contain pages in other languages, for instance if an English-language hub page links to the Japanese-language home pages of Japanese elementary schools. Because the subsequent computation of the top hubs and authorities is entirely linkbased, some of these non-English pages will appear among the top hubs and authorities. ?Exercise 21.19 If all the hub and authority scores are initialized to 1, what is the hub/authority score of a node after one iteration?      480 21 Link analysis Exercise 21.20 How would you interpret the entries of the matrices AATand ATA? What is the connection to the co-occurrence matrix CCTin Chapter 18? Exercise 21.21 What are the principal eigenvalues of AATand ATA? d1d2 d3 ◮Figure 21.7 Web graph for Exercise 21.22. Exercise 21.22 For the web graph in Figure 21.7, compute PageRank, hub and authority scores for each of the three pages. Also give the relative ordering of the 3 nodes for each of these scores, indicating any ties. PageRank: Assume that at each step of the PageRank random walk, we teleport to a random page with probability 0.1, with a uniform distribution over which particular page we teleport to. Hubs/Authorities: Normalize the hub (authority) scores so that the maximum hub (authority) score is 1. Hint 1: Using symmetries to simplify and solving with linear equations might be easier than using iterative methods. Hint 2: Provide the relative ordering (indicating any ties) of the three nodes for each of the three scoring measures.  ",21.3
2282,iir,iir-2282,21.4 References and further reading," 21.4 References and further reading Garfield (1955) is seminal in the science of citation analysis. This was built on by Pinski and Narin (1976) to develop a journal influence weight, whose definition is remarkably similar to that of the PageRank measure. The use of anchor text as an aid to searching and ranking stems from the work of McBryan (1994). Extended anchor-text was implicit in his work, with systematic experiments reported in Chakrabarti et al. (1998). Kemeny and Snell (1976) is a classic text on Markov chains. The PageRank measure was developed in Brin and Page (1998) and in Page et al. (1998).   21.4 References and further reading 481 A number of methods for the fast computation of PageRank values are surveyed in Berkhin (2005) and in Langville and Meyer (2006); the former also details how the PageRank eigenvector solution may be viewed as solving a linear system, leading to one way of solving Exercise 21.16. The effect of the teleport probability αhas been studied by Baeza-Yates et al. (2005) and by Boldi et al. (2005). Topic-specific PageRank and variants were developed in Haveliwala (2002), Haveliwala (2003) and in Jeh and Widom (2003). Berkhin (2006a) develops an alternate view of topic-specific PageRank. Ng et al. (2001b) suggests that the PageRank score assignment is more robust than HITS in the sense that scores are less sensitive to small changes in graph topology. However, it has also been noted that the teleport operation contributes significantly to PageRank’s robustness in this sense. Both PageRank and HITS can be “spammed” by the orchestrated insertion of links into the web graph; indeed, the Web is known to have such link farms that col-LINK FARMS lude to increase the score assigned to certain pages by various link analysis algorithms. The HITS algorithm is due to Kleinberg (1999). Chakrabarti et al. (1998) developed variants that weighted links in the iterative computation based on the presence of query terms in the pages being linked and compared these to results from several web search engines. Bharat and Henzinger (1998) further developed these and other heuristics, showing that certain combinations outperformed the basic HITS algorithm. Borodin et al. (2001) provides a systematic study of several variants of the HITS algorithm. Ng et al. (2001b) introduces a notion of stability for link analysis, arguing that small changes to link topology should not lead to significant changes in the ranked list of results for a query. Numerous other variants of HITS have been developed by a number of authors, the best know of which is perhaps SALSA (Lempel and Moran 2000).  ",21.4
2401,iir,iir-2401,3 Dictionaries and tolerant retrieval,"       49 3Dictionaries and tolerant retrieval In Chapters 1and 2we developed the ideas underlying inverted indexes for handling Boolean and proximity queries. Here, we develop techniques that are robust to typographical errors in the query, as well as alternative spellings. In Section 3.1 we develop data structures that help the search for terms in the vocabulary in an inverted index. In Section 3.2 we study the idea of a wildcard query: a query such as *a*e*i*o*u*, which seeks documents containing any term that includes all the five vowels in sequence. The *symbol indicates any (possibly empty) string of characters. Users pose such queries to a search engine when they are uncertain about how to spell a query term, or seek documents containing variants of a query term; for instance, the query automat* would seek documents containing any of the terms automatic, automation and automated. We then turn to other forms of imprecisely posed queries, focusing on spelling errors in Section 3.3. Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection. We detail a number of techniques for correcting spelling errors in queries, one term at a time as well as for an entire string of query terms. Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s). This can be especially useful in cases like the Herman example, where the user may not know how a proper name is spelled in documents in the collection. Because we will develop many variants of inverted indexes in this chapter, we will use sometimes the phrase standard inverted index to mean the inverted index developed in Chapters 1and 2, in which each vocabulary term has a postings list with the documents in the collection.  ",3.1
2402,iir,iir-2402,3.1 Search structures for dictionaries," 3.1 Search structures for dictionaries Given an inverted index and a query, our first task is to determine whether each query term exists in the vocabulary and if so, identify the pointer to the      50 3 Dictionaries and tolerant retrieval corresponding postings. This vocabulary lookup operation uses a classical data structure called the dictionary and has two broad classes of solutions: hashing, and search trees. In the literature of data structures, the entries in the vocabulary (in our case, terms) are often referred to as keys. The choice of solution (hashing, or search trees) is governed by a number of questions: (1) How many keys are we likely to have? (2) Is the number likely to remain static, or change a lot – and in the case of changes, are we likely to only have new keys inserted, or to also have some keys in the dictionary be deleted? (3) What are the relative frequencies with which various keys will be accessed? Hashing has been used for dictionary lookup in some search engines. Each vocabulary term (key) is hashed into an integer over a large enough space that hash collisions are unlikely; collisions if any are resolved by auxiliary structures that can demand care to maintain.1At query time, we hash each query term separately and following a pointer to the corresponding postings, taking into account any logic for resolving hash collisions. There is no easy way to find minor variants of a query term (such as the accented and non-accented versions of a word like resume), since these could be hashed to very different integers. In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2. Finally, in a setting (such as the Web) where the size of the vocabulary keeps growing, a hash function designed for current needs may not suffice in a few years’ time. Search trees overcome many of these issues – for instance, they permit us to enumerate all vocabulary terms beginning with automat. The best-known search tree is the binary tree, in which each internal node has two children. The search for a term begins at the root of the tree. Each internal node (including the root) represents a binary test, based on whose outcome the search proceeds to one of the two sub-trees below that node. Figure 3.1 gives an example of a binary search tree used for a dictionary. Efficient search (with a number of comparisons that is O(log M)) hinges on the tree being balanced: the numbers of terms under the two sub-trees of any node are either equal or differ by one. The principal issue here is that of rebalancing: as terms are inserted into or deleted from the binary search tree, it needs to be rebalanced so that the balance property is maintained. To mitigate rebalancing, one approach is to allow the number of sub-trees under an internal node to vary in a fixed interval. A search tree commonly used for a dictionary is the B-tree – a search tree in which every internal node has a number of children in the interval [a,b], where aand bare appropriate positive integers; Figure 3.2 shows an example with a=2 and b=4\. Each branch under an internal node again represents a test for a range of char1. So-called perfect hash functions are designed to preclude collisions, but are rather more complicated both to implement and to compute.       ",3.1
2403,iir,iir-2403,3.2 Wildcard queries," 3.2 Wildcard queries 51 ◮Figure 3.1 A binary search tree. In this example the branch at the root partitions vocabulary terms into two subtrees, those whose first letter is between aand m, and the rest. acter sequences, as in the binary tree example of Figure 3.1. A B-tree may be viewed as “collapsing” multiple levels of the binary tree into one; this is especially advantageous when some of the dictionary is disk-resident, in which case this collapsing serves the function of pre-fetching imminent binary tests. In such cases, the integers aand bare determined by the sizes of disk blocks. Section 3.5 contains pointers to further background on search trees and B-trees. It should be noted that unlike hashing, search trees demand that the characters used in the document collection have a prescribed ordering; for instance, the 26 letters of the English alphabet are always listed in the specific order Athrough Z. Some Asian languages such as Chinese do not always have a unique ordering, although by now all languages (including Chinese and Japanese) have adopted a standard ordering system for their character sets. 3.2 Wildcard queries Wildcard queries are used in any of the following situations: (1) the user is uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which      52 3 Dictionaries and tolerant retrieval ◮Figure 3.2 A B-tree. In this example every internal node has between 2 and 4 children. leads to the wildcard query S*dney); (2) the user is aware of multiple variants of spelling a term and (consciously) seeks documents containing any of the variants (e.g., color vs. colour); (3) the user seeks documents containing variants of a term that would be caught by stemming, but is unsure whether the search engine performs stemming (e.g., judicial vs. judiciary, leading to the wildcard query judicia*); (4) the user is uncertain of the correct rendition of a foreign word or phrase (e.g., the query Universit* Stuttgart). A query such as mon* is known as a trailing wildcard query, because the * WILDCARD QUERY symbol occurs only once, at the end of the search string. A search tree on the dictionary is a convenient way of handling trailing wildcard queries: we walk down the tree following the symbols m, o and nin turn, at which point we can enumerate the set Wof terms in the dictionary with the prefix mon. Finally, we use |W|lookups on the standard inverted index to retrieve all documents containing any term in W. But what about wildcard queries in which the *symbol is not constrained to be at the end of the search string? Before handling this general case, we mention a slight generalization of trailing wildcard queries. First, consider leading wildcard queries, or queries of the form *mon. Consider a reverse B-tree on the dictionary – one in which each root-to-leaf path of the B-tree corresponds to a term in the dictionary written backwards: thus, the term lemon would, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down the reverse B-tree then enumerates all terms Rin the vocabulary with a given prefix. In fact, using a regular B-tree together with a reverse B-tree, we can handle an even more general case: wildcard queries in which there is a single *symbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set Wof dictionary terms beginning with the prefix se, then the reverse B-tree to   3.2 Wildcard queries 53 enumerate the set Rof terms ending with the suffix mon. Next, we take the intersection W∩Rof these two sets, to arrive at the set of terms that begin with the prefix se and end with the suffix mon. Finally, we use the standard inverted index to retrieve all documents containing any terms in this intersection. We can thus handle wildcard queries that contain a single *symbol using two B-trees, the normal B-tree and a reverse B-tree. 3.2.1 General wildcard queries We now ",3.2
2404,iir,iir-2404,3.2.1 General wildcard queries," 3.2.1 General wildcard queries We now study two techniques for handling general wildcard queries. Both techniques share a common strategy: express the given wildcard query qwas a Boolean query Qon a specially constructed index, such that the answer to Qis a superset of the set of vocabulary terms matching qw. Then, we check each term in the answer to Qagainst qw, discarding those vocabulary terms that do not match qw. At this point we have the vocabulary terms matching qwand can resort to the standard inverted index. Permuterm indexes Our first special index for general wildcard queries is the permuterm index, a form of inverted index. First, we introduce a special symbol $into our character set, to mark the end of a term. Thus, the term hello is shown here as the augmented term hello$. Next, we construct a permuterm index, in which the various rotations of each term (augmented with $) all link to the original vocabulary term. Figure 3.3 gives an example of such a permuterm index entry for the term hello. We refer to the set of rotated terms in the permuterm index as the permuterm vocabulary. How does this index help us with wildcard queries? Consider the wildcard query m*n. The key is to rotate such a wildcard query so that the *symbol appears at the end of the string – thus the rotated wildcard query becomes n$m*. Next, we look up this string in the permuterm index, where seeking n$m* (via a search tree) leads to rotations of (among others) the terms man and moron. Now that the permuterm index enables us to identify the original vocabulary terms matching a wildcard query, we look up these terms in the standard inverted index to retrieve matching documents. We can thus handle any wildcard query with a single *symbol. But what about a query such as fi*mo*er? In this case we first enumerate the terms in the dictionary that are in the permuterm index of er$fi*. Not all such dictionary terms will have the string mo in the middle - we filter these out by exhaustive enumeration, checking each candidate to see if it contains mo. In this example, the term fishmonger would survive this filtering but filibuster would not. We then      54 3 Dictionaries and tolerant retrieval ◮Figure 3.3 A portion of a permuterm index. run the surviving terms through the standard inverted index for document retrieval. One disadvantage of the permuterm index is that its dictionary becomes quite large, including as it does all rotations of each term. Notice the close interplay between the B-tree and the permuterm index above. Indeed, it suggests that the structure should perhaps be viewed as a permuterm B-tree. However, we follow traditional terminology here in describing the permuterm index as distinct from the B-tree that allows us to select the rotations with a given prefix.  ",3.2
2405,iir,iir-2405,3.2.2 k-gram indexes for wildcard queries," 3.2.2 k-gram indexes for wildcard queries Whereas the permuterm index is simple, it can lead to a considerable blowup from the number of rotations per term; for a dictionary of English terms, this can represent an almost ten-fold space increase. We now present a second technique, known as the k-gram index, for processing wildcard queries. We will also use k-gram indexes in Section 3.3.4. A k-gram is a sequence of k characters. Thus cas,ast and stl are all 3-grams occurring in the term castle. We use a special character $to denote the beginning or end of a term, so the full set of 3-grams generated for castle is: $ca,cas,ast,stl,tle,le$. In a k-gram index, the dictionary contains all k-grams that occur in any term in the vocabulary. Each postings list points from a k-gram to all vocabulary      3.2 Wildcard queries 55 etr beetroot metric petrify retrieval \---◮Figure 3.4 Example of a postings list in a 3-gram index. Here the 3-gram etr is illustrated. Matching vocabulary terms are lexicographically ordered in the postings. terms containing that k-gram. For instance, the 3-gram etr would point to vocabulary terms such as metric and retrieval. An example is given in Figure 3.4. How does such an index help us with wildcard queries? Consider the wildcard query re*ve. We are seeking documents containing any term that begins with re and ends with ve. Accordingly, we run the Boolean query $re AND ve$. This is looked up in the 3-gram index and yields a list of matching terms such as relive,remove and retrieve. Each of these matching terms is then looked up in the standard inverted index to yield documents matching the query. There is however a difficulty with the use of k-gram indexes, that demands one further step of processing. Consider using the 3-gram index described above for the query red*. Following the process described above, we first issue the Boolean query $re AND red to the 3-gram index. This leads to a match on terms such as retired, which contain the conjunction of the two 3grams $re and red, yet do not match the original wildcard query red*. To cope with this, we introduce a post-filtering step, in which the terms enumerated by the Boolean query on the 3-gram index are checked individually against the original query red*. This is a simple string-matching operation and weeds out terms such as retired that do not match the original query. Terms that survive are then searched in the standard inverted index as usual. We have seen that a wildcard query can result in multiple terms being enumerated, each of which becomes a single-term query on the standard inverted index. Search engines do allow the combination of wildcard queries using Boolean operators, for example, re*d AND fe*ri. What is the appropriate semantics for such a query? Since each wildcard query turns into a disjunction of single-term queries, the appropriate interpretation of this example is that we have a conjunction of disjunctions: we seek all documents that contain any term matching re*d and any term matching fe*ri. Even without Boolean combinations of wildcard queries, the processing of a wildcard query can be quite expensive, because of the added lookup in the special index, filtering and finally the standard inverted index. A search engine may support such rich functionality, but most commonly, the capability is hidden behind an interface (say an “Advanced Query” interface) that most     56 3 Dictionaries and tolerant retrieval users never use. Exposing such functionality in the search interface often encourages users to invoke it even when they do not require it (say, by typing a prefix of their query followed by a *), increasing the processing load on the search engine. ?Exercise 3.1 In the permuterm index, each permuterm vocabulary term points to the original vocabulary term(s) from which it was derived. How many original vocabulary terms can there be in the postings list of a permuterm vocabulary term? Exercise 3.2 Write down the entries in the permuterm index dictionary that are generated by the term mama. Exercise 3.3 If you wanted to search for s*ng in a permuterm wildcard index, what key(s) would one do the lookup on? Exercise 3.4 Refer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the postings are lexicographically ordered. Why is this ordering useful? Exercise 3.5 Consider again the query fi*mo*er from Section 3.2.1. What Boolean query on a bigram index would be generated for this query? Can you think of a term that matches the permuterm query in Section 3.2.1, but does not satisfy this Boolean query? Exercise 3.6 Give an example of a sentence that falsely matches the wildcard query mon*h if the search were to simply use a conjunction of bigrams.  ",3.2
2406,iir,iir-2406,3.3 Spelling correction," 3.3 Spelling correction We next look at the problem of correcting spelling errors in queries. For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot. Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears: britian spears, britney’s spears, brandy spears and prittany spears. We look at two steps to solving this problem: the first based on edit distance and the second based on k-gram overlap. Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience.  ",3.3
2407,iir,iir-2407,3.3.1 Implementing spelling correction," 3.3.1 Implementing spelling correction There are two basic principles underlying most spelling correction algorithms. 1\. Of various alternative correct spellings for a mis-spelled query, choose the “nearest” one. This demands that we have a notion of nearness or proximity between a pair of queries. We will develop these proximity measures in Section 3.3.3. 2\. When two correctly spelled queries are tied (or nearly tied), select the one that is more common. For instance, grunt and grant both seem equally plausible as corrections for grnt. Then, the algorithm should choose the more common of grunt and grant as the correction. The simplest notion of more common is to consider the number of occurrences of the term in the collection; thus if grunt occurs more often than grant, it would be the chosen correction. A different notion of more common is employed in many search engines, especially on the web. The idea is to use the correction that is most common among queries typed in by other users. The idea here is that if grunt is typed as a query more often than grant, then it is more likely that the user who typed grnt intended to type the query grunt. Beginning in Section 3.3.3 we describe notions of proximity between queries, as well as their efficient computation. Spelling correction algorithms build on these computations of proximity; their functionality is then exposed to users in one of several ways: 1\. On the query carot always retrieve documents containing carot as well as any “spell-corrected” version of carot, including carrot and tarot. 2\. As in (1) above, but only when the query term carot is not in the dictionary. 3\. As in (1) above, but only when the original query returned fewer than a preset number of documents (say fewer than five documents). 4\. When the original query returns fewer than a preset number of documents, the search interface presents a spelling suggestion to the end user: this suggestion consists of the spell-corrected query term(s). Thus, the search engine might respond to the user: “Did you mean carrot?” 3.3.2 Forms of spelling correction We focus on two specific form ",3.3
2408,iir,iir-2408,3.3.2 Forms of spelling correction," 3.3.2 Forms of spelling correction We focus on two specific forms of spelling correction that we refer to as isolated-term correction and context-sensitive correction. In isolated-term correction, we attempt to correct a single query term at a time – even when we     58 3 Dictionaries and tolerant retrieval have a multiple-term query. The carot example demonstrates this type of correction. Such isolated-term correction would fail to detect, for instance, that the query flew form Heathrow contains a mis-spelling of the term from – because each term in the query is correctly spelled in isolation. We begin by examining two techniques for addressing isolated-term correction: edit distance, and k-gram overlap. We then proceed to contextsensitive correction.  ",3.3
2409,iir,iir-2409,3.3.3 Edit distance," 3.3.3 Edit distance Given two character strings s1and s2, the edit distance between them is the minimum number of edit operations required to transform s1into s2. Most commonly, the edit operations allowed for this purpose are: (i) insert a character into a string; (ii) delete a character from a string and (iii) replace a character of a string by another character; for these operations, edit distance is sometimes known as Levenshtein distance. For example, the edit distance between cat and dog is 3. In fact, the notion of edit distance can be generalized to allowing different weights for different kinds of edit operations, for instance a higher weight may be placed on replacing the character sby the character p, than on replacing it by the character a(the latter being closer to s on the keyboard). Setting weights in this way depending on the likelihood of letters substituting for each other is very effective in practice (see Section 3.4 for the separate issue of phonetic similarity). However, the remainder of our treatment here will focus on the case in which all edit operations have the same weight. It is well-known how to compute the (weighted) edit distance between two strings in time O(|s1| × |s2|), where |si|denotes the length of a string si. The idea is to use the dynamic programming algorithm in Figure 3.5, where the characters in s1and s2are given in array form. The algorithm fills the (integer) entries in a matrix mwhose two dimensions equal the lengths of the two strings whose edit distances is being computed; the (i,j)entry of the matrix will hold (after the algorithm is executed) the edit distance between the strings consisting of the first icharacters of s1and the first jcharacters of s2. The central dynamic programming step is depicted in Lines 8-10 of Figure 3.5, where the three quantities whose minimum is taken correspond to substituting a character in s1, inserting a character in s1and inserting a character in s2. Figure 3.6 shows an example Levenshtein distance computation of Figure 3.5. The typical cell [i,j]has four entries formatted as a 2 ×2 cell. The lower right entry in each cell is the min of the other three, corresponding to the main dynamic programming step in Figure 3.5. The 2 ×2 cell in the [i,j] entry of the table shows the three numbers whose minimum yields the fourth. The cells in italics determine the edit distance in this example. s2[j],m[i−1, j] + 1 and m[i,j−1] + 1. The cells with numbers in italics depict the path by which we determine the Levenshtein distance. The spelling correction problem however demands more than computing edit distance: given a set Sof strings (corresponding to terms in the vocabulary) and a query string q, we seek the string(s) in Vof least edit distance from q. We may view this as a decoding problem, in which the codewords (the strings in V) are prescribed in advance. The obvious way of doing this is to compute the edit distance from qto each string in V, before selecting the     60 3 Dictionaries and tolerant retrieval string(s) of minimum edit distance. This exhaustive search is inordinately expensive. Accordingly, a number of heuristics are used in practice to efficiently retrieve vocabulary terms likely to have low edit distance to the query term(s). The simplest such heuristic is to restrict the search to dictionary terms beginning with the same letter as the query string; the hope would be that spelling errors do not occur in the first character of the query. A more sophisticated variant of this heuristic is to use a version of the permuterm index, in which we omit the end-of-word symbol $. Consider the set of all rotations of the query string q. For each rotation rfrom this set, we traverse the B-tree into the permuterm index, thereby retrieving all dictionary terms that have a rotation beginning with r. For instance, if qis mase and we consider the rotation r=sema, we would retrieve dictionary terms such as semantic and semaphore that do not have a small edit distance to q. Unfortunately, we would miss more pertinent dictionary terms such as mare and mane. To address this, we refine this rotation scheme: for each rotation, we omit a suffix of ℓcharacters before performing the B-tree traversal. This ensures that each term in the set Rof terms retrieved from the dictionary includes a “long” substring in common with q. The value of ℓcould depend on the length of q. Alternatively, we may set it to a fixed constant such as 2.  ",3.3
2410,iir,iir-2410,3.3.4 k-gram indexes for spelling correction," 3.3.4 k-gram indexes for spelling correction To further limit the set of vocabulary terms for which we compute edit distances to the query term, we now show how to invoke the k-gram index of Section 3.2.2 (page 54) to assist with retrieving vocabulary terms with low edit distance to the query q. Once we retrieve such terms, we can then find the ones of least edit distance from q. In fact, we will use the k-gram index to retrieve vocabulary terms that have many k-grams in common with the query. We will argue that for reasonable definitions of “many k-grams in common,” the retrieval process is essentially that of a single scan through the postings for the k-grams in the query string q. The 2-gram (or bigram) index in Figure 3.7 shows (a portion of) the postings for the three bigrams in the query bord. Suppose we wanted to retrieve vocabulary terms that contained at least two of these three bigrams. A single scan of the postings (much as in Chapter 1) would let us enumerate all such terms; in the example of Figure 3.7 we would enumerate aboard, boardroom and border. This straightforward application of the linear scan intersection of postings immediately reveals the shortcoming of simply requiring matched vocabulary terms to contain a fixed number of k-grams from the query q: terms like boardroom, an implausible “correction” of bord, get enumerated. Conse      3.3 Spelling correction 61 rd aboard ardent boardroom border or border lord morbid sordid bo aboard about boardroom border \---\---\---◮Figure 3.7 Matching at least two of the three 2-grams in the query bord. quently, we require more nuanced measures of the overlap in k-grams between a vocabulary term and q. The linear scan intersection can be adapted when the measure of overlap is the Jaccard coefficient for measuring the overlap between two sets Aand B, defined to be |A∩B|/|A∪B|. The two sets we consider are the set of k-grams in the query q, and the set of k-grams in a vocabulary term. As the scan proceeds, we proceed from one vocabulary term tto the next, computing on the fly the Jaccard coefficient between qand t. If the coefficient exceeds a preset threshold, we add tto the output; if not, we move on to the next term in the postings. To compute the Jaccard coefficient, we need the set of k-grams in qand t. Since we are scanning the postings for all k-grams in q, we immediately have these k-grams on hand. What about the k-grams of t? In principle, we could enumerate these on the fly from t; in practice this is not only slow but potentially infeasible since, in all likelihood, the postings entries themselves do not contain the complete string tbut rather some encoding of t. The crucial observation is that to compute the Jaccard coefficient, we only need the length of the string t. To see this, recall the example of Figure 3.7 and consider the point when the postings scan for query q=bord reaches term t=boardroom. We know that two bigrams match. If the postings stored the (pre-computed) number of bigrams in boardroom (namely, 8), we have all the information we require to compute the Jaccard coefficient to be 2/(8+3−2); the numerator is obtained from the number of postings hits (2, from bo and rd) while the denominator is the sum of the number of bigrams in bord and boardroom, less the number of postings hits. We could replace the Jaccard coefficient by other measures that allow efficient on the fly computation during postings scans. How do we use these     62 3 Dictionaries and tolerant retrieval for spelling correction? One method that has some empirical support is to first use the k-gram index to enumerate a set of candidate vocabulary terms that are potential corrections of q. We then compute the edit distance from q to each term in this set, selecting terms from the set with small edit distance to q.  ",3.3
2411,iir,iir-2411,3.3.5 Context sensitive spelling correction," 3.3.5 Context sensitive spelling correction Isolated-term correction would fail to correct typographical errors such as flew form Heathrow, where all three query terms are correctly spelled. When a phrase such as this retrieves few documents, a search engine may like to offer the corrected query flew from Heathrow. The simplest way to do this is to enumerate corrections of each of the three query terms (using the methods leading up to Section 3.3.4) even though each query term is correctly spelled, then try substitutions of each correction in the phrase. For the example flew form Heathrow, we enumerate such phrases as fled form Heathrow and flew fore Heathrow. For each such substitute phrase, the search engine runs the query and determines the number of matching results. This enumeration can be expensive if we find many corrections of the individual terms, since we could encounter a large number of combinations of alternatives. Several heuristics are used to trim this space. In the example above, as we expand the alternatives for flew and form, we retain only the most frequent combinations in the collection or in the query logs, which contain previous queries by users. For instance, we would retain flew from as an alternative to try and extend to a three-term corrected query, but perhaps not fled fore or flea form. In this example, the biword fled fore is likely to be rare compared to the biword flew from. Then, we only attempt to extend the list of top biwords (such as flew from), to corrections of Heathrow. As an alternative to using the biword statistics in the collection, we may use the logs of queries issued by users; these could of course include queries with spelling errors. ?Exercise 3.7 If |si|denotes the length of string si, show that the edit distance between s1and s2is never more than max{|s1|,|s2|}. Exercise 3.8 Compute the edit distance between paris and alice. Write down the 5 ×5 array of distances between all prefixes as computed by the algorithm in Figure 3.5. Exercise 3.9 Write pseudocode showing the details of computing on the fly the Jaccard coefficient while scanning the postings of the k-gram index, as mentioned on page 61. Exercise 3.10 Compute the Jaccard coefficients between the query bord and each of the terms in Figure 3.7 that contain the bigram or.  ",3.3
2412,iir,iir-2412,3.4 Phonetic correction," 3.4 Phonetic correction 63 Exercise 3.11 Consider the four-term query catched in the rye and suppose that each of the query terms has five alternative terms suggested by isolated-term correction. How many possible corrected phrases must we consider if we do not trim the space of corrected phrases, but instead try all six variants for each of the terms? Exercise 3.12 For each of the prefixes of the query — catched,catched in and catched in the — we have a number of substitute prefixes arising from each term and its alternatives. Suppose that we were to retain only the top 10 of these substitute prefixes, as measured by its number of occurrences in the collection. We eliminate the rest from consideration for extension to longer prefixes: thus, if batched in is not one of the 10 most common 2-term queries in the collection, we do not consider any extension of batched in as possibly leading to a correction of catched in the rye. How many of the possible substitute prefixes are we eliminating at each phase? Exercise 3.13 Are we guaranteed that retaining and extending only the 10 commonest substitute prefixes of catched in will lead to one of the 10 commonest substitute prefixes of catched in the? 3.4 Phonetic correction Our final technique for tolerant retrieval has to do with phonetic correction: misspellings that arise because the user types a query that sounds like the target term. Such algorithms are especially applicable to searches on the names of people. The main idea here is to generate, for each term, a “phonetic hash” so that similar-sounding terms hash to the same value. The idea owes its origins to work in international police departments from the early 20th century, seeking to match names for wanted criminals despite the names being spelled differently in different countries. It is mainly used to correct phonetic misspellings in proper nouns. Algorithms for such phonetic hashing are commonly collectively known as soundex algorithms. However, there is an original soundex algorithm, with various variants, built on the following scheme: 1\. Turn every term to be indexed into a 4-character reduced form. Build an inverted index from these reduced forms to the original terms; call this the soundex index. 2\. Do the same with query terms. 3\. When the query calls for a soundex match, search this soundex index. The variations in different soundex algorithms have to do with the conversion of terms to 4-character forms. A commonly used conversion results in a 4-character code, with the first character being a letter of the alphabet and the other three being digits between 0 and 9.  ",3.4
2104,iir,iir-2104,4 Index construction,"In this chapter, we look at how to construct an inverted index. We call this process index construction or indexing; the process or machine that performs the indexer. The design of indexing algorithms is governed by hardware constraints. We therefore begin this chapter with a review of the basics of computer hardware that are relevant for indexing. We then introduce blocked sort-based indexing (Section 4.2), an efficient single-machine algorithm designed for static collections that can be viewed as a more scalable version of the basic sort-based indexing algorithm we introduced in Chapter 1. Section 4.3 describes single-pass in-memory indexing, an algorithm that has even better scaling properties because it does not hold the vocabulary in memory. For very large collections like the web, indexing has to be distributed over computer clusters with hundreds or thousands of machines. We discuss this in Section 4.4. Collections with frequent changes require dynamic indexing introduced in Section 4.5 so that changes in the collection are immediately reflected in the index. Finally, we cover some complicating issues that can arise in indexing – such as security and indexes for ranked retrieval – in Section 4.6. Index construction interacts with several topics covered in other chapters. The indexer needs raw text, but documents are encoded in many ways (see Chapter 2). Indexers compress and decompress intermediate files and the final index (see Chapter 5). In web search, documents are not on a local file system, but have to be spidered or crawled (see Chapter 20). In enterprise search, most documents are encapsulated in varied content management systems, email applications, and databases. We give some examples in Section 4.7. Although most of these applications can be accessed via http, native Application Programming Interfaces (APIs) are usually more efficient. The reader should be aware that building the subsystem that feeds raw text to the indexing process can in itself be a challenging problem.   The seek time is the time needed to position the disk head in a new position. The transfer time per byte is the rate of transfer from disk to memory when the head is in the right position. Symbol Statistic Value is average seek time 5 ms =5×10−3s b transfer time per byte 0.02 µs=2×10−8s processor’s clock rate 109s−1 plowlevel operation (e.g., compare &amp; swap a word) 0.01 µs=10−8s size of main memory several GB size of disk space 1 TB or more  ",4.1
2105,iir,iir-2105,4.1 Hardware basics,"  When building an information retrieval (IR) system, many decisions are based on the characteristics of the computer hardware on which the system runs. We therefore begin this chapter with a brief review of computer hardware. Performance characteristics typical of systems in 2007 are shown in Table 4.1. A list of hardware basics that we need in this book to motivate IR system design follows. Access to data in memory is much faster than access to data on disk. It takes a few clock cycles (perhaps 5 ×10−9seconds) to access a byte in memory, but much longer to transfer it from disk (about 2 ×10−8seconds). Consequently, we want to keep as much data as possible in memory, especially those data that we need to access frequently. We call the technique of keeping frequently used disk data in main memory caching. When doing a disk read or write, it takes a while for the disk head to move to the part of the disk where the data are located. This time is called the seek time and it averages 5 ms for typical disks. No data are beingtransferred during the seek. To maximize data transfer rates, chunks of data that will be read together should therefore be stored contiguously on disk. For example, using the numbers in Table 4.1 it may take as little as 0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is stored as one chunk, but up to 0.2 +100 ×(5×10−3) = 0.7 seconds if it is stored in 100 noncontiguous chunks because we need to move the disk head up to 100 times. Operating systems generally read and write entire blocks. Thus, reading a single byte from disk can take as much time as reading the entire block.      ",4.1
2106,iir,iir-2106,4.2 Blocked sort-based indexing," Block sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We call the part of main memory where a block being read or written is stored a buffer. Data transfers from disk to memory are handled by the system bus, not by the processor. This means that the processor is available to process data during disk I/O. We can exploit this fact to speed up data transfers by storing compressed data on disk. Assuming an efficient decompression algorithm, the total time of reading and then decompressing compressed data is usually less than reading uncompressed data. Servers used in IR systems typically have several gigabytes (GB) of main memory, sometimes tens of GB. Available disk space is several orders of magnitude larger.  The basic steps in constructing a nonpositional index are depicted in Figure 1.4 (page 8). We first make a pass through the collection assembling all term–docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. In this chapter, we describe methods for large collections that require the use of secondary storage. To make index construction more efficient, we represent terms as termIDs (instead of strings as we did in Figure 1.4), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection; or, in a two-pass approach, we compile the vocabulary in the first pass and construct the inverted index in the second pass. The index construction algorithms described in this chapter all do a single pass through the data. Section 4.7 gives references to multipass algorithms that are preferable in certain applications, for example, when disk space is scarce. We work with the Reuters-RCV1 collection as our model collection in this chapter, a collection with roughly 1 GB of text. It consists of about 800,000 documents that were sent over the Reuters newswire during a 1-year period between August 20, 1996, and August 19, 1997. A typical document is shown in Figure 4.1, but note that we ignore multimedia information like images in this book and are only concerned with text. Reuters-RCV1 covers a wide range of international topics, including politics, business, sports, and (as in this example) science. Some key statistics of the collection are shown in Table 4.2.  Values are rounded for the computations in this book. The unrounded values are: 806,791 documents, 222 tokens per document, 391,523 (distinct) terms, 6.04 bytes per token with spaces and punctuation, 4.5 bytes per token without spaces and punctuation, 7.5 bytes per term, and 96,969,056 tokens. The numbers in this table correspond to the third line (“case folding”) in Table 5.1 (page 87). Symbol Statistic Value N documents 800,000 Lave avg. # tokens per document 200 Mterms 400,000 avg. # bytes per token (incl. spaces/punct.) 6 avg. # bytes per token (without spaces/punct.) 4.5 avg. # bytes per term 7.5 Tokens 100,000,000  Extreme conditions create rare Antarctic clouds Document from the Reuters newswire. Reuters-RCV1 has 100 million tokens. Collecting all termID–docID pairs of the collection using 4 bytes each for termID and docID therefore requires 0.8 GB of storage. Typical collections today are often one or two orders of magnitude larger than Reuters-RCV1. You can easily see how such collections overwhelm even large computers if we try to sort their termID–docID pairs in memory. If the size of the intermediate files during index construction is within a small factor of available memory, then the compression techniques introduced in Chapter 5can help; however, the postings file of many large collections cannot fit into memory even after compression. With main memory insufficient, we need to use an external sorting algorithm, that is, one that uses disk. For acceptable speed, the central require.  The algorithm stores inverted blocks in files f1, . . . , find the merged index in fmerged. ment of such an algorithm is that it minimize the number of random disk seeks during sorting – sequential disk reads are far faster than seeks as we explained in Section 4.1. One solution is the blocked sort-based indexing algorithm or BSBI in Figure 4.2. BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID–docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. The algorithm parses documents into termID–docID pairs and accumulates the pairs in memory until a block of a fixed size is full ( in Figure 4.2). We choose the block size to fit comfortably into memory to permit a fast in-memory sort. The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID–docID pairs. Next, we collect all termID–docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk. Applying this to Reuters-RCV1 and assuming we can fit 10 million termID–docID pairs into memory, we end up with ten blocks, each an inverted index of one part of the collection. In the final step, the algorithm simultaneously merges the ten blocks into one large merged index. An example with two blocks is shown in Figure 4.3, where we use dito denote the ith document of the collection. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. In each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary. How expensive is BSBI? Its time complexity is Θ(Tlog T)because the step with the highest time complexity is sorting and Tis an upper bound for the number of items we must sort (i.e., the number of termID–docID pairs). But   Merging in blocked sort-based indexing. Two blocks (“postings lists to be merged”) are loaded from disk into memory, merged in memory (“merged postings lists”) and written back to disk. We show terms instead of termIDs for better readability. the actual indexing time is usually dominated by the time it takes to parse the documents (PARSENEXTBLOCK) and to do the final merge (MERGEBLOCKS). Exercise 4.6 asks you to compute the total index construction time for RCV1 that includes these steps as well as inverting the blocks and writing them to disk. Notice that Reuters-RCV1 is not particularly large in an age when one or more GB of memory are standard on personal computers. With appropriate compression (Chapter 5), we could have created an inverted index for RCV1 in memory on a not overly beefy server. The techniques we have described are needed, however, for collections that are several orders of magnitude larger. ?Exercise 4.1 If we need Tlog2Tcomparisons (where Tis the number of termID–docID pairs) and two disk seeks for each comparison, how much time would index construction for Reuters-RCV1 take if we used disk instead of memory for storage and an unoptimized sorting algorithm (i.e., not an external sorting algorithm)? Use the system parameters in Table 4.1. Exercise 4.2 [⋆] How would you create the dictionary in blocked sort-based indexing on the fly to avoid an extra pass through the data?       ",4.2
2107,iir,iir-2107,4.3 Single-pass in-memory indexing," Single-pass in-memory indexing Blocked sort-based indexing has excellent scaling properties, but it needs a data structure for mapping terms to termIDs. For very large collections, this data structure does not fit into memory. A more scalable alternative is single-pass in-memory indexing or SPIMI. SPIMI uses terms instead of termIDs ,SINGLE-PASS IN-MEMORY INDEXING writes each block’s dictionary to disk, and then starts a new dictionary for the next block. SPIMI can index collections of any size as long as there is enough disk space available. The SPIMI algorithm is shown in Figure 4.4. The part of the algorithm that parses documents and turns them into a stream of term–docID pairs, which we call tokens here, has been omitted. SPIMI-INVERT is called repeatedly on the token stream until the entire collection has been processed. Tokens are processed one by one (line 4) during each successive call of SPIMI-INVERT. When a term occurs for the first time, it is added to the dictionary (best implemented as a hash), and a new postings list is created (line 6). The call in line 7 returns this postings list for subsequent occurrences of the term. A difference between BSBI and SPIMI is that SPIMI adds a posting directly to its postings list (line 10). Instead of first collecting all termID–docID pairs and then sorting them (as we did in BSBI), each postings list is dynamic (i.e., its size is adjusted as it grows) and it is immediately available to collect postings. This has two advantages: It is faster because there is no sorting required, and it saves memory because we keep track of the term a postings      list belongs to, so the termIDs of postings need not be stored. As a result, the blocks that individual calls of SPIMI-INVERT can process are much larger and the index construction process as a whole is more efficient. Because we do not know how large the postings list of a term will be when we first encounter it, we allocate space for a short postings list initially and double the space each time it is full (lines 8–9). This means that some memory is wasted, which counteracts the memory savings from the omission of termIDs in intermediate data structures. However, the overall memory requirements for the dynamically constructed index of a block in SPIMI are still lower than in BSBI. When memory has been exhausted, we write the index of the block (which consists of the dictionary and the postings lists) to disk (line 12). We have to sort the terms (line 11) before doing this because we want to write postings lists in lexicographic order to facilitate the final merging step. If each block’s postings lists were written in unsorted order, merging blocks could not be accomplished by a simple linear scan through each block. Each call of SPIMI-INVERT writes a block to disk, just as in BSBI. The last step of SPIMI (corresponding to line 7 in Figure 4.2; not shown in Figure 4.4) is then to merge the blocks into the final inverted index. In addition to constructing a new dictionary structure for each block and eliminating the expensive sorting step, SPIMI has a third important component: compression. Both the postings and the dictionary terms can be stored compactly on disk if we employ compression. Compression increases the efficiency of the algorithm further because we can process even larger blocks, and because the individual blocks require less space on disk. We refer readers to the literature for this aspect of the algorithm (Section 4.7). The time complexity of SPIMI is Θ(T)because no sorting of tokens is required and all operations are at most linear in the size of the collection.  ",4.3
2108,iir,iir-2108,4.4 Distributed indexing,"  Distributed indexing Collections are often so large that we cannot perform index construction efficiently on a single machine. This is particularly true of the World Wide Web for which we need large computer clusters1to construct any reasonably sized web index. Web search engines, therefore, use distributed indexing algorithms for index construction. The result of the construction process is a distributed index that is partitioned across several machines – either according to term or according to document. In this section, we describe distributed indexing for a term-partitioned index. Most large search engines prefer a document1. A cluster in this chapter is a group of tightly coupled computers that work together closely. This sense of the word is different from the use of cluster as a group of documents that are semantically similar in Chapters 16–18.   4.4 Distributed indexing 75 partitioned index (which can be easily generated from a term-partitioned index). We discuss this topic further in Section 20.3 (page 454). The distributed index construction method we describe in this section is an application of MapReduce, a general architecture for distributed computing. MapReduce is designed for large computer clusters. The point of a cluster is to solve large computing problems on cheap commodity machines or nodes that are built from standard parts (processor, memory, disk) as opposed to on a supercomputer with specialized hardware. Although hundreds or thousands of machines are available in such clusters, individual machines can fail at any time. One requirement for robust distributed indexing is, therefore, that we divide the work up into chunks that we can easily assign and – in case of failure – reassign. A master node directs the process of assigning and reassigning tasks to individual worker nodes. The map and reduce phases of MapReduce split up the computing job into chunks that standard machines can process in a short time. The various steps of MapReduce are shown in Figure 4.5 and an example on a collection consisting of two documents is shown in Figure 4.6. First, the input data, in our case a collection of web pages, are split into n splits where the size of the split is chosen to ensure that the work can be distributed evenly (chunks should not be too large) and efficiently (the total number of chunks we need to manage should not be too large); 16 or 64 MB are good sizes in distributed indexing. Splits are not preassigned to machines, but are instead assigned by the master node on an ongoing basis: As a machine finishes processing one split, it is assigned the next one. If a machine dies or becomes a laggard due to hardware problems, the split it is working on is simply reassigned to another machine. In general, MapReduce breaks a large computing problem into smaller parts by recasting it in terms of manipulation of key-value pairs. For indexing, a key-value pair has the form (termID,docID). In distributed indexing, the mapping from terms to termIDs is also distributed and therefore more complex than in single-machine indexing. A simple solution is to maintain a (perhaps precomputed) mapping for frequent terms that is copied to all nodes and to use terms directly (instead of termIDs) for infrequent terms. We do not address this problem here and assume that all nodes share a consistent term →termID mapping. The map phase of MapReduce consists of mapping splits of the input data to key-value pairs. This is the same parsing task we also encountered in BSBI and SPIMI, and we therefore call the machines that execute the map phase parsers. Each parser writes its output to local intermediate files, the segment files (shown as a-f g-p q-z in Figure 4.5). For the reduce phase, we want all values for a given key to be stored close together, so that they can be read and processed quickly. This is achieved by  Index construction master assign map phase reduce phase assign parser splits parser parser inverter postings inve rter inve rter a-f g-p q-z a-f g-p q-z a-f g-p q-z a-f segment files g-p q-z ◮Figure 4.5 An example of distributed indexing with MapReduce. Adapted from Dean and Ghemawat (2004), partitioning the keys into jterm partitions and having the parsers write keyvalue pairs for each term partition into a separate segment file. In Figure 4.5, the term partitions are according to first letter: a–f, g–p, q–z, and j=3. (We chose these key ranges for ease of exposition. In general, key ranges need not correspond to contiguous terms or termIDs.) The term partitions are defined by the person who operates the indexing system (Exercise 4.10). The parsers then write corresponding segment files, one for each term partition. Each term partition thus corresponds to segments files, where is the number of parsers. For instance, Figure 4.5 shows three a–f segment files of the a–f partition, corresponding to the three parsers shown in the figure. Collecting all values (here: docIDs) for a given key (here: termID) into one list is the task of the inverters in the reduce phase. The master assigns each term partition to a different inverter – and, as in the case of parsers, reassigns term partitions in case of failing or slow inverters. Each term partition (corresponding to r segment files, one on each parser) is processed by one inverter. We assume here that segment files are of a size that a single machine can handle (Exercise 4.9). Finally, the list of values is sorted for each key and written to the final sorted postings list (“postings” in the figure). (Note that postings in Figure 4.6 include term frequencies, whereas each posting in the other sections of this chapter is simply a docID without term frequency information.) The data flow is shown for a–f in Figure 4.5. This completes the construction of the inverted index. In general, the map function produces a list of key-value pairs. All values for a key are collected into one list in the reduce phase. This list is then processed further. The instantiations of the two functions and an example are shown for index construction. Because the map phase processes documents in a distributed fashion, termID–docID pairs need not be ordered correctly initially as in this example. The example shows terms instead of termIDs for better readability. We abbreviate Caesar as Cand conquered as c’ed. Parsers and inverters are not separate sets of machines. The master identifies idle machines and assigns tasks to them. The same machine can be a parser in the map phase and an inverter in the reduce phase. And there are often other jobs that run in parallel with index construction, so in between being a parser and an inverter a machine might do some crawling or another unrelated task. To minimize write times before inverters reduce the data, each parser writes its segment files to its local disk. In the reduce phase, the master communicates to an inverter the locations of the relevant segment files (e.g., of the r segment files of the a–f partition). Each segment file only requires one sequential read because all data relevant to a particular inverter were written to a single segment file by the parser. This setup minimizes the amount of network traffic needed during indexing. Figure 4.6 shows the general schema of the MapReduce functions. Input and output are often lists of key-value pairs themselves, so that several MapReduce jobs can run in sequence. In fact, this was the design of the Google indexing system in 2004. What we describe in this section corresponds to only one of five to ten MapReduce operations in that indexing system. Another MapReduce operation transforms the term-partitioned index we just created into a document-partitioned one. MapReduce offers a robust and conceptually simple framework for implementing index construction in a distributed environment. By providing a semiautomatic method for splitting index construction into smaller tasks, it can scale to almost arbitrarily large collections, given computer clusters of     78 4 Index construction sufficient size. Exercise 4.3 For n=15 splits, r=10 segments, and j=3 term partitions, how long would distributed index creation take for Reuters-RCV1 in a MapReduce architecture? Base your assumptions about cluster machines on Table 4.1.  ",4.4
2109,iir,iir-2109,4.5 Dynamic indexing," Thus far, we have assumed that the document collection is static. This is fine for collections that change infrequently or never (e.g., the Bible or Shakespeare). But most collections are modified frequently with documents being added, deleted, and updated. This means that new terms need to be added to the dictionary, and postings lists need to be updated for existing terms. The simplest way to achieve this is to periodically reconstruct the index from scratch. This is a good solution if the number of changes over time is small and a delay in making new documents searchable is acceptable – and if enough resources are available to construct a new index while the old one is still available for querying. If there is a requirement that new documents be included quickly, one solution is to maintain two indexes: a large main index and a small auxiliary index  that stores new documents. The auxiliary index is kept in memory. Searches are run across both indexes and results merged. Deletions are stored in an invalidation bit vector. We can then filter out deleted documents before returning the search result. Documents are updated by deleting and reinserting them. Each time the auxiliary index becomes too large, we merge it into the main index. The cost of this merging operation depends on how we store the index in the file system. If we store each postings list as a separate file, then the merge simply consists of extending each postings list of the main index by the corresponding postings list of the auxiliary index. In this scheme, the reason for keeping the auxiliary index is to reduce the number of disk seeks required over time. Updating each document separately requires up to Mave disk seeks, where ave is the average size of the vocabulary of documents in the collection. With an auxiliary index, we only put additional load on the disk when we merge auxiliary and main indexes. Unfortunately, the one-file-per-postings-list scheme is infeasible because most file systems cannot efficiently handle very large numbers of files. The simplest alternative is to store the index as one large file, that is, as a concatenation of all postings lists. In reality, we often choose a compromise between the two extremes (Section 4.7). To simplify the discussion, we choose the simple option of storing the index as one large file here.    Each token (termID,docID) is initially added to in-memory index Z0 by LMERGEADDTOKEN. LOGARITHMIC MERGE initializes Z0 and indexes. In this scheme, we process each posting ⌊T/n⌋times because we touch it during each of ⌊T/n⌋merges where nis the size of the auxiliary index and T the total number of postings. Thus, the overall time complexity is Θ(T2/n). (We neglect the representation of terms here and consider only the docIDs. For the purpose of time complexity, a postings list is simply a list of docIDs.) We can do better than Θ(T2/n)by introducing log2(T/n)indexes I0,I1, I2, . ..of size 20×n, 21×n, 22×n. . . . Postings percolate up this sequence of indexes and are processed only once on each level. This scheme is called log-arithmatic merging (Figure 4.7). As before, up to n postings are accumulated in an in-memory auxiliary index, which we call Z0. When the limit nis reached, the 20×npostings in Z0 are transferred to a new index I0 that is created on disk. The next time Z0 is full, it is merged with I0 to create an index Z1 of size 21×n. Then Z1 is either stored as I1(if there isn’t already an I1) or merged with I1 into Z2 (if I1exists); and so on. We service search requests by querying in-memory Z0 and all currently valid indexes I ion disk and merging the results. Readers familiar with the binomial heap data structure 2 will recog2. See, for example, (Cormen et al. 1990, Chapter 19).Index construction size its similarity with the structure of the inverted indexes in logarithmic merging. Overall index construction time is Θ(Tlog(T/n)) because each posting is processed only once on each of the log(T/n)levels. We trade this efficiency gain for a slow down of query processing; we now need to merge results from log(T/n) indexes as opposed to just two (the main and auxiliary indexes). As in the auxiliary index scheme, we still need to merge very large indexes occasionally (which slows down the search system during the merge), but this happens less frequently and the indexes involved in a merge on average are smaller. Having multiple indexes complicates the maintenance of collection-wide statistics. For example, it affects the spelling correction algorithm in Section 3.3 (page 56) that selects the corrected alternative with the most hits. With multiple indexes and an invalidation bit vector, the correct number of hits for a term is no longer a simple lookup. In fact, all aspects of an IR system – index maintenance, query processing, distribution, and so on – are more complex in logarithmic merging. Because of this complexity of dynamic indexing, some large search engines adopt a reconstruction-from-scratch strategy. They do not construct indexes dynamically. Instead, a new index is built from scratch periodically. Query processing is then switched from the new index and the old index is deleted.   ",4.5
2110,iir,iir-2110,4.6 Other types of indexes," This chapter only describes construction of nonpositional indexes. Except for the much larger data volume we need to accommodate, the main difference for positional indexes is that (termID, docID, (position1, position2, . ..)) triples, instead of (termID, docID) pairs have to be processed and that tokens and postings contain positional information in addition to docIDs. With this change, the algorithms discussed here can all be applied to positional indexes. In the indexes we have considered so far, postings lists are ordered with respect to docID. As we see in Chapter 5, this is advantageous for compres   4.6 Other types of indexes 81 users documents 0/1 doc e., 1 otherwise 0 if user can’t read ◮Figure 4.8 A user-document matrix for access control lists. Element (i,j)is 1 if user has access to document jand 0 otherwise. During query processing, a user’s access postings list is intersected with the results list returned by the text part of the index. sion – instead of docIDs we can compress smaller gaps between IDs, thus reducing space requirements for the index. However, this structure for the index is not optimal when we build ranked (Chapters 6and 7) – as opposed to Boolean – retrieval systems. In ranked retrieval, postings are often ordered according to weight or impact, with the highest-weighted postings occurring first. With this organization, scanning of long postings lists during query processing can usually be terminated early when weights have become so small that any further documents can be predicted to be of low similarity to the query (see Chapter 6). In a docID-sorted index, new documents are always inserted at the end of postings lists. In an impact-sorted index (Section 7.1.5, page 140), the insertion can occur anywhere, thus complicating the update of the inverted index. Security is an important consideration for retrieval systems in corporations. A low-level employee should not be able to find the salary roster of the corporation, but authorized managers need to be able to search for it. Users’ results lists must not contain documents they are barred from opening; the very existence of a document can be sensitive information. User authorization is often mediated through access control lists or ACLs. ACLs can be dealt with in an information retrieval system by representing each document as the set of users that can access them (Figure 4.8) and then inverting the resulting user-document matrix. The inverted ACL index has, for each user, a “postings list” of documents they can access – the user’s access list. Search results are then intersected with this list. However, such an index is difficult to maintain when access permissions change – we discussed these difficulties in the context of incremental indexing for regular postings lists in Section 4.5. It also requires the processing of very long postings lists for users with access to large document subsets. User membership is therefore often verified by retrieving access information directly from the file system at query time – even though this slows down retrieval.  The five steps in constructing an index for Reuters-RCV1 in blocked sort-based indexing. Line numbers refer to Figure 4.2. Step Time 1 reading of collection (line 4) 2 10 initial sorts of 107records each (line 5) 3 writing of 10 blocks (line 6) 4 total disk transfer time for merging (line 7) 5 time of actual merging (line 7) total ◮Table 4.4 Collection statistics for a large collection. Symbol Statistic Value N# documents 1,000,000,000 Lave # tokens per document 1000 M# distinct terms 44,000,000 We discussed indexes for storing and retrieving terms (as opposed to documents) in Chapter 3. ?Exercise 4.5 Can spelling correction compromise document-level security? Consider the case where a spelling correction is based on documents to which the user does not have access. ?Exercise 4.6 Total index construction time in blocked sort-based indexing is broken down in Table 4.3. Fill out the time column of the table for Reuters-RCV1 assuming a system with the parameters given in Table 4.1. Exercise 4.7 Repeat Exercise 4.6 for the larger collection in Table 4.4. Choose a block size that is realistic for current technology (remember that a block should easily fit into main memory). How many blocks do you need? Exercise 4.8 Assume that we have a collection of modest size whose index can be constructed with the simple in-memory indexing algorithm in Figure 1.4 (page 8). For this collection, compare memory, disk and time requirements of the simple algorithm in Figure 1.4 and blocked sort-based indexing. Exercise 4.9 Assume that machines in MapReduce have 100 GB of disk space each. Assume further that the postings list of the term the has a size of 200 GB. Then the MapReduce algorithm as described cannot be run to construct the index. How would you modify MapReduce so that it can handle this case?      ",4.6
2111,iir,iir-2111,4.7 References and further reading," 4.7 References and further reading 83 Exercise 4.10 For optimal load balancing, the inverters in MapReduce must get segmented postings files of similar sizes. For a new collection, the distribution of key-value pairs may not be known in advance. How would you solve this problem? Exercise 4.11 Apply MapReduce to the problem of counting how often each term occurs in a set of files. Specify map and reduce operations for this task. Write down an example along the lines of Figure 4.6. Exercise 4.12 We claimed (on page 80) that an auxiliary index can impair the quality of collection statistics. An example is the term weighting method idf, which is defined as log(N/dfi)where Nis the total number of documents and dfiis the number of documents that term ioccurs in (Section 6.2.1, page 117). Show that even a small auxiliary index can cause significant error in idf when it is computed on the main index only. Consider a rare term that suddenly occurs frequently (e.g., Flossie as in Tropical Storm Flossie). 4.7 References and further reading Witten et al. (1999, Chapter 5) present an extensive treatment of the subject of index construction and additional indexing algorithms with different tradeoffs of memory, disk space, and time. In general, blocked sort-based indexing does well on all three counts. However, if conserving memory or disk space is the main criterion, then other algorithms may be a better choice. See Witten et al. (1999), Tables 5.4 and 5.5; BSBI is closest to “sort-based multiway merge,” but the two algorithms differ in dictionary structure and use of compression. Moffat and Bell (1995) show how to construct an index “in situ,” that is, with disk space usage close to what is needed for the final index and with a minimum of additional temporary files (cf. also Harman and Candela (1990)). They give Lesk (1988) and Somogyi (1990) credit for being among the first to employ sorting for index construction. The SPIMI method in Section 4.3 is from (Heinz and Zobel 2003). We have simplified several aspects of the algorithm, including compression and the fact that each term’s data structure also contains, in addition to the postings list, its document frequency and house keeping information. We recommend Heinz and Zobel (2003) and Zobel and Moffat (2006) as up-do-date, in-depth treatments of index construction. Other algorithms with good scaling properties with respect to vocabulary size require several passes through the data, e.g., FAST-INV (Fox and Lee 1991,Harman et al. 1992). The MapReduce architecture was introduced by Dean and Ghemawat (2004). An open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/. Ribeiro-Neto et al. (1999) and Melnik et al. (2001) describe other approaches     84 4 Index construction to distributed indexing. Introductory chapters on distributed IR are (BaezaYates and Ribeiro-Neto 1999, Chapter 9) and (Grossman and Frieder 2004, Chapter 8). See also Callan (2000). Lester et al. (2005) and Büttcher and Clarke (2005a) analyze the properties of logarithmic merging and compare it with other construction methods. One of the first uses of this method was in Lucene (http://lucene.apache.org). Other dynamic indexing methods are discussed by Büttcher et al. (2006) and Lester et al. (2006). The latter paper also discusses the strategy of replacing the old index by one built from scratch. Heinz et al. (2002) compare data structures for accumulating the vocabulary in memory. Büttcher and Clarke (2005b) discuss security models for a common inverted index for multiple users. A detailed characterization of the Reuters-RCV1 collection can be found in (Lewis et al. 2004). NIST distributes the collection (see http://trec.nist.gov/data/reuters/reuters.html). Garcia-Molina et al. (1999, Chapter 2) review computer hardware relevant to system design in depth. An effective indexer for enterprise search needs to be able to communicate efficiently with a number of applications that hold text data in corporations, including Microsoft Outlook, IBM’s Lotus software, databases like Oracle and MySQL, content management systems like Open Text, and enterprise resource planning software like SAP.         85 5Index compression Chapter 1introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). In this chapter, we employ a number of compression techniques for dictionary and inverted index that are essential for efficient IR systems. One benefit of compression is immediately clear. We need less disk space. As we will see, compression ratios of 1:4 are easy to achieve, potentially cutting the cost of storing the index by 75%. There are two more subtle benefits of compression. The first is increased use of caching. Search systems use some parts of the dictionary and the index much more than others. For example, if we cache the postings list of a frequently used query term t, then the computations necessary for responding to the one-term query tcan be entirely done in memory. With compression, we can fit a lot more information into main memory. Instead of having to expend a disk seek when processing a query with t, we instead access its postings list in memory and decompress it. As we will see below, there are simple and efficient decompression methods, so that the penalty of having to decompress the postings list is small. As a result, we are able to decrease the response time of the IR system substantially. Because memory is a more expensive resource than disk space, increased speed owing to caching – rather than decreased space requirements – is often the prime motivator for compression. The second more subtle advantage of compression is faster transfer of data from disk to memory. Efficient decompression algorithms run so fast on modern hardware that the total time of transferring a compressed chunk of data from disk and then decompressing it is usually less than transferring the same chunk of data in uncompressed form. For instance, we can reduce input/output (I/O) time by loading a much smaller compressed postings list, even when you add on the cost of decompression. So, in most cases, the retrieval system runs faster on compressed postings lists than on uncompressed postings lists. If the main goal of compression is to conserve disk space, then the speed  ",4.7
2112,iir,iir-2112,5 Index compression,"      Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). In this chapter, we employ a number of compression techniques for dictionary and inverted index that are essential for efficient IR systems. One benefit of compression is immediately clear. We need less disk space. As we will see, compression ratios of 1:4 are easy to achieve, potentially cutting the cost of storing the index by 75%. There are two more subtle benefits of compression. The first is increased use of caching. Search systems use some parts of the dictionary and the index much more than others. For example, if we cache the postings list of a frequently used query term t, then the computations necessary for responding to the one-term query can be entirely done in memory. With compression, we can fit a lot more information into main memory. Instead of having to expend a disk seek when processing a query with t, we instead access its postings list in memory and decompress it. As we will see below, there are simple and efficient decompression methods, so that the penalty of having to decompress the postings list is small. As a result, we are able to decrease the response time of the IR system substantially. Because memory is a more expensive resource than disk space, increased speed owing to caching – rather than decreased space requirements – is often the prime motivator for compression. The second more subtle advantage of compression is faster transfer of data from disk to memory. Efficient decompression algorithms run so fast on modern hardware that the total time of transferring a compressed chunk of data from disk and then decompressing it is usually less than transferring the same chunk of data in uncompressed form. For instance, we can reduce input/output (I/O) time by loading a much smaller compressed postings list, even when you add on the cost of decompression. So, in most cases, the retrieval system runs faster on compressed postings lists than on uncompressed postings lists. If the main goal of compression is to conserve disk space, then the speed  of compression algorithms is of no concern. But for improved cache utilization and faster disk-to-memory transfer, decompression speeds must be high. The compression algorithms we discuss in this chapter are highly efficient and can therefore serve all three purposes of index compression. In this chapter, we define a posting as a docID in a postings list. For example, the postings list (6; 20, 45, 100), where 6 is the termID of the list’s term, contains three postings. As discussed in Section 2.4.2 (page 41), postings in most search systems also contain frequency and position information; but we will only consider simple docID postings here. See Section 5.4 for references on compressing frequencies and positions. This chapter first gives a statistical characterization of the distribution of the entities we want to compress – terms and postings in large collections (Section 5.1). We then look at compression of the dictionary, using the dictionary as-a-string method and blocked storage (Section 5.2). Section 5.3 describes two techniques for compressing the postings file, variable byte encoding and γ-encoding. 5.1 Statistical properties of terms in i ",5.1
2113,iir,iir-2113,5.1 Statistical properties of terms in information retrieval," As in the last chapter, we use Reuters-RCV1 as our model collection (see Table 4.2, page 70). We give some term and postings statistics for the collection in Table 5.1. “∆%” indicates the reduction in size from the previous line. “T%” is the cumulative reduction from unfiltered. The table shows the number of terms for different levels of preprocessing (column 2). The number of terms is the main factor in determining the size of the dictionary. The number of nonpositional postings (column 3) is an indicator of the expected size of the nonpositional index of the collection. The expected size of a positional index is related to the number of positions it must encode (column 4). In general, the statistics in Table 5.1 show that preprocessing affects the size of the dictionary and the number of nonpositional postings greatly. Stemming and case folding reduce the number of (distinct) terms by 17% each and the number of nonpositional postings by 4% and 3%, respectively. The treatment of the most frequent words is also important. The rule of 30 states that the 30 most common words account for 30% of the tokens in written text (31% in the table). Eliminating the 150 most common words from indexing (as stop words; cf. Section 2.2.2, page 27) cuts 25% to 30% of the nonpositional postings. But, although a stop list of 150 words reduces the number of postings by a quarter or more, this size reduction does not carry over to the size of the compressed index. As we will see later in this chapter, the postings lists of frequent words require only a few bits per posting after compression. The deltas in the table are in a range typical of large collections. Note,   5.1 Statistical properties of terms in information retrieval 87 ◮Table 5.1 The effect of preprocessing on the number of terms, nonpositional postings, and tokens for Reuters-RCV1. “∆%” indicates the reduction in size from the previous line, except that “30 stop words” and “150 stop words” both use “case folding” as their reference line. “T%” is the cumulative (“total”) reduction from unfiltered. We performed stemming with the Porter stemmer (Chapter 2, page 33). tokens (=number of position (distinct) terms nonpositional postings entries in postings) number ∆% T% number ∆% T% number ∆% T% unfiltered 484,494 109,971,179 197,879,290 no numbers 473,723 −2−2 100,680,242 −8−8 179,158,204 −9−9 case folding 391,523 −17 −19 96,969,056 −3−12 179,158,204 −0−9 30 stop words 391,493 −0−19 83,390,443 −14 −24 121,857,825 −31 −38 150 stop words 391,373 −0−19 67,001,847 −30 −39 94,516,599 −47 −52 stemming 322,383 −17 −33 63,812,300 −4−42 94,516,599 −0−52 however, that the percentage reductions can be very different for some text collections. For example, for a collection of web pages with a high proportion of French text, a lemmatizer for French reduces vocabulary size much more than the Porter stemmer does for an English-only collection because French is a morphologically richer language than English. The compression techniques we describe in the remainder of this chapter are lossless, that is, all information is preserved. Better compression ratios can be achieved with lossy compression, which discards some information. Case folding, stemming, and stop word elimination are forms of lossy compression. Similarly, the vector space model (Chapter 6) and dimensionality reduction techniques like latent semantic indexing (Chapter 18) create compact representations from which we cannot fully restore the original collection. Lossy compression makes sense when the “lost” information is unlikely ever to be used by the search system. For example, web search is characterized by a large number of documents, short queries, and users who only look at the first few pages of results. As a consequence, we can discard postings of documents that would only be used for hits far down the list. Thus, there are retrieval scenarios where lossy methods can be used for compression without any reduction in effectiveness. Before introducing techniques for compressing the dictionary, we want to estimate the number of distinct terms Min a collection. It is sometimes said that languages have a vocabulary of a certain size. The second edition of the Oxford English Dictionary (OED) defines more than 600,000 words. But the vocabulary of most large collections is much larger than the OED. The OED does not include most names of people, locations, products, or scientific Vocabulary size as a function of collection size T (number of tokens) for Reuters-RCV1. For these data, the dashed line log10 M= 0.49 ∗log10 T+1.64 is the best least-squares fit. Thus, k=101.64 ≈44 and b=0.49. entities like genes. These names need to be included in the inverted index, so our users can search for them.  ",5.1
2114,iir,iir-2114,5.1.1 Heaps’ law: Estimating the number of terms," 5.1.1 Heaps’ law: Estimating the number of terms A better way of getting a handle on Mis Heaps’ law, which estimates vocabulary size as a function of collection size: M=kTb (5.1) where Tis the number of tokens in the collection. Typical values for the parameters kand bare: 30 ≤k≤100 and b≈0.5. The motivation for Heaps’ law is that the simplest possible relationship between collection size and vocabulary size is linear in log–log space and the assumption of linearity is usually born out in practice as shown in Figure 5.1 for Reuters-RCV1. In this case, the fit is excellent for T&gt;105=100,000, for the parameter values b=0.49 and k=44. For example, for the first 1,000,020 tokens Heaps’ law      5.1 Statistical properties of terms in information retrieval 89 predicts 38,323 terms: 44 ×1,000,0200.49 ≈38,323. The actual number is 38,365 terms, very close to the prediction. The parameter kis quite variable because vocabulary growth depends a lot on the nature of the collection and how it is processed. Case-folding and stemming reduce the growth rate of the vocabulary, whereas including numbers and spelling errors increase it. Regardless of the values of the parameters for a particular collection, Heaps’ law suggests that (i) the dictionary size continues to increase with more documents in the collection, rather than a maximum vocabulary size being reached, and (ii) the size of the dictionary is quite large for large collections. These two hypotheses have been empirically shown to be true of large text collections (Section 5.4). So dictionary compression is important for an effective information retrieval system.  ",5.1
2115,iir,iir-2115,5.1.2 Zipf’s law: Modeling the distribution of terms," 5.1.2 Zipf’s law: Modeling the distribution of terms We also want to understand how terms are distributed across documents. This helps us to characterize the properties of the algorithms for compressing postings lists in Section 5.3. A commonly used model of the distribution of terms in a collection is Zipf’s law. It states that, if t1is the most common term in the collection, t2is the next most common, and so on, then the collection frequency cfiof the ith most common term is proportional to 1/i: cfi∝1 i. (5.2) So if the most frequent term occurs cf1times, then the second most frequent term has half as many occurrences, the third most frequent term a third as many occurrences, and so on. The intuition is that frequency decreases very rapidly with rank. Equation (5.2) is one of the simplest ways of formalizing such a rapid decrease and it has been found to be a reasonably good model. Equivalently, we can write Zipf’s law as cfi=cikor as log cfi=log c+ klog i where k=−1 and cis a constant to be defined in Section 5.3.2. It is therefore a power law with exponent k=−1. See Chapter 19, page 426, for another power law, a law characterizing the distribution of links on web pages. The log–log graph in Figure 5.2 plots the collection frequency of a term as a function of its rank for Reuters-RCV1. A line with slope –1, corresponding to the Zipf function log cfi=log c−log i, is also shown. The fit of the data to the law is not particularly good, but good enough to serve as a model for term distributions in our calculations in Section 5.3.      90 5 Index compression 0123456 01234567 log10 rank 7 log10 cf ◮Figure 5.2 Zipf’s law for Reuters-RCV1. Frequency is plotted as a function of frequency rank for the terms in the collection. The line is the distribution predicted by Zipf’s law (weighted least-squares fit; intercept is 6.95). ?Exercise 5.1 [⋆] Assuming one machine word per posting, what is the size of the uncompressed (nonpositional) index for different tokenizations based on Table 5.1? How do these numbers compare with Table 5.6?  ",5.1
2116,iir,iir-2116,5.2 Dictionary compression," 5.2 Dictionary compression This section presents a series of dictionary data structures that achieve increasingly higher compression ratios. The dictionary is small compared with the postings file as suggested by Table 5.1. So why compress it if it is responsible for only a small percentage of the overall space requirements of the IR system? One of the primary factors in determining the response time of an IR system is the number of disk seeks necessary to process a query. If parts of the dictionary are on disk, then many more disk seeks are necessary in query evaluation. Thus, the main goal of compressing the dictionary is to fit it in main memory, or at least a large portion of it, to support high query through   5.2 Dictionary compression 91 term document frequency pointer to postings list a 656,265 −→ aachen 65 −→ ... ... ... zulu 221 −→ space needed: 20 bytes 4 bytes 4 bytes ◮Figure 5.3 Storing the dictionary as an array of fixed-width entries. put. Although dictionaries of very large collections fit into the memory of a standard desktop machine, this is not true of many other application scenarios. For example, an enterprise search server for a large corporation may have to index a multiterabyte collection with a comparatively large vocabulary because of the presence of documents in many different languages. We also want to be able to design search systems for limited hardware such as mobile phones and onboard computers. Other reasons for wanting to conserve memory are fast startup time and having to share resources with other applications. The search system on your PC must get along with the memory-hogging word processing suite you are using at the same time. 5.2.1 Dictionary as a string The simpl ",5.2
2117,iir,iir-2117,5.2.1 Dictionary as a string," 5.2.1 Dictionary as a string The simplest data structure for the dictionary is to sort the vocabulary lexicographically and store it in an array of fixed-width entries as shown in Figure 5.3. We allocate 20 bytes for the term itself (because few terms have more than twenty characters in English), 4 bytes for its document frequency, and 4 bytes for the pointer to its postings list. Four-byte pointers resolve a 4 gigabytes (GB) address space. For large collections like the web, we need to allocate more bytes per pointer. We look up terms in the array by binary search. For Reuters-RCV1, we need M×(20 +4+4) = 400,000 ×28 = 11.2megabytes (MB) for storing the dictionary in this scheme. Using fixed-width entries for terms is clearly wasteful. The average length of a term in English is about eight characters (Table 4.2, page 70), so on average we are wasting twelve characters in the fixed-width scheme. Also, we have no way of storing terms with more than twenty characters like hydrochlorofluorocarbons and supercalifragilisticexpialidocious. We can overcome these shortcomings by storing the dictionary terms as one long string of characters, as shown in Figure 5.4. The pointer to the next term is also used to demarcate the end of the current term. As before, we locate terms in the data structure by way of binary search in the (now smaller) table. This scheme saves us 60% compared to fixed-width storage – 12 bytes on average of the.  Pointers mark the end of the preceding term and the beginning of the next. For example, the first three terms in this example are systile,syzygetic, and syzygial. 20 bytes we allocated for terms before. However, we now also need to store term pointers. The term pointers resolve 400,000 ×8=3.2 ×106positions, so they need to be log23.2 ×106≈22 bits or 3 bytes long. In this new scheme, we need 400,000 ×(4+4+3+8) = 7.6 MB for the Reuters-RCV1 dictionary: 4 bytes each for frequency and postings pointer, 3 bytes for the term pointer, and 8 bytes on average for the term. So we have reduced the space requirements by one third from 11.2 to 7.6 MB.  ",5.2
2118,iir,iir-2118,5.2.2 Blocked storage," We can further compress the dictionary by grouping terms in the string into blocks of size k and keeping a term pointer only for the first term of each block (Figure 5.5). We store the length of the term in the string as an additional byte at the beginning of the term. We thus eliminate k−1 term pointers, but need an additional k bytes for storing the length of each term. For k=4, we save (k−1)×3=9 bytes for term pointers, but need an additional k=4 bytes for term lengths. So the total space requirements for the dictionary of Reuters-RCV1 are reduced by 5 bytes per four-term block, or a total of 400,000 ×1/4 ×5=0.5 MB, bringing us down to 7.1 MB.       The first block consists of systile,syzygetic,syzygial, and syzygy with lengths of seven, nine, eight, and six characters, respectively. Each term is preceded by a byte encoding its length that indicates how many bytes to skip to reach subsequent terms. By increasing the block size k, we get better compression. However, there is a tradeoff between compression and the speed of term lookup. For the eight-term dictionary in Figure 5.6, steps in binary search are shown as double lines and steps in list search as simple lines. We search for terms in the uncompressed dictionary by binary search (a). In the compressed dictionary, we first locate the term’s block by binary search and then its position within the list by linear search through the block (b). Searching the uncompressed dictionary in (a) takes on average (0+1+2+3+2+1+2+2)/8 ≈1.6 steps, assuming each term is equally likely to come up in a query. For example, finding the two terms, aid and box, takes three and two steps, respectively. With blocks of size k=4 in (b), we need (0+1+2+3+4+1+2+3)/8 =2 steps on average, ≈25% more. For example, finding den takes one binary search step and two steps through the block. By increasing k, we can get the size of the compressed dictionary arbitrarily close to the minimum of 400,000 ×(4+4+1+8) = 6.8 MB, but term lookup becomes prohibitively slow for large values of k. One source of redundancy in the dictionary we have not exploited yet is the fact that consecutive entries in an alphabetically sorted list share common prefixes. This observation leads to front coding (Figure 5.7).  A sequence of terms with identical prefix (“automat”) is encoded by marking the end of the prefix with ∗and replacing it with ⋄in subsequent terms. As before, the first byte of each entry encodes the number of characters.       ",5.2
2119,iir,iir-2119,5.3 Postings file compression," Dictionary compression for Reuters-RCV1. data structure size in MB dictionary, fixed-width 11.2 dictionary, term pointers into string 7.6 ∼, with blocking, k=4 7.1 ∼, with blocking &amp; front coding 5.9 is identified for a subsequence of the term list and then referred to with a special character. In the case of Reuters, front coding saves another 1.2 MB, as we found in an experiment. Other schemes with even greater compression rely on minimal perfect hashing, that is, a hash function that maps Mterms onto [1, . . . , M] without collisions. However, we cannot adapt perfect hashes incrementally because each new term causes a collision and therefore requires the creation of a new perfect hash function. Therefore, they cannot be used in a dynamic environment. Even with the best compression scheme, it may not be feasible to store the entire dictionary in main memory for very large text collections and for hardware with limited memory. If we have to partition the dictionary onto pages that are stored on disk, then we can index the first term of each page using a B-tree. For processing most queries, the search system has to go to disk anyway to fetch the postings. One additional seek for retrieving the term’s dictionary page from disk is a significant, but tolerable increase in the time it takes to process a query. Table 5.2 summarizes the compression achieved by the four dictionary data structures. ?Exercise 5.2 Estimate the space usage of the Reuters-RCV1 dictionary with blocks of size k=8 and k=16 in blocked dictionary storage. Exercise 5.3 Estimate the time needed for term lookup in the compressed dictionary of ReutersRCV1 with block sizes of k=4 (Figure 5.6, b), k=8, and k=16. What is the slowdown compared with k=1 (Figure 5.6, a)? 5.3 Postings file compression Recall from Table 4.2 (page 70) that Reuters-RCV1 has 800,000 documents, 200 tokens per document, six characters per token, and 100,000,000 postings where we define a posting in this chapter as a docID in a postings list, that is, excluding frequency and position information. These numbers      96 5 Index compression ◮Table 5.3 Encoding gaps instead of document IDs. For example, we store gaps 107, 5, 43, . . . , instead of docIDs 283154, 283159, 283202, . . . for computer. The first docID is left unchanged (only shown for arachnocentric). encoding postings list the docIDs . .. 283042 283043 283044 283045 gaps 1 1 1 computer docIDs . . . 283047 283154 283159 283202 gaps 107 5 43 arachnocentric docIDs 252000 500100 gaps 252000 248100 correspond to line 3 (“case folding”) in Table 5.1. Document identifiers are log2800,000 ≈20 bits long. Thus, the size of the collection is about 800,000 × 200 ×6 bytes =960 MB and the size of the uncompressed postings file is 100,000,000 ×20/8 =250 MB. To devise a more efficient representation of the postings file, one that uses fewer than 20 bits per document, we observe that the postings for frequent terms are close together. Imagine going through the documents of a collection one by one and looking for a frequent term like computer. We will find a document containing computer, then we skip a few documents that do not contain it, then there is again a document with the term and so on (see Table 5.3). The key idea is that the gaps between postings are short, requiring a lot less space than 20 bits to store. In fact, gaps for the most frequent terms such as the and for are mostly equal to 1. But the gaps for a rare term that occurs only once or twice in a collection (e.g., arachnocentric in Table 5.3) have the same order of magnitude as the docIDs and need 20 bits. For an economical representation of this distribution of gaps, we need a variable encoding method that uses fewer bits for short gaps. To encode small numbers in less space than large numbers, we look at two types of methods: bytewise compression and bitwise compression. As the names suggest, these methods attempt to encode gaps with the minimum number of bytes and bits, respectively.  ",5.3
2120,iir,iir-2120,5.3.1 Variable byte codes," 5.3.1 Variable byte codes Variable byte (VB) encoding uses an integral number of bytes to encode a gap. The last 7 bits of a byte are “payload” and encode part of the gap. The first bit of the byte is a continuation bit.It is set to 1 for the last byte of the encoded gap and to 0 otherwise. To decode a variable byte code, we read a sequence of bytes with continuation bit 0 terminated by a byte with continuation bit 1. We then extract and concatenate the 7-bit parts. The functions div and mod compute integer division and remainder after integer division, respectively. PREPEND adds an element to the beginning of a list, for example, (h1,2i, 3) = h3, 1, 2i.  extends a list, for example, EXTEND(h1,2i,h3, 4i) = h1, 2, 3, 4i. ◮Table 5.4 VB encoding. Gaps are encoded using an integral number of bytes. The first bit, the continuation bit, of each byte indicates whether the code ends with this byte (1) or not (0). docIDs 824 829 215406 gaps 5 214577 VB code 00000110 10111000 10000101 00001101 00001100 10110001    Some examples of unary and γcodes. Unary codes are only shown for the smaller numbers. Commas in γcodes are for readability only and are not part of the actual codes. number unary code length offset γcode 0 0 1 10 0 0 2 110 10 0 10,0 3 1110 10 1 10,1 4 11110 110 00 110,00 9 1111111110 1110 001 1110,001 13 1110 101 1110,101 24 11110 1000 11110,1000 511 111111110 11111111 111111110,11111111 1025 11111111110 0000000001 11111111110,0000000001 for VB encoding and decoding and Table 5.4 an example of a VB-encoded postings list. 1 With VB compression, the size of the compressed index for Reuters-RCV1 is 116 MB as we verified in an experiment. This is a more than 50% reduction of the size of the uncompressed index (see Table 5.6). The idea of VB encoding can also be applied to larger or smaller units than bytes: 32-bit words, 16-bit words, and 4-bit words or nibbles. Larger words further decrease the amount of bit manipulation necessary at the cost of less effective (or no) compression. Word sizes smaller than bytes get even better compression ratios at the cost of more bit manipulation. In general, bytes offer a good compromise between compression ratio and speed of decompression. For most IR systems variable byte codes offer an excellent tradeoff between time and space. They are also simple to implement – most of the alternatives referred to in Section 5.4 are more complex. But if disk space is a scarce resource, we can achieve better compression ratios by using bit-level encodings, in particular two closely related encodings: γcodes, which we will turn to next, and δcodes (Exercise 5.9). ✄5.3.2 γcodes VB codes use an adaptive number of bytes depending on the size of the gap. Bit-level codes adapt the length of the code on the finer grained bit level. The 1. Note that the origin is 0 in the table. Because we never need to encode a docID or a gap of 0, in practice the origin is usually 1, so that 10000000 encodes 1, 10000101 encodes 6 (not 5 as in the table), and so on.  ",5.3
2121,iir,iir-2121,5.3.2 Gamma codes,"    Some examples of unary and γcodes. Unary codes are only shown for the smaller numbers. Commas in γcodes are for readability only and are not part of the actual codes. number unary code length offset γcode 0 0 1 10 0 0 2 110 10 0 10,0 3 1110 10 1 10,1 4 11110 110 00 110,00 9 1111111110 1110 001 1110,001 13 1110 101 1110,101 24 11110 1000 11110,1000 511 111111110 11111111 111111110,11111111 1025 11111111110 0000000001 11111111110,0000000001 for VB encoding and decoding and Table 5.4 an example of a VB-encoded postings list. 1 With VB compression, the size of the compressed index for Reuters-RCV1 is 116 MB as we verified in an experiment. This is a more than 50% reduction of the size of the uncompressed index (see Table 5.6). The idea of VB encoding can also be applied to larger or smaller units than bytes: 32-bit words, 16-bit words, and 4-bit words or nibbles. Larger words further decrease the amount of bit manipulation necessary at the cost of less effective (or no) compression. Word sizes smaller than bytes get even better compression ratios at the cost of more bit manipulation. In general, bytes offer a good compromise between compression ratio and speed of decompression. For most IR systems variable byte codes offer an excellent tradeoff between time and space. They are also simple to implement – most of the alternatives referred to in Section 5.4 are more complex. But if disk space is a scarce resource, we can achieve better compression ratios by using bit-level encodings, in particular two closely related encodings: γcodes, which we will turn to next, and δcodes (Exercise 5.9). ✄5.3.2 γcodes VB codes use an adaptive number of bytes depending on the size of the gap. Bit-level codes adapt the length of the code on the finer grained bit level. The 1. Note that the origin is 0 in the table. Because we never need to encode a docID or a gap of 0, in practice the origin is usually 1, so that 10000000 encodes 1, 10000101 encodes 6 (not 5 as in the table), and so on.      5.3 Postings file compression 99 simplest bit-level code is unary code. The unary code of nis a string of n1s followed by a 0 (see the first two columns of Table 5.5). Obviously, this is not a very efficient code, but it will come in handy in a moment. How efficient can a code be in principle? Assuming the 2ngaps Gwith 1≤G≤2nare all equally likely, the optimal encoding uses nbits for each G. So some gaps (G=2nin this case) cannot be encoded with fewer than log2Gbits. Our goal is to get as close to this lower bound as possible. A method that is within a factor of optimal is γencoding.γcodes implement variable-length encoding by splitting the representation of a gap G into a pair of length and offset.Offset is Gin binary, but with the leading 1 removed.2For example, for 13 (binary 1101) offset is 101. Length encodes the length of offset in unary code. For 13, the length of offset is 3 bits, which is 1110 in unary. The γcode of 13 is therefore 1110101, the concatenation of length 1110 and offset 101. The right hand column of Table 5.5 gives additional examples of γcodes. Aγcode is decoded by first reading the unary code up to the 0 that terminates it, for example, the four bits 1110 when decoding 1110101. Now we know how long the offset is: 3 bits. The offset 101 can then be read correctly and the 1 that was chopped off in encoding is prepended: 101 →1101 = 13. The length of offset is ⌊log2G⌋bits and the length of length is ⌊log2G⌋+1 bits, so the length of the entire code is 2 × ⌊log2G⌋+1 bits. γcodes are always of odd length and they are within a factor of 2 of what we claimed to be the optimal encoding length log2G. We derived this optimum from the assumption that the 2ngaps between 1 and 2nare equiprobable. But this need not be the case. In general, we do not know the probability distribution over gaps a priori. The characteristic of a discrete probability distribution3Pthat determines its coding properties (including whether a code is optimal) is its entropy H(P), which is defined as follows: H(P) = −∑ x∈X P(x)log2P(x) where Xis the set of all possible numbers we need to be able to encode (and therefore ∑x∈XP(x) = 1.0). Entropy is a measure of uncertainty as shown in Figure 5.9 for a probability distribution Pover two possible outcomes, namely, X={x1,x2}. Entropy is maximized (H(P) = 1) for P(x1) = P(x2) = 0.5 when uncertainty about which xiwill appear next is largest; and 2. We assume here that Ghas no leading 0s. If there are any, they are removed before deleting the leading 1. 3. Readers who want to review basic concepts of probability theory may want to consult Rice (2006) or Ross (2006). Note that we are interested in probability distributions over integers (gaps, frequencies, etc.), but that the coding properties of a probability distribution are independent of whether the outcomes are integers or something else.      100 5 Index compression 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 P(x1) H(P) ◮Figure 5.9 Entropy H(P)as a function of P(x1)for a sample space with two outcomes x1and x2. minimized (H(P) = 0) for P(x1) = 1, P(x2) = 0 and for P(x1) = 0, P(x2) = 1 when there is absolute certainty. It can be shown that the lower bound for the expected length E(L)of a code Lis H(P)if certain conditions hold (see the references). It can further be shown that for 1 &lt;H(P)&lt;∞,γencoding is within a factor of 3 of this optimal encoding, approaching 2 for large H(P): E(Lγ) H(P)≤2+1 H(P)≤3. What is remarkable about this result is that it holds for any probability distribution P. So without knowing anything about the properties of the distribution of gaps, we can apply γcodes and be certain that they are within a factor of ≈2 of the optimal code for distributions of large entropy. A code like γ code with the property of being within a factor of optimal for an arbitrary distribution Pis called universal. In addition to universality, γcodes have two other properties that are useful for index compression. First, they are prefix free, namely, no γcode is the prefix of another. This means that there is always a unique decoding of a sequence of γcodes – and we do not need delimiters between them, which would decrease the efficiency of the code. The second property is that γ codes are parameter free. For many other efficient codes, we have to fit the parameters of a model (e.g., the binomial distribution) to the distribution      5.3 Postings file compression 101 of gaps in the index. This complicates the implementation of compression and decompression. For instance, the parameters need to be stored and retrieved. And in dynamic indexing, the distribution of gaps can change, so that the original parameters are no longer appropriate. These problems are avoided with a parameter-free code. How much compression of the inverted index do γcodes achieve? To answer this question we use Zipf’s law, the term distribution model introduced in Section 5.1.2. According to Zipf’s law, the collection frequency cfi is proportional to the inverse of the rank i, that is, there is a constant c′such that: cfi=c′ i. (5.3) We can choose a different constant csuch that the fractions c/iare relative frequencies and sum to 1 (that is, c/i=cfi/T): 1= M ∑ i=1 c i=c M ∑ i=1 1 i=c HM (5.4) c=1 HM (5.5) where Mis the number of distinct terms and HMis the Mth harmonic number. 4Reuters-RCV1 has M=400,000 distinct terms and HM≈ln M, so we have c=1 HM≈1 ln M=1 ln 400,000 ≈1 13. Thus the ith term has a relative frequency of roughly 1/(13i), and the expected average number of occurrences of term iin a document of length L is: Lc i≈200 ×1 13 i≈15 i where we interpret the relative frequency as a term occurrence probability. Recall that 200 is the average number of tokens per document in ReutersRCV1 (Table 4.2). Now we have derived term statistics that characterize the distribution of terms in the collection and, by extension, the distribution of gaps in the postings lists. From these statistics, we can calculate the space requirements for an inverted index compressed with γencoding. We first stratify the vocabulary into blocks of size Lc =15. On average, term ioccurs 15/itimes per 4. Note that, unfortunately, the conventional symbol for both entropy and harmonic number is H. Context should make clear which is meant in this chapter.      102 5 Index compression Ndocuments Lc most frequent Ngaps of 1 each terms Lc next most frequent N/2 gaps of 2 each terms Lc next most frequent N/3 gaps of 3 each terms ... ... ◮Figure 5.10 Stratification of terms for estimating the size of a γencoded inverted index. document. So the average number of occurrences fper document is 1 ≤ffor terms in the first block, corresponding to a total number of Ngaps per term. The average is 1 2≤f&lt;1 for terms in the second block, corresponding to N/2 gaps per term, and 1 3≤f&lt;1 2for terms in the third block, corresponding to N/3 gaps per term, and so on. (We take the lower bound because it simplifies subsequent calculations. As we will see, the final estimate is too pessimistic, even with this assumption.) We will make the somewhat unrealistic assumption that all gaps for a given term have the same size as shown in Figure 5.10. Assuming such a uniform distribution of gaps, we then have gaps of size 1 in block 1, gaps of size 2 in block 2, and so on. Encoding the N/jgaps of size jwith γcodes, the number of bits needed for the postings list of a term in the jth block (corresponding to one row in the figure) is: bits-per-row =N j×(2× ⌊log2j⌋+1) ≈2Nlog2j j. To encode the entire block, we need (Lc)·(2Nlog2j)/jbits. There are M/(Lc) blocks, so the postings file as a whole will take up: M Lc ∑ j=1 2NLc log2j j. (5.6)      5.3 Postings file compression 103 ◮Table 5.6 Index and dictionary compression for Reuters-RCV1. The compression ratio depends on the proportion of actual text in the collection. Reuters-RCV1 contains a large amount of XML markup. Using the two best compression schemes, γ encoding and blocking with front coding, the ratio compressed index to collection size is therefore especially small for Reuters-RCV1: (101 +5.9)/3600 ≈0.03. data structure size in MB dictionary, fixed-width 11.2 dictionary, term pointers into string 7.6 ∼, with blocking, k=4 7.1 ∼, with blocking &amp; front coding 5.9 collection (text, xml markup etc) 3600.0 collection (text) 960.0 term incidence matrix 40,000.0 postings, uncompressed (32-bit words) 400.0 postings, uncompressed (20 bits) 250.0 postings, variable byte encoded 116.0 postings, γencoded 101.0 For Reuters-RCV1, M Lc ≈400,000/15 ≈27,000 and 27,000 ∑ j=1 2×106×15 log2j j≈224 MB. (5.7) So the postings file of the compressed inverted index for our 960 MB collection has a size of 224 MB, one fourth the size of the original collection. When we run γcompression on Reuters-RCV1, the actual size of the compressed index is even lower: 101 MB, a bit more than one tenth of the size of the collection. The reason for the discrepancy between predicted and actual value is that (i) Zipf’s law is not a very good approximation of the actual distribution of term frequencies for Reuters-RCV1 and (ii) gaps are not uniform. The Zipf model predicts an index size of 251 MB for the unrounded numbers from Table 4.2. If term frequencies are generated from the Zipf model and a compressed index is created for these artificial terms, then the compressed size is 254 MB. So to the extent that the assumptions about the distribution of term frequencies are accurate, the predictions of the model are correct. Table 5.6 summarizes the compression techniques covered in this chapter. The term incidence matrix (Figure 1.1, page 4) for Reuters-RCV1 has size 400,000 ×800,000 =40 ×8×109bits or 40 GB. γcodes achieve great compression ratios – about 15% better than variable byte codes for Reuters-RCV1. But they are expensive to decode. This is because many bit-level operations – shifts and masks – are necessary to decode a sequence of γcodes as the boundaries between codes will usually be     104 5 Index compression somewhere in the middle of a machine word. As a result, query processing is more expensive for γcodes than for variable byte codes. Whether we choose variable byte or γencoding depends on the characteristics of an application, for example, on the relative weights we give to conserving disk space versus maximizing query response time. The compression ratio for the index in Table 5.6 is about 25%: 400 MB (uncompressed, each posting stored as a 32-bit word) versus 101 MB (γ) and 116 MB (VB). This shows that both γand VB codes meet the objectives we stated in the beginning of the chapter. Index compression substantially improves time and space efficiency of indexes by reducing the amount of disk space needed, increasing the amount of information that can be kept in the cache, and speeding up data transfers from disk to memory. ?Exercise 5.4 [⋆] Compute variable byte codes for the numbers in Tables 5.3 and 5.5. Exercise 5.5 [⋆] Compute variable byte and γcodes for the postings list h777, 17743, 294068, 31251336i. Use gaps instead of docIDs where possible. Write binary codes in 8-bit blocks. Exercise 5.6 Consider the postings list h4, 10, 11, 12, 15, 62, 63, 265, 268, 270, 400iwith a corresponding list of gaps h4, 6, 1, 1, 3, 47, 1, 202, 3, 2, 130i. Assume that the length of the postings list is stored separately, so the system knows when a postings list is complete. Using variable byte encoding: (i) What is the largest gap you can encode in 1 byte? (ii) What is the largest gap you can encode in 2 bytes? (iii) How many bytes will the above postings list require under this encoding? (Count only space for encoding the sequence of numbers.) Exercise 5.7 A little trick is to notice that a gap cannot be of length 0 and that the stuff left to encode after shifting cannot be 0. Based on these observations: (i) Suggest a modification to variable byte encoding that allows you to encode slightly larger gaps in the same amount of space. (ii) What is the largest gap you can encode in 1 byte? (iii) What is the largest gap you can encode in 2 bytes? (iv) How many bytes will the postings list in Exercise 5.6 require under this encoding? (Count only space for encoding the sequence of numbers.) Exercise 5.8 [⋆] From the following sequence of γ-coded gaps, reconstruct first the gap sequence and then the postings sequence: 1110001110101011111101101111011. Exercise 5.9 γcodes are relatively inefficient for large numbers (e.g., 1025 in Table 5.5) as they encode the length of the offset in inefficient unary code. δcodes differ from γcodes in that they encode the first part of the code (length) in γcode instead of unary code. The encoding of offset is the same. For example, the δcode of 7 is 10,0,11 (again, we add commas for readability). 10,0 is the γcode for length (2 in this case) and the encoding of offset (11) is unchanged. (i) Compute the δcodes for the other numbers       ",5.3
2122,iir,iir-2122,5.4 References and further reading,"    5.3 Postings file compression 95 ◮Table 5.2 Dictionary compression for Reuters-RCV1. data structure size in MB dictionary, fixed-width 11.2 dictionary, term pointers into string 7.6 ∼, with blocking, k=4 7.1 ∼, with blocking &amp; front coding 5.9 is identified for a subsequence of the term list and then referred to with a special character. In the case of Reuters, front coding saves another 1.2 MB, as we found in an experiment. Other schemes with even greater compression rely on minimal perfect hashing, that is, a hash function that maps Mterms onto [1, . . . , M] without collisions. However, we cannot adapt perfect hashes incrementally because each new term causes a collision and therefore requires the creation of a new perfect hash function. Therefore, they cannot be used in a dynamic environment. Even with the best compression scheme, it may not be feasible to store the entire dictionary in main memory for very large text collections and for hardware with limited memory. If we have to partition the dictionary onto pages that are stored on disk, then we can index the first term of each page using a B-tree. For processing most queries, the search system has to go to disk anyway to fetch the postings. One additional seek for retrieving the term’s dictionary page from disk is a significant, but tolerable increase in the time it takes to process a query. Table 5.2 summarizes the compression achieved by the four dictionary data structures. ?Exercise 5.2 Estimate the space usage of the Reuters-RCV1 dictionary with blocks of size k=8 and k=16 in blocked dictionary storage. Exercise 5.3 Estimate the time needed for term lookup in the compressed dictionary of ReutersRCV1 with block sizes of k=4 (Figure 5.6, b), k=8, and k=16. What is the slowdown compared with k=1 (Figure 5.6, a)? 5.3 Postings file compression Recall from Table 4.2 (page 70) that Reuters-RCV1 has 800,000 documents, 200 tokens per document, six characters per token, and 100,000,000 postings where we define a posting in this chapter as a docID in a postings list, that is, excluding frequency and position information. These numbers     106 5 Index compression Subsection 5.3.1 is based on (Scholer et al. 2002). The authors find that variable byte codes process queries two times faster than either bit-level compressed indexes or uncompressed indexes with a 30% penalty in compression ratio compared with the best bit-level compression method. They also show that compressed indexes can be superior to uncompressed indexes not only in disk usage, but also in query processing speed. Compared with VB codes, “variable nibble” codes showed 5% to 10% better compression and up to one third worse effectiveness in one experiment (Anh and Moffat 2005). Trotman (2003) also recommends using VB codes unless disk space is at a premium. In recent work, Anh and Moffat (2005;2006a) and Zukowski et al. (2006) have constructed word-aligned binary codes that are both faster in decompression and at least as efficient as VB codes. Zhang et al. (2007) investigate the increased effectiveness of caching when a number of different compression techniques for postings lists are used on modern hardware. δcodes (Exercise 5.9) and γcodes were introduced by Elias (1975), who proved that both codes are universal. In addition, δcodes are asymptotically optimal for H(P)→∞.δcodes perform better than γcodes if large numbers (greater than 15) dominate. A good introduction to information theory, including the concept of entropy, is (Cover and Thomas 1991). While Elias codes are only asymptotically optimal, arithmetic codes (Witten et al. 1999, Section 2.4) can be constructed to be arbitrarily close to the optimum H(P) for any P. Several additional index compression techniques are covered by Witten et al. (1999; Sections 3.3 and 3.4 and Chapter 5). They recommend using parameterized codes for index compression, codes that explicitly model the probability distribution of gaps for each term. For example, they show that Golomb codes achieve better compression ratios than γcodes for large collections. Moffat and Zobel (1992) compare several parameterized methods, including LLRUN (Fraenkel and Klein 1985). The distribution of gaps in a postings list depends on the assignment of docIDs to documents. A number of researchers have looked into assigning docIDs in a way that is conducive to the efficient compression of gap sequences (Moffat and Stuiver 1996;Blandford and Blelloch 2002; Silvestri et al. 2004;Blanco and Barreiro 2006;Silvestri 2007). These techniques assign docIDs in a small range to documents in a cluster where a cluster can consist of all documents in a given time period, on a particular web site, or sharing another property. As a result, when a sequence of documents from a cluster occurs in a postings list, their gaps are small and can be more effectively compressed. Different considerations apply to the compression of term frequencies and word positions than to the compression of docIDs in postings lists. See Scholer et al. (2002) and Zobel and Moffat (2006). Zobel and Moffat (2006) is recommended in general as an in-depth and up-to-date tutorial on inverted   5.4 References and further reading 107 indexes, including index compression. This chapter only looks at index compression for Boolean retrieval. For ranked retrieval (Chapter 6), it is advantageous to order postings according to term frequency instead of docID. During query processing, the scanning of many postings lists can then be terminated early because smaller weights do not change the ranking of the highest ranked kdocuments found so far. It is not a good idea to precompute and store weights in the index (as opposed to frequencies) because they cannot be compressed as well as integers (see Section 7.1.5, page 140). Document compression can also be important in an efficient information retrieval system. de Moura et al. (2000) and Brisaboa et al. (2007) describe compression schemes that allow direct searching of terms and phrases in the compressed text, which is infeasible with standard text compression utilities like gzip and compress. ?Exercise 5.14 [⋆] We have defined unary codes as being “10”: sequences of 1s terminated by a 0. Interchanging the roles of 0s and 1s yields an equivalent “01” unary code. When this 01 unary code is used, the construction of a γcode can be stated as follows: (1) Write Gdown in binary using b=⌊log2j⌋+1 bits. (2) Prepend (b−1)0s. (i) Encode the numbers in Table 5.5 in this alternative γcode. (ii) Show that this method produces a well-defined alternative γcode in the sense that it has the same length and can be uniquely decoded. Exercise 5.15 [⋆ ⋆ ⋆] Unary code is not a universal code in the sense defined above. However, there exists a distribution over gaps for which unary code is optimal. Which distribution is this? Exercise 5.16 Give some examples of terms that violate the assumption that gaps all have the same size (which we made when estimating the space requirements of a γ-encoded index). What are general characteristics of these terms? Exercise 5.17 Consider a term whose postings list has size n, say, n=10,000. Compare the size of the γ-compressed gap-encoded postings list if the distribution of the term is uniform (i.e., all gaps have the same size) versus its size when the distribution is not uniform. Which compressed postings list is smaller? Exercise 5.18 Work out the sum in Equation (5.7) and show it adds up to about 251 MB. Use the numbers in Table 4.2, but do not round Lc,c, and the number of vocabulary blocks.      ",5.3
2159,iir,iir-2159,"6 Scoring, term weighting and the vector space model","        Thus far we have dealt with indexes that support Boolean queries: a document either matches or does not match a query. In the case of large document collections, the resulting number of matching documents can far exceed the number a human user could possibly sift through. Accordingly, it is essential for a search engine to rank-order the documents matching a query. To do this, the search engine computes, for each matching document, a score with respect to the query at hand. In this chapter we initiate the study of assigning a score to a (query, document) pair. This chapter consists of three main ideas. 1\. We introduce parametric and zone indexes in Section 6.1, which serve two purposes. First, they allow us to index and retrieve documents by metadata such as the language in which a document is written. Second, they give us a simple means for scoring (and thereby ranking) documents in response to a query. 2\. Next, in Section 6.2 we develop the idea of weighting the importance of a term in a document, based on the statistics of occurrence of the term. 3\. In Section 6.3 we show that by viewing each document as a vector of such weights, we can compute a score between a query and each document. This view is known as vector space scoring. Section 6.4 develops several variants of term-weighting for the vector space model. Chapter 7develops computational aspects of vector space scoring, and related topics. As we develop these ideas, the notion of a query will assume multiple nuances. In Section 6.1 we consider queries in which specific query terms occur in specified regions of a matching document. Beginning Section 6.2 we will in fact relax the requirement of matching specific regions of a document; instead, we will look at so-called free text queries that simply consist of query terms with no specification on their relative order, importance or where in a document they should be found. The bulk of our study of scoring will be in this latter notion of a query being such a set of terms.   6 Scoring, term weighting and the vector space model 6.1 Parametric and zone indexes We have t ",6.1
2160,iir,iir-2160,6.1 Parametric and zone indexes,"  We have thus far viewed a document as a sequence of terms. In fact, most documents have additional structure. Digital documents generally encode, in machine-recognizable form, certain metadata associated with each document. By metadata, we mean specific forms of data about a document, such as its author(s), title and date of publication. This metadata would generally include fields such as the date of creation and the format of the document, as well the author and possibly the title of the document. The possible values of a field should be thought of as finite – for instance, the set of all dates of authorship. Consider queries of the form “find documents authored by William Shakespeare in 1601, containing the phrase alas poor Yorick”. Query processing then consists as usual of postings intersections, except that we may merge postings from standard inverted as well as parametric indexes. There is one parametric index for each field (say, date of creation); it allows us to select only the documents matching a date specified in the query. Figure 6.1 illustrates the user’s view of such a parametric search. Some of the fields may assume ordered values, such as dates; in the example query above, the year 1601 is one such field value. The search engine may support querying ranges on such ordered values; to this end, a structure like a B-tree may be used for the field’s dictionary. Zones are similar to fields, except the contents of a zone can be arbitrary free text. Whereas a field may take on a relatively small set of values, a zone can be thought of as an arbitrary, unbounded amount of text. For instance, document titles and abstracts are generally treated as zones. We may build a separate inverted index for each zone of a document, to support queries such as “find documents with merchant in the title and william in the author list and the phrase gentle rain in the body”. This has the effect of building an index that looks like Figure 6.2. Whereas the dictionary for a parametric index comes from a fixed vocabulary (the set of languages, or the set of dates), the dictionary for a zone index must structure whatever vocabulary stems from the text of that zone. In fact, we can reduce the size of the dictionary by encoding the zone in which a term occurs in the postings. In Figure 6.3 for instance, we show how occurrences of william in the title and author zones of various documents are encoded. Such an encoding is useful when the size of the dictionary is a concern (because we require the dictionary to fit in main memory). But there is another important reason why the encoding of Figure 6.3 is useful: the efficient computation of scores using a technique we will call weighted zone scoring.   6.1 Parametric and zone indexes 111 ◮Figure 6.1 Parametric search. In this example we have a collection with fields allowing us to select publications by zones such as Author and fields such as Language. william.author 2358 william.title 248 16 william.abstract 11 121 1441 1729 \---\---\---◮Figure 6.2 Basic zone index ; zones are encoded as extensions of dictionary entries. william 2.author,2.title 3.author 4.title 5.author - \- - ◮Figure 6.3 Zone index in which the zone is encoded in the postings rather than the dictionary.     ",6.1
2161,iir,iir-2161,6.1.1 Weighted zone scoring,"Thus far in Section 6.1 we have focused on retrieving documents based on Boolean queries on fields and zones. We now turn to a second application of zones and fields. Given a Boolean query q and a document d, weighted zone scoring assigns to the pair (q,d)a score in the interval [0, 1], by computing a linear combination of zone scores, where each zone of the document contributes a Boolean value. More specifically, consider a set of documents each of which has ℓ zones. Let g1, . . . , gℓ∈[0, 1]such that ∑ℓ i=1gi=1. For 1 ≤i≤ℓ, let sibe the Boolean score denoting a match (or absence thereof) between qand the ith zone. For instance, the Boolean score from a zone could be 1 if all the query term(s) occur in that zone, and zero otherwise; indeed, it could be any Boolean function that maps the presence of query terms in a zone to 0, 1. Then, the weighted zone score is defined to be ℓ ∑ i=1 gisi. (6.1) Weighted zone scoring is sometimes referred to also as ranked Boolean retrieval. ✎Example 6.1: Consider the query shakespeare in a collection in which each document has three zones: author, title and body. The Boolean score function for a zone takes on the value 1 if the query term shakespeare is present in the zone, and zero otherwise. Weighted zone scoring in such a collection would require three weights g1,g2and g3, respectively corresponding to the author, title and body zones. Suppose we set g1=0.2, g2=0.3 and g3=0.5 (so that the three weights add up to 1); this corresponds to an application in which a match in the author zone is least important to the overall score, the title zone somewhat more, and the body contributes even more. Thus if the term shakespeare were to appear in the title and body zones but not the author zone of a document, the score of this document would be 0.8. How do we implement the computation of weighted zone scores? A simple approach would be to compute the score for each document in turn, adding in all the contributions from the various zones. However, we now show how we may compute weighted zone scores directly from inverted indexes. The algorithm of Figure 6.4 treats the case when the query qis a twoterm query consisting of query terms q1and q2, and the Boolean function is AND: 1 if both query terms are present in a zone and 0 otherwise. Following the description of the algorithm, we describe the extension to more complex queries and Boolean functions. The reader may have noticed the close similarity between this algorithm and that in Figure 1.6. Indeed, they represent the same postings traversal, except that instead of merely adding a document to the set of results for      6.1 Parametric and zone indexes 113 ZONESCORE(q1,q2) 1 float scores[N] = [0] 2 constant g[ℓ] 3p1←postings(q1) 4p2←postings(q2) 5 // scores[] is an array with a score entry for each document, initialized to zero. 6 //p1and p2are initialized to point to the beginning of their respective postings. 7 //Assume g[] is initialized to the respective zone weights. 8while p16=NIL and p26=NIL 9do if docID(p1) = docID(p2) 10 then scores[docID(p1)] WEIGHTEDZONE(p1,p2,g) 11 p1←next(p1) 12 p2←next(p2) 13 else if docID(p1)&lt;docID(p2) 14 then p1←next(p1) 15 else p2←next(p2) 16 return scores ◮Figure 6.4 Algorithm for computing the weighted zone score from two postings lists. Function WEIGHTEDZONE (not shown here) is assumed to compute the inner loop of Equation 6.1. a Boolean AND query, we now compute a score for each such document. Some literature refers to the array scores[] above as a set of accumulators. The reason for this will be clear as we consider more complex Boolean functions than the AND; thus we may assign a non-zero score to a document even if it does not contain all query terms.  ",6.1
2162,iir,iir-2162,6.1.2 Learning weights," How do we determine the weights gifor weighted zone scoring? These weights could be specified by an expert (or, in principle, the user); but increasingly, these weights are “learned” using training examples that have been judged editorially. This latter methodology falls under a general class of approaches to scoring and ranking in information retrieval, known as machine-learned relevance. We provide a brief introduction to this topic here because weighted zone scoring presents a clean setting for introducing it; a complete development demands an understanding of machine learning and is deferred to Chapter 15. 1\. We are provided with a set of training examples, each of which is a tuple consisting of a query qand a document d, together with a relevance. Scoring, term weighting and the vector space model judgment for don q. In the simplest form, each relevance judgments is either Relevant or Non-relevant. More sophisticated implementations of the methodology make use of more nuanced judgments. 2\. The weights giare then “learned” from these examples, in order that the learned scores approximate the relevance judgments in the training examples. For weighted zone scoring, the process may be viewed as learning a linear function of the Boolean match scores contributed by the various zones. The expensive component of this methodology is the labor-intensive assembly of user-generated relevance judgments from which to learn the weights, especially in a collection that changes frequently (such as the Web). We now detail a simple example that illustrates how we can reduce the problem of learning the weights gito a simple optimization problem. We now consider a simple case of weighted zone scoring, where each document has a title zone and a body zone. Given a query qand a document d, we use the given Boolean match function to compute Boolean variables sT(d,q) and sB(d,q), depending on whether the title (respectively, body) zone of d matches query q. For instance, the algorithm in Figure 6.4 uses an AND of the query terms for this Boolean function. We will compute a score between 0 and 1 for each (document, query) pair using sT(d,q)and sB(d,q)by using a constant g∈[0, 1], as follows: score(d,q) = g·sT(d,q) + (1−g)sB(d,q). (6.2) We now describe how to determine the constant gfrom a set of training examples, each of which is a triple of the form Φj= (dj,qj,r(dj,qj)). In each training example, a given training document djand a given training query qj are assessed by a human editor who delivers a relevance judgment r(dj,qj) that is either Relevant or Non-relevant. This is illustrated in Figure 6.5, where seven training examples are shown. For each training example Φjwe have Boolean values sT(dj,qj)and sB(dj,qj) that we use to compute a score from (6.2) score(dj,qj) = g·sT(dj,qj) \+ (1−g)sB(dj,qj). (6.3) We now compare this computed score to the human relevance judgment for the same document-query pair (dj,qj); to this end, we will quantize each Relevant judgment as a 1 and each Non-relevant judgment as a 0. Suppose that we define the error of the scoring function with weight gas ε(g,Φj) = (r(dj,qj)−score(dj,qj))2,      6.1 Parametric and zone indexes 115 Example DocID Query sTsBJudgment Φ137 linux 1 1 Relevant Φ237 penguin 0 1 Non-relevant Φ3238 system 0 1 Relevant Φ4238 penguin 0 0 Non-relevant Φ51741 kernel 1 1 Relevant Φ62094 driver 0 1 Relevant Φ73191 driver 1 0 Non-relevant ◮Figure 6.5 An illustration of training examples. sTsBScore 0 0 0 0 1 1 −g 1 0 g 1 1 1 ◮Figure 6.6 The four possible combinations of sTand sB. where we have quantized the editorial relevance judgment r(dj,qj)to 0 or 1. Then, the total error of a set of training examples is given by ∑ j ε(g,Φj). (6.4) The problem of learning the constant gfrom the given training examples then reduces to picking the value of gthat minimizes the total error in (6.4). Picking the best value of gin (6.4) in the formulation of Section 6.1.3 reduces to the problem of minimizing a quadratic function of gover the interval [0, 1]. This reduction is detailed in Section 6.1.3. ✄ ",6.1
2163,iir,iir-2163,6.1.3 The optimal weight," 6.1.3 The optimal weight g We begin by noting that for any training example Φjfor which sT(dj,qj) = 0 and sB(dj,qj) = 1, the score computed by Equation (6.2) is 1 −g. In similar fashion, we may write down the score computed by Equation (6.2) for the three other possible combinations of sT(dj,qj)and sB(dj,qj); this is summarized in Figure 6.6. Let n01r(respectively, n01n) denote the number of training examples for which sT(dj,qj) = 0 and sB(dj,qj) = 1 and the editorial judgment is Relevant (respectively, Non-relevant). Then the contribution to the total error in Equation (6.4) from training examples for which sT(dj,qj) = 0 and sB(dj,qj) = 1      116 6 Scoring, term weighting and the vector space model is [1−(1−g)]2n01r\+ [0−(1−g)]2n01n. By writing in similar fashion the error contributions from training examples of the other three combinations of values for sT(dj,qj)and sB(dj,qj)(and extending the notation in the obvious manner), the total error corresponding to Equation (6.4) is (n01r+n10n)g2\+ (n10r+n01n)(1−g)2+n00r+n11n. (6.5) By differentiating Equation (6.5) with respect to gand setting the result to zero, it follows that the optimal value of gis n10r+n01n n10r+n10n+n01r+n01n . (6.6) ?Exercise 6.1 When using weighted zone scoring, is it necessary for all zones to use the same Boolean match function? Exercise 6.2 In Example 6.1 above with weights g1=0.2, g2=0.31 and g3=0.49, what are all the distinct score values a document may get? Exercise 6.3 Rewrite the algorithm in Figure 6.4 to the case of more than two query terms. Exercise 6.4 Write pseudocode for the function WeightedZone for the case of two postings lists in Figure 6.4. Exercise 6.5 Apply Equation 6.6 to the sample training set in Figure 6.5 to estimate the best value of gfor this sample. Exercise 6.6 For the value of gestimated in Exercise 6.5, compute the weighted zone score for each (query, document) example. How do these scores relate to the relevance judgments in Figure 6.5 (quantized to 0/1)? Exercise 6.7 Why does the expression for gin (6.6) not involve training examples in which sT(dt,qt) and sB(dt,qt)have the same value?      ",6.1
2164,iir,iir-2164,6.2 Term frequency and weighting," 6.2 Term frequency and weighting 117 6.2 Term frequency and weighting Thus far, scoring has hinged on whether or not a query term is present in a zone within a document. We take the next logical step: a document or zone that mentions a query term more often has more to do with that query and therefore should receive a higher score. To motivate this, we recall the notion of a free text query introduced in Section 1.4: a query in which the terms of the query are typed freeform into the search interface, without any connecting search operators (such as Boolean operators). This query style, which is extremely popular on the web, views the query as simply a set of words. A plausible scoring mechanism then is to compute a score that is the sum, over the query terms, of the match scores between each query term and the document. Towards this end, we assign to each term in a document a weight for that term, that depends on the number of occurrences of the term in the document. We would like to compute a score between a query term tand a document d, based on the weight of tin d. The simplest approach is to assign the weight to be equal to the number of occurrences of term tin document d. This weighting scheme is referred to as term frequency and is denoted tft,d,TERM FREQUENCY with the subscripts denoting the term and the document in order. For a document d, the set of weights determined by the tf weights above (or indeed any weighting function that maps the number of occurrences of t in dto a positive real value) may be viewed as a quantitative digest of that document. In this view of a document, known in the literature as the bag of words model, the exact ordering of the terms in a document is ignored but the number of occurrences of each term is material (in contrast to Boolean retrieval). We only retain information on the number of occurrences of each term. Thus, the document “Mary is quicker than John” is, in this view, identical to the document “John is quicker than Mary”. Nevertheless, it seems intuitive that two documents with similar bag of words representations are similar in content. We will develop this intuition further in Section 6.3. Before doing so we first study the question: are all words in a document equally important? Clearly not; in Section 2.2.2 (page 27) we looked at the idea of stop words – words that we decide not to index at all, and therefore do not contribute in any way to retrieval and scoring. 6.2.1 Inverse document frequency Raw  ",6.2
2165,iir,iir-2165,6.2.1 Inverse document frequency," 6.2.1 Inverse document frequency Raw term frequency as above suffers from a critical problem: all terms are considered equally important when it comes to assessing relevancy on a query. In fact certain terms have little or no discriminating power in determining relevance. For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document. To this      118 6 Scoring, term weighting and the vector space model Word cf df try 10422 8760 insurance 10440 3997 ◮Figure 6.7 Collection frequency (cf) and document frequency (df) behave differently, as in this example from the Reuters collection. end, we introduce a mechanism for attenuating the effect of terms that occur too often in the collection to be meaningful for relevance determination. An immediate idea is to scale down the term weights of terms with high collection frequency, defined to be the total number of occurrences of a term in the collection. The idea would be to reduce the tf weight of a term by a factor that grows with its collection frequency. Instead, it is more common place to use for this purpose the document frequency dft, defined to be the number of documents in the collection that contain a term t. This is because in trying to discriminate between documents for the purpose of scoring it is better to use a document-level statistic (such as the number of documents containing a term) than to use a collection-wide statistic for the term. The reason to prefer df to cf is illustrated in Figure 6.7, where a simple example shows that collection frequency (cf) and document frequency (df) can behave rather differently. In particular, the cf values for both try and insurance are roughly equal, but their df values differ significantly. Intuitively, we want the few documents that contain insurance to get a higher boost for a query on insurance than the many documents containing try get from a query on try. How is the document frequency df of a term used to scale its weight? Denoting as usual the total number of documents in a collection by N, we define the inverse document frequency (idf) of a term tas follows: idft=log N dft. (6.7) Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low. Figure 6.8 gives an example of idf’s in the Reuters collection of 806,791 documents; in this example logarithms are to the base 10. In fact, as we will see in Exercise 6.12, the precise base of the logarithm is not material to ranking. We will give on page 227 a justification of the particular form in Equation (6.7).  ",6.2
2166,iir,iir-2166,6.2.2 Tf-idf weighting,"  We now combine the definitions of term frequency and inverse document frequency, to produce a composite weight for each term in each document.      6.2 Term frequency and weighting 119 term dftidft car 18,165 1.65 auto 6723 2.08 insurance 19,241 1.62 best 25,235 1.5 ◮Figure 6.8 Example of idf values. Here we give the idf’s of terms with various frequencies in the Reuters collection of 806,791 documents. The tf-idf weighting scheme assigns to term ta weight in document dgiven TF-IDF by tf-idft,d=tft,d×idft. (6.8) In other words, tf-idft,dassigns to term ta weight in document dthat is 1\. highest when to occurs many times within a small number of documents (thus lending high discriminating power to those documents); 2\. lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal); 3\. lowest when the term occurs in virtually all documents. At this point, we may view each document as a vector with one component corresponding to each term in the dictionary, together with a weight for each component that is given by (6.8). For dictionary terms that do not occur in a document, this weight is zero. This vector form will prove to be crucial to scoring and ranking; we will develop these ideas in Section 6.3. As a first step, we introduce the overlap score measure: the score of a document dis the sum, over all query terms, of the number of times each of the query terms occurs in d. We can refine this idea so that we add up not the number of occurrences of each query term tin d, but instead the tf-idf weight of each term in d. Score(q,d) = ∑ t∈q tf-idft,d. (6.9) In Section 6.3 we will develop a more rigorous form of Equation (6.9). ?Exercise 6.8 Why is the idf of a term always finite? Exercise 6.9 What is the idf of a term that occurs in every document? Compare this with the use of stop word lists.      120 6 Scoring, term weighting and the vector space model Doc1 Doc2 Doc3 car 27 4 24 auto 3 33 0 insurance 0 33 29 best 14 0 17 ◮Figure 6.9 Table of tf values for Exercise 6.10. Exercise 6.10 Consider the table of term frequencies for 3 documents denoted Doc1, Doc2, Doc3 in Figure 6.9. Compute the tf-idf weights for the terms car, auto, insurance, best, for each document, using the idf values from Figure 6.8. Exercise 6.11 Can the tf-idf weight of a term in a document exceed 1? Exercise 6.12 How does the base of the logarithm in (6.7) affect the score calculation in (6.9)? How does the base of the logarithm affect the relative scores of two documents on a given query? Exercise 6.13 If the logarithm in (6.7) is computed base 2, suggest a simple approximation to the idf of a term.  ",6.2
2167,iir,iir-2167,6.3 The vector space model for scoring," 6.3 The vector space model for scoring In Section 6.2 (page 117) we developed the notion of a document vector that captures the relative importance of the terms in a document. The representation of a set of documents as vectors in a common vector space is known as the vector space model and is fundamental to a host of information retrieval operations ranging from scoring documents on a query, document classification and document clustering. We first develop the basic ideas underlying vector space scoring; a pivotal step in this development is the view (Section 6.3.2) of queries as vectors in the same vector space as the document collection. 6.3.1 Dot products We denote by ~ V(d)the vector derived from document d, with one component in the vector for each dictionary term. Unless otherwise specified, the reader may assume that the components are computed using the tf-idf weighting scheme, although the particular weighting scheme is immaterial to the discussion that follows. The set of documents in a collection then may be viewed as a set of vectors in a vector space, in which there is one axis for  ",6.3
2168,iir,iir-2168,6.3.1 Dot products," 6.3.1 Dot products We denote by ~ V(d)the vector derived from document d, with one component in the vector for each dictionary term. Unless otherwise specified, the reader may assume that the components are computed using the tf-idf weighting scheme, although the particular weighting scheme is immaterial to the discussion that follows. The set of documents in a collection then may be viewed as a set of vectors in a vector space, in which there is one axis for      6.3 The vector space model for scoring 121 0 1 0 1 jealous gossip ~v(q) ~v(d1) ~v(d2) ~v(d3) θ ◮Figure 6.10 Cosine similarity illustrated. sim(d1,d2) = cos θ. each term. This representation loses the relative ordering of the terms in each document; recall our example from Section 6.2 (page 117), where we pointed out that the documents Mary is quicker than John and John is quicker than Mary are identical in such a bag of words representation. How do we quantify the similarity between two documents in this vector space? A first attempt might consider the magnitude of the vector difference between two document vectors. This measure suffers from a drawback: two documents with very similar content can have a significant vector difference simply because one is much longer than the other. Thus the relative distributions of terms may be identical in the two documents, but the absolute term frequencies of one may be far larger. To compensate for the effect of document length, the standard way of quantifying the similarity between two documents d1and d2is to compute the cosine similarity of their vector representations ~ V(d1)and ~ V(d2) sim(d1,d2) = ~ V(d1)·~ V(d2) |~ V(d1)||~ V(d2)|,(6.10) where the numerator represents the dot product (also known as the inner product) of the vectors ~ V(d1)and ~ V(d2), while the denominator is the product of their Euclidean lengths. The dot product ~x·~yof two vectors is defined as ∑M i=1xiyi. Let ~ V(d)denote the document vector for d, with Mcomponents ~ V1(d)...~ VM(d). The Euclidean length of dis defined to be q∑M i=1~ V2 i(d). The effect of the denominator of Equation (6.10) is thus to length- normalize the vectors ~ V(d1)and ~ V(d2)to unit vectors ~v(d1) = ~ V(d1)/|~ V(d1)|and      122 6 Scoring, term weighting and the vector space model Doc1 Doc2 Doc3 car 0.88 0.09 0.58 auto 0.10 0.71 0 insurance 0 0.71 0.70 best 0.46 0 0.41 ◮Figure 6.11 Euclidean normalized tf values for documents in Figure 6.9. term SaS PaP WH affection 115 58 20 jealous 10 7 11 gossip 2 0 6 ◮Figure 6.12 Term frequencies in three novels. The novels are Austen’s Sense and Sensibility, Pride and Prejudice and Brontë’s Wuthering Heights. ~v(d2) = ~ V(d2)/|~ V(d2)|. We can then rewrite (6.10) as sim(d1,d2) = ~v(d1)·~v(d2). (6.11) ✎Example 6.2: Consider the documents in Figure 6.9. We now apply Euclidean normalization to the tf values from the table, for each of the three documents in the table. The quantity q∑M i=1~ V2 i(d)has the values 30.56, 46.84 and 41.30 respectively for Doc1, Doc2 and Doc3. The resulting Euclidean normalized tf values for these documents are shown in Figure 6.11. Thus, (6.11) can be viewed as the dot product of the normalized versions of the two document vectors. This measure is the cosine of the angle θbetween the two vectors, shown in Figure 6.10. What use is the similarity measure sim(d1,d2)? Given a document d(potentially one of the diin the collection), consider searching for the documents in the collection most similar to d. Such a search is useful in a system where a user may identify a document and seek others like it – a feature available in the results lists of search engines as a more like this feature. We reduce the problem of finding the document(s) most similar to dto that of finding the diwith the highest dot products (sim values)~v(d)·~v(di). We could do this by computing the dot products between ~v(d)and each of ~v(d1), . . . ,~v(dN), then picking off the highest resulting sim values. ✎Example 6.3: Figure 6.12 shows the number of occurrences of three terms (affection, jealous and gossip) in each of the following three novels: Jane Austen’s Sense and Sensibility (SaS) and Pride and Prejudice (PaP) and Emily Brontë’s Wuthering Heights (WH).      6.3 The vector space model for scoring 123 term SaS PaP WH affection 0.996 0.993 0.847 jealous 0.087 0.120 0.466 gossip 0.017 0 0.254 ◮Figure 6.13 Term vectors for the three novels of Figure 6.12. These are based on raw term frequency only and are normalized as if these were the only terms in the collection. (Since affection and jealous occur in all three documents, their tf-idf weight would be 0 in most formulations.) Of course, there are many other terms occurring in each of these novels. In this example we represent each of these novels as a unit vector in three dimensions, corresponding to these three terms (only); we use raw term frequencies here, with no idf multiplier. The resulting weights are as shown in Figure 6.13. Now consider the cosine similarities between pairs of the resulting three- dimensional vectors. A simple computation shows that sim(~v(SAS), ~v(PAP)) is 0.999, whereas sim(~v(SAS), ~v(WH)) is 0.888; thus, the two books authored by Austen (SaS and PaP) are considerably closer to each other than to Brontë’s Wuthering Heights. In fact, the similarity between the first two is almost perfect (when restricted to the three terms we consider). Here we have considered tf weights, but we could of course use other term weight functions. Viewing a collection of Ndocuments as a collection of vectors leads to a natural view of a collection as a term-document matrix: this is an M ×N matrix whose rows represent the Mterms (dimensions) of the Ncolumns, each of which corresponds to a document. As always, the terms being indexed could be stemmed before indexing; for instance, jealous and jealousy would under stemming be considered as a single dimension. This matrix view will prove to be useful in Chapter 18.  ",6.3
2169,iir,iir-2169,6.3.2 Queries as vectors," 6.3.2 Queries as vectors There is a far more compelling reason to represent documents as vectors: we can also view a query as a vector. Consider the query q=jealous gossip. This query turns into the unit vector ~v(q) = (0, 0.707, 0.707)on the three coordinates of Figures 6.12 and 6.13. The key idea now: to assign to each document da score equal to the dot product ~v(q)·~v(d). In the example of Figure 6.13,Wuthering Heights is the top-scoring document for this query with a score of 0.509, with Pride and Prejudice a distant second with a score of 0.085, and Sense and Sensibility last with a score of 0.074. This simple example is somewhat misleading: the number of dimen      124 6 Scoring, term weighting and the vector space model sions in practice will be far larger than three: it will equal the vocabulary size M. To summarize, by viewing a query as a “bag of words”, we are able to treat it as a very short document. As a consequence, we can use the cosine similarity between the query vector and a document vector as a measure of the score of the document for that query. The resulting scores can then be used to select the top-scoring documents for a query. Thus we have score(q,d) = ~ V(q)·~ V(d) |~ V(q)||~ V(d)|.(6.12) A document may have a high cosine score for a query even if it does not contain all query terms. Note that the preceding discussion does not hinge on any specific weighting of terms in the document vector, although for the present we may think of them as either tf or tf-idf weights. In fact, a number of weighting schemes are possible for query as well as document vectors, as illustrated in Example 6.4 and developed further in Section 6.4. Computing the cosine similarities between the query vector and each document vector in the collection, sorting the resulting scores and selecting the top Kdocuments can be expensive — a single similarity computation can entail a dot product in tens of thousands of dimensions, demanding tens of thousands of arithmetic operations. In Section 7.1 we study how to use an inverted index for this purpose, followed by a series of heuristics for improving on this. ✎Example 6.4: We now consider the query best car insurance on a fictitious collection with N=1,000,000 documents where the document frequencies of auto, best, car and insurance are respectively 5000, 50000, 10000 and 1000. term query document product tf df idf wt,qtf wf wt,d auto 0 5000 2.3 0 1 1 0.41 0 best 1 50000 1.3 1.3 0 0 0 0 car 1 10000 2.0 2.0 1 1 0.41 0.82 insurance 1 1000 3.0 3.0 2 2 0.82 2.46 In this example the weight of a term in the query is simply the idf (and zero for a term not in the query, such as auto); this is reflected in the column header wt,q(the entry for auto is zero because the query does not contain the termauto). For documents, we use tf weighting with no use of idf but with Euclidean normalization. The former is shown under the column headed wf, while the latter is shown under the column headed wt,d. Invoking (6.9) now gives a net score of 0 +0+0.82 +2.46 =3.28.  ",6.3
2170,iir,iir-2170,6.3.3 Computing vector scores," 6.3.3 Computing vector scores In a typical setting we have a collection of documents each represented by a vector, a free text query represented by a vector, and a positive integer K. We      6.3 The vector space model for scoring 125 COSINESCORE(q) 1 float Scores[N] = 0 2 Initialize Length[N] 3for each query term t 4do calculate wt,qand fetch postings list for t 5for each pair(d, tft,d)in postings list 6do Scores[d] += wft,d×wt,q 7 Read the array Length[d] 8for each d 9do Scores[d] = Scores[d]/Length[d] 10 return Top Kcomponents of Scores[] ◮Figure 6.14 The basic algorithm for computing vector space scores. seek the Kdocuments of the collection with the highest vector space scores on the given query. We now initiate the study of determining the Kdocuments with the highest vector space scores for a query. Typically, we seek these Ktop documents in ordered by decreasing score; for instance many search engines use K=10 to retrieve and rank-order the first page of the ten best results. Here we give the basic algorithm for this computation; we develop a fuller treatment of efficient techniques and approximations in Chapter 7. Figure 6.14 gives the basic algorithm for computing vector space scores. The array Length holds the lengths (normalization factors) for each of the N documents, whereas the array Scores holds the scores for each of the documents. When the scores are finally computed in Step 9, all that remains in Step 10 is to pick off the Kdocuments with the highest scores. The outermost loop beginning Step 3 repeats the updating of Scores, iterating over each query term tin turn. In Step 5 we calculate the weight in the query vector for term t. Steps 6-8 update the score of each document by adding in the contribution from term t. This process of adding in contributions one query term at a time is sometimes known as term-at-a-time scoring or accumulation, and the Nelements of the array Scores are therefore known as accumulators. For this purpose, it would appear necessary to store, with each postings entry, the weight wft,dof term tin document d(we have thus far used either tf or tf-idf for this weight, but leave open the possibility of other functions to be developed in Section 6.4). In fact this is wasteful, since storing this weight may require a floating point number. Two ideas help alleviate this space problem. First, if we are using inverse document frequency, we need not precompute idft; it suffices to store N/dftat the head of the postings for t. Second, we store the term frequency tft,dfor each postings entry. Finally, Step 12 extracts the top Kscores – this requires a priority queue     126 6 Scoring, term weighting and the vector space model data structure, often implemented using a heap. Such a heap takes no more than 2Ncomparisons to construct, following which each of the Ktop scores can be extracted from the heap at a cost of O(log N)comparisons. Note that the general algorithm of Figure 6.14 does not prescribe a specific implementation of how we traverse the postings lists of the various query terms; we may traverse them one term at a time as in the loop beginning at Step 3, or we could in fact traverse them concurrently as in Figure 1.6. In such a concurrent postings traversal we compute the scores of one document at a time, so that it is sometimes called document-at-a-time scoring. We will say more about this in Section 7.1.5. ?Exercise 6.14 If we were to stem jealous and jealousy to a common stem before setting up the vector space, detail how the definitions of tf and idf should be modified. Exercise 6.15 Recall the tf-idf weights computed in Exercise 6.10. Compute the Euclidean normalized document vectors for each of the documents, where each vector has four components, one for each of the four terms. Exercise 6.16 Verify that the sum of the squares of the components of each of the document vectors in Exercise 6.15 is 1 (to within rounding error). Why is this the case? Exercise 6.17 With term weights as computed in Exercise 6.15, rank the three documents by computed score for the query car insurance, for each of the following cases of term weighting in the query: 1\. The weight of a term is 1 if present in the query, 0 otherwise. 2\. Euclidean normalized idf.  ",6.3
2171,iir,iir-2171,6.4 Variant tf-idf functions," 6.4 Variant tf-idf functions. For assigning a weight for each term in each document, a number of alternatives to tf and tf-idf have been considered. We discuss some of the principal ones here; a more complete development is deferred to Chapter 11. We will summarize these alternatives in Section 6.4.3 (page 128). 6.4.1 Sublinear tf scaling It seems unlikely that twenty occurrences of a term in a document truly carry twenty times the significance of a single occurrence. Accordingly, there has been considerable research into variants of term frequency that go beyond counting the number of occurrences of a term. A common modification is  ",6.4
2172,iir,iir-2172,6.4.1 Sublinear tf scaling," 6.4.1 Sublinear tf scaling. It seems unlikely that twenty occurrences of a term in a document truly carry twenty times the significance of a single occurrence. Accordingly, there has been considerable research into variants of term frequency that go beyond counting the number of occurrences of a term. A common modification is      to use instead the logarithm of the term frequency, which assigns a weight given by wft,d=1+log tft,dif tft,d&gt;0 0 otherwise . (6.13) In this form, we may replace tf by some other function wf as in (6.13), to obtain: wf-idft,d=wft,d×idft.(6.14) Equation (6.9) can then be modified by replacing tf-idf by wf-idf as defined in (6.14).  ",6.4
2173,iir,iir-2173,6.4.2 Maximum tf normalization," 6.4.2 Maximum tf normalization One well-studied technique is to normalize the tf weights of all terms occurring in a document by the maximum tf in that document. For each document d, let tfmax(d) = maxτ∈dtfτ,d, where τranges over all terms in d. Then, we compute a normalized term frequency for each term tin document dby ntft,d=a\+ (1−a)tft,d tfmax(d), (6.15) where ais a value between 0 and 1 and is generally set to 0.4, although some early work used the value 0.5. The term ain (6.15) is a smoothing term whose role is to damp the contribution of the second term – which may be viewed as a scaling down of tf by the largest tf value in d. We will encounter smoothing further in Chapter 13 when discussing classification; the basic idea is to avoid a large swing in ntft,dfrom modest changes in tft,d(say from 1 to 2). The main idea of maximum tf normalization is to mitigate the following anomaly: we observe higher term frequencies in longer documents, merely because longer documents tend to repeat the same words over and over again. To appreciate this, consider the following extreme example: supposed we were to take a document dand create a new document d′by simply appending a copy of d to itself. While d′should be no more relevant to any query than dis, the use of (6.9) would assign it twice as high a score as d. Replacing tf-idft,din (6.9) by ntf-idft,deliminates the anomaly in this example. Maximum tf normalization does suffer from the following issues: 1\. The method is unstable in the following sense: a change in the stop word list can dramatically alter term weightings (and therefore ranking). Thus, it is hard to tune. 2\. A document may contain an outlier term with an unusually large number of occurrences of that term, not representative of the content of that document.      128 6 Scoring, term weighting and the vector space model Term frequency. Document frequency Normalization n (natural) tft,dn (no) 1 n (none) 1 l (logarithm) 1 +log(tft,d)t (idf) log N dftc (cosine) 1 √w2 1+w2 2+...+w2 M a (augmented) 0.5 +0.5×tft,d maxt(tft,d)p (prob idf) max{0, log N−dft dft}u (pivoted unique) 1/u(Section 6.4.4) b (boolean) 1 if tft,d&gt;0 0 otherwise b (byte size) 1/CharLengthα,α&lt;1 L (log ave) 1+log(tft,d) 1+log(avet∈d(tft,d)) ◮Figure 6.15 SMART notation for tf-idf variants. Here CharLength is the number of characters in the document. 3\. More generally, a document in which the most frequent term appears roughly as often as many other terms should be treated differently from one with a more skewed distribution.  ",6.4
2174,iir,iir-2174,6.4.3 Document and query weighting schemes," 6.4.3 Document and query weighting schemes Equation (6.12) is fundamental to information retrieval systems that use any form of vector space scoring. Variations from one vector space scoring method to another hinge on the specific choices of weights in the vectors ~ V(d)and ~ V(q). Figure 6.15 lists some of the principal weighting schemes in use for each of ~ V(d)and ~ V(q), together with a mnemonic for representing a specific combination of weights; this system of mnemonics is sometimes called SMART notation, following the authors of an early text retrieval system. The mnemonic for representing a combination of weights takes the form ddd.qqq where the first triplet gives the term weighting of the document vector, while the second triplet gives the weighting in the query vector. The first letter in each triplet specifies the term frequency component of the weighting, the second the document frequency component, and the third the form of normalization used. It is quite common to apply different normalization functions to ~ V(d)and ~ V(q). For example, a very standard weighting scheme is lnc.ltc, where the document vector has log-weighted term frequency, no idf (for both effectiveness and efficiency reasons), and cosine normalization, while the query vector uses log-weighted term frequency, idf weighting, and cosine normalization.     6.4 Variant tf-idf functions 129 ✄ ",6.4
2175,iir,iir-2175,6.4.4 Pivoted normalized document length," 6.4.4 Pivoted normalized document length In Section 6.3.1 we normalized each document vector by the Euclidean length of the vector, so that all document vectors turned into unit vectors. In doing so, we eliminated all information on the length of the original document; this masks some subtleties about longer documents. First, longer documents will – as a result of containing more terms – have higher tf values. Second, longer documents contain more distinct terms. These factors can conspire to raise the scores of longer documents, which (at least for some information needs) is unnatural. Longer documents can broadly be lumped into two categories: (1) verbose documents that essentially repeat the same content – in these, the length of the document does not alter the relative weights of different terms; (2) documents covering multiple different topics, in which the search terms probably match small segments of the document but not all of it – in this case, the relative weights of terms are quite different from a single short document that matches the query terms. Compensating for this phenomenon is a form of document length normalization that is independent of term and document frequencies. To this end, we introduce a form of normalizing the vector representations of documents in the collection, so that the resulting “normalized” documents are not necessarily of unit length. Then, when we compute the dot product score between a (unit) query vector and such a normalized document, the score is skewed to account for the effect of document length on relevance. This form of compensation for document length is known as pivoted document length normalization. Consider a document collection together with an ensemble of queries for that collection. Suppose that we were given, for each query qand for each document d, a Boolean judgment of whether or not dis relevant to the query q; in Chapter 8 we will see how to procure such a set of relevance judgments for a query ensemble and a document collection. Given this set of relevance judgments, we may compute a probability of relevance as a function of document length, averaged over all queries in the ensemble. The resulting plot may look like the curve drawn in thick lines in Figure 6.16. To compute this curve, we bucket documents by length and compute the fraction of relevant documents in each bucket, then plot this fraction against the median document length of each bucket. (Thus even though the “curve” in Figure 6.16 appears to be continuous, it is in fact a histogram of discrete buckets of document length.) On the other hand, the curve in thin lines shows what might happen with the same documents and query ensemble if we were to use relevance as prescribed by cosine normalization Equation (6.12) – thus, cosine normalization has a tendency to distort the computed relevance vis-à-vis the true relevance, at the expense of longer documents. The thin and thick curves crossover at a point corresponding to document length ℓp, which we refer to as the pivot term weighting and the vector space model Document length Relevance     ℓp p    6 ◮Figure 6.16 Pivoted document length normalization. length; dashed lines mark this point on the x−and y−axes. The idea of pivoted document length normalization would then be to “rotate” the cosine normalization curve counter-clockwise about pso that it more closely matches thick line representing the relevance vs. document length curve. As mentioned at the beginning of this section, we do so by using in Equation (6.12) a normalization factor for each document vector ~ V(d)that is not the Euclidean length of that vector, but instead one that is larger than the Euclidean length for documents of length less than ℓp, and smaller for longer documents. To this end, we first note that the normalizing term for ~ V(d)in the denominator of Equation (6.12) is its Euclidean length, denoted |~ V(d)|. In the simplest implementation of pivoted document length normalization, we use a normalization factor in the denominator that is linear in |~ V(d)|, but one of slope &lt;1 as in Figure 6.17. In this figure, the x−axis represents |~ V(d)|, while the y−axis represents possible normalization factors we can use. The thin line y=xdepicts the use of cosine normalization. Notice the following aspects of the thick line representing pivoted length normalization: 1\. It is linear in the document length and has the form a|~ V(d)|\+ (1−a)piv, (6.16)      6.4 Variant tf-idf functions 131 |~ V(d)| Pivoted normalization y=x; Cosine Pivoted  piv  6 ◮Figure 6.17 Implementing pivoted document length normalization by linear scaling. where piv is the cosine normalization value at which the two curves intersect. 2\. Its slope is a&lt;1 and (3) it crosses the y=xline at piv. It has been argued that in practice, Equation (6.16) is well approximated by aud\+ (1−a)piv, where udis the number of unique terms in document d. Of course, pivoted document length normalization is not appropriate for all applications. For instance, in a collection of answers to frequently asked questions (say, at a customer service website), relevance may have little to do with document length. In other cases the dependency may be more complex than can be accounted for by a simple linear pivoted normalization. In such cases, document length can be used as a feature in the machine learning based scoring approach of Section 6.1.2. ?Exercise 6.18 One measure of the similarity of two vectors is the Euclidean distance (or L2distance)  between them: |~x−~y|=v u u tM ∑ i=1 (xi−yi)2      132 6 Scoring, term weighting and the vector space model query document word tf wf df idf qi=wf-idf tf wf di=normalized wf qi·di digital 10,000 video 100,000 cameras 50,000 ◮Table 6.1 Cosine computation for Exercise 6.19. Given a query qand documents d1,d2, . . ., we may rank the documents diin order of increasing Euclidean distance from q. Show that if qand the diare all normalized to unit vectors, then the rank ordering produced by Euclidean distance is identical to that produced by cosine similarities. Exercise 6.19 Compute the vector space similarity between the query “digital cameras” and the document “digital cameras and video cameras” by filling out the empty columns in Table 6.1. Assume N=10,000,000, logarithmic term weighting (wf columns) for query and document, idf weighting for the query only and cosine normalization for the document only. Treat and as a stop word. Enter term counts in the tf columns. What is the final similarity score? Exercise 0 Show that for the query affection, the relative ordering of the scores of the three documents in Figure 6.13 is the reverse of the ordering of the scores for the query jealous gossip. Exercise 6.21 In turning a query into a unit vector in Figure 6.13, we assigned equal weights to each of the query terms. What other principled approaches are plausible? Exercise 6.22 Consider the case of a query term that is not in the set of Mindexed terms; thus our standard construction of the query vector results in ~ V(q)not being in the vector space created from the collection. How would one adapt the vector space representation to handle this case? Exercise 6.23 Refer to the tf and idf values for four terms and three documents in Exercise 6.10. Compute the two top scoring documents on the query best car insurance for each of the following weighing schemes: (i) nnn.atc; (ii) ntc.atc. Exercise 6.24 Suppose that the word coyote does not occur in the collection used in Exercises 6.10 and 6.23. How would one compute ntc.atc scores for the query coyote insurance?      ",6.4
2176,iir,iir-2176,6.5 References and further reading," 6.5 References and further reading 133 6.5 References and further reading Chapter 7develops the computational aspects of vector space scoring. Luhn (1957;1958) describes some of the earliest reported applications of term weighting. His paper dwells on the importance of medium frequency terms (terms that are neither too commonplace nor too rare) and may be thought of as anticipating tf-idf and related weighting schemes. Spärck Jones (1972) builds on this intuition through detailed experiments showing the use of inverse document frequency in term weighting. A series of extensions and theoretical justifications of idf are due to Salton and Buckley (1987)Robertson and Jones (1976), Croft and Harper (1979) and Papineni (2001). Robertson maintains a web page (http://www.soi.city.ac.uk/˜ser/idf.html) containing the history of idf, including soft copies of early papers that predated electronic versions of journal article. Singhal et al. (1996a) develop pivoted document length normalization. Probabilistic language models (Chapter 11) develop weighting techniques that are more nuanced than tf-idf; the reader will find this development in Section 11.4.3. We observed that by assigning a weight for each term in a document, a document may be viewed as a vector of term weights, one for each term in the collection. The SMART information retrieval system at Cornell (Salton 1971b) due to Salton and colleagues was perhaps the first to view a document as a vector of weights. The basic computation of cosine scores as described in Section 6.3.3 is due to Zobel and Moffat (2006). The two query evaluation strategies term-at-a-time and document-at-a-time are discussed by Turtle and Flood (1995). The SMART notation for tf-idf term weighting schemes in Figure 6.15 is presented in (Salton and Buckley 1988,Singhal et al. 1995;1996b). Not all versions of the notation are consistent; we most closely follow (Singhal et al. 1996b). A more detailed and exhaustive notation was developed in Moffat and Zobel (1998), considering a larger palette of schemes for term and document frequency weighting. Beyond the notation, Moffat and Zobel (1998) sought to set up a space of feasible weighting functions through which hillclimbing approaches could be used to begin with weighting schemes that performed well, then make local improvements to identify the best combinations. However, they report that such hill-climbing methods failed to lead to any conclusions on the best weighting schemes.  ",6.5
2123,mir,mir-2123,7.4 Text Compression," 7.4 Text Compression 7.4.1 Motivation Text compression is about finding ways to represent the text in fewer bits or bytes. The amount of space required to store text on computers can be reduced significantly using compression techniques. Compression methods create a reduced representation by identifying and using structures that exist in the text. From the compressed version, the original text can be reconstructed exactly. Text compression is becoming an important issue in an information retrieval environment. The widespread use of digital libraries, office automation  ",5.4
2124,mir,mir-2124,7.4.1 Motivation," 7.4.1 Motivation Text compression is about finding ways to represent the text in fewer bits or bytes. The amount of space required to store text on computers can be reduced significantly using compression techniques. Compression methods create a reduced representation by identifying and using structures that exist in the text. From the compressed version, the original text can be reconstructed exactly. Text compression is becoming an important issue in an information retrieval environment. The widespread use of digital libraries, office automation   systems, document databases, and the Web has led to an explosion of textual information available online. In this scenario, text compression appears as an attractive option for reducing costs associated with space requirements, input/output (I/O) overhead, and communication delays. The gain obtained from compressing text is that it requires less storage space, it takes less time to be transmitted over a communication link, and it takes less time to search directly the compressed text. The price paid is the time necessary to code and decode the text. A major obstacle for storing text in compressed form is the need for IR systems to access text randomly. To access a given word in a compressed text, it is usually necessary to decode the entire text from the beginning until the desired word is reached. It could be argued that a large text could be divided into blocks that are compressed independently, thus allowing fast random access to each block. However, efficient compression methods need to process some text before making compression effective (usually more than 10 kilobytes). The smaller the blocks, the less effective compression is expected to be. Our discussion here focuses on text compression methods which are suitable for use in an IR environment. For instance, a successful idea aimed at merging the requirements of compression algorithms and the needs of IR systems is to consider that the symbols to be compressed are words and not characters (character-based compression is the more conventional approach). Words are the atoms on which most IR systems are built. Moreover, it is now known that much better compression is achieved by taking words as symbols (instead of characters). Further, new word-based compression methods allow random access to words within the compressed text which is a critical issue for an IR system. Besides the economy of space obtained by a compression method, there are other important characteristics to be considered such as compression and decompression speed. In some situations, decompression speed is more important than compression speed. For instance, this is the case with textual databases in which it is common to compress the text once and to read it many times from disk. Another important characteristic of a compression method is the possibility of performing compressed pattern matching, defined as the task of performing pattern matching in a compressed text without decompressing it. In this case, sequential searching can be speeded up by compressing the search key rather than decoding the compressed text being searched. As a consequence, it is possible to search faster on compressed text because much less text has to be scanned. Chapter 8 presents efficient methods to deal with searching the compressed text directly. When the text collection is large, efficient text retrieval requires specialized index techniques. A simple and popular indexing structure for text collections are the inverted files. Inverted files (see Chapter 8 for details) are especially adequate when the pattern to be searched for is formed by simple words. Since this is a common type of query (for instance, when searching the Web), inverted files are widely used for indexing large text collections. An inverted file is typically composed of (a) a vector containing all the distinct words in the text collection (which is called the vocabulary) and (b) for    TEXT COMPRESSION 175 each word in the vocabulary, a list of all documents (identified by document numbers) in which that word occurs. Because each list of document numbers (within the inverted file) is organized in ascending order, specific compression methods have been proposed for them, leading to very efficient index compression schemes. This is important because query processing time is highly related to index access time. Thus, in this section, we also discuss some of the most important index compression techniques. We first introduce basic concepts related to text compression. We then present some of the most important statistical compression methods, followed by a brief review of compression methods based on a dictionary. At the end, we discuss the application of compression to inverted files.  ",7.4
2125,mir,mir-2125,7.4.2 Basic Concepts," 7.4.2 Basic Concepts There are two general approaches to text compression: statistical and dictionary based. Statistical methods rely on generating good probability estimates (of appearance in the text) for each symbol. The more accurate the estimates are, the better the compression obtained. A symbol here is usually a character, a text word, or a fixed number of characters. The set of all possible symbols in the text is called the alphabet. The task of estimating the probability on each next symbol is called modeling. A model is essentially a collection of probability distributions, one for each context in which a symbol can be coded. Once these probabilities are available the symbols are converted into binary digits, a process called coding. In practice, both the encoder and decoder use the same model. The decoder interprets the output of the encoder (with reference to the same model) to find out the original symbol. . There are two well known statistical coding -strategies: Huffman coding and arithmetic coding. The idea of Huffman coding is to assign a fixed-length bit encoding to each different symbol of the text. Compression is achieved by assigning a smaller number of bits to symbols with higher probabilities of appearance. Huffman coding was first proposed in the early 1950s and was the most important compression method until the late 1970s, when arithmetic coding made higher compression rates possible. Arithmetic coding computes the code incrementally, one symbol at a time, as opposed to the Huffman coding scheme in which each different symbol is pre-encoded using a fixed-length number of bits. The incremental nature does not allow decoding a string which starts in the middle of a compressed file. To decode a symbol in the middle of a file compressed with arithmetic coding, it is necessary to decode the whole text from the very beginning until the desired word is reached. This characteristic makes arithmetic coding inadequate for use in an IR environment. Dictionary methods substitute a sequence of symbols by a pointer to a previous occurrence of that sequence. The pointer representations are references to entries in a dictionary composed of a list of symbols (often called phrases) that are expected to occur frequently. Pointers to the dictionary entries are    chosen so that they need less space than the phrase they replace, thus obtaining compression. The distinction between modeling and coding does not exist in dictionary methods and there are no explicit probabilities associated to phrases. The most well known dictionary methods are represented by a family of methods, known as the Ziv-Lempel family. Character-based Huffman methods are typically able to compress English texts to approximately five bits per character (usually, each uncompressed character takes 7-8 bits to be represented). More recently, a word-based Huffman method has been proposed as a better alternative for natural language texts. This method is able to reduce English texts to just over two bits per character. As we will see later on, word-based Huffman coding achieves compression rates close to the entropy and allows random access to intermediate points in the compressed text. Ziv-Lempel methods are able to reduce English texts to fewer than four bits per character. Methods based on arithmetic coding can also compress English texts to just over two bits per character. However, the price paid is slower compression and decompression, and the impossibility of randomly accessing intermediate points in the compressed text. Before proceeding, let us present an important definition which will be useful from now on. Definition Compression ratio is the size of the compressed file as a fraction of the uncompressed file.  ",
2126,mir,mir-2126,7.4.3 Statistical Methods," 7.4.3 Statistical Methods In a statistical method, a probability is estimated for each symbol (the modeling task) and, based on this probability,a code is assigned to each symbol at a time (the coding task). Shorter codes are assigned to the most likely symbols. The relationship between probabilities and codes was established by Claude Shannon in his source code theorem [718J. He showed that, in an optimal encoding scheme, a symbol that is expected to occur with probability p should be assigned a code of length log2 ~ bits. The number of bits in which a symbol is best coded represents the information content of the symbol. The average amount of information per symbol over the whole alphabet is called the entropy of the probability distribution, and is given by: 1 E= ~Pilog2L..J Pi E is a lower bound on compression , measured in bits per symbol, which applies to any coding method based on the probability distribution Pi. It is important to note that E is calculated from the probabilities and so is a property of the model. See Chapter 6 for more details on this topic.    Modeling The basic junction of a model is to provide a probability assignment for the next symbol to be coded. High compression can be obtained by forming good models of the text that is to be coded. The probability assignment is explained in the following section. Compression models can be adaptive, static, or semi-static. Adaptive models start with no information about the text and progressively learn about its statistical distribution as the compression process goes on. Thus, adaptive models need only one pass over the text and store no additional information apart from the compressed text. For long enough texts, such models converge to the true statistical distribution of the text. One major disadvantage, however, is that decompression of a file has to start from its beginning, since information on the distribution of the data is stored incrementally inside the file. Adaptive modeling is a good option for general purpose compression programs, but an inadequate alternative for full-text retrieval where random access to compressed patterns is a must. Static models assume an average distribution for all input texts. The modeling phase is done only once for all texts to be coded in the future (i.e., somehow a probability distribution is estimated and then used for all texts to be compressed in the future). These models tend to achieve poor compression ratios when the data deviates from initial statistical assumptions. For example, a model adequate for English literary texts will probably perform poorly for financial texts containing a lot of different numbers, as each number is relatively rare and so receives long codes. Semi-static models do not assume any distribution on the data, but learn it in a first pass . In a second pass, they compress the data by using a fixed code derived from the distribution learned from the first pass. At decoding time, information on the data distribution is sent to the decoder before transmitting the encoded symbols. The disadvantages of semi-static models are that they must make two passes over the text and that information on the data distribution must be stored to be used by the decoder to decompress. In situations where interactive data communications are involved it may be impractical to make two passes over the text. However, semi-static models have a crucial advantage in IR contexts: since the same codes are used at every point in the compressed file, direct access is possible. Word-based models take words instead of characters as symbols, Usually, a word is a contiguous string of characters in the set {A .. Z, a ..z} separated by other characters not in the set {A .. Z, a ..z}. There are many good reasons to use word-based models in an IR context. First, much better compression rates are achieved by taking words as symbols because words carry a lot of meaning in natural languages and, as a result, their distribution is much more related to the semantic structure of the text than the individual letters. Second, words are the atoms on which most information retrieval systems are built. Words are already stored for indexing purposes and so might be used as part of the model for compression. Third, the word frequencies are also useful in answering queries involving combinations of words because the best strategy is to start with the    least frequent words first. Since the text is not only composed of words but also of separators, a model must also be chosen for them. There are many different ways to deal with separators. As words and separators always follow one another, two different alphabets are usually used: one for words and one for separators. Consider the following example: each rose, a rose is a rose. In the word-based model, the set of symbols of the alphabet is {a, each, is, rose}, whose frequencies are 2,1,1, and 3, respectively, and -the set of separators is {' ,U', U}, whose frequencies are 1 and 5, respectively (where U represents a space). Once it is known that the text starts with a word or a separator, there is confusion about which alphabet to use. In natural language texts, a word is followed by a single space in most cases. In the texts ofthe TREC-3 collection [342] (see Chapter 3),70-80% of the separators are single spaces. Another good alternative is to consider the single space that follows a word as part of the same word. That is, if a word is followed by a space, we can encode just the word. If not, we can encode the word and then the following separator. At decoding time, we decode a word and assume that a space follows unless the next symbol corresponds to a separator. Notice that now a single alphabet for words and separators (single space excluded) is used. For instance, in the example above, the single alphabet is {' ,U', a, each, is, rose} and there is no longer an alphabet for separators. As the alphabet excludes the single space then the words are called spaceless words. In some situations word-based models for full-text databases have a potential to generate a great quantity of different codes and care must be exercised to deal with this fact. For instance, as discussed in the section on lexical analysis (at the beginning of this chapter), one has to consider whether a sequence of digits is to be considered as a word. If it is, then a collection which contains one million documents and includes document numbers as identifiers will generate one million words composed solely of digits, each one occurring once in the collection. This can be very inefficient for any kind of compression method available. One possible good solution is to divide long numbers into shorter ones by using a null (or implicit) punctuation marker in between. This diminishes the alphabet size resulting in considerable improvements in the compression ratio and in the decoding time. Another important consideration is the size of the alphabet in word-based schemes. How large is the number of different words in a full-text database? It is empirically known that the vocabulary V of natural language texts with n words grows sublinearly. Heaps [352] shows that V = O(n.B), where (3 is a constant dependent on the particular text. For the 2 gigabyte TREC-3 collection [342], (3 is between 0.4 and 0.6 which means that the alphabet size grows roughly proportional to the square root of n. Even for this growth of the alphabet, the generalized Zipf law shows that the probability distribution is skewed so that the entropy remains constant. This implies that the compression ratio does not degrade as the text (and hence the number of different symbols) grows. Heaps' and Zipfs' laws are explained in Chapter 6\. Finally, it is important to mention that word-based Huffman methods need large texts to be effective (i.e., they are not adequate to compress and transmit    a single Web page over a network). The need to store the vocabulary represents an important space overhead when the text is small (say, less than 10 megabytes). However, this is not a concern in IR in general as the texts are large and the vocabulary is needed anyway for other purposes such as indexing and querying. Coding Coding corresponds to the task of obtaining the representation (code) of a symbol based on a probability distribution given by a model. The main goal of a coder is to assign short codes to likely symbols. and long codes to unlikely ones. As we have seen in the previous section, the entropy of a probability distribution is a lower bound on how short the average length of a code can be, and the quality of a coder is measured in terms of how close to the entropy it is able to get. Another important consideration is the speed of both the coder and the decoder. Sometimes it is necessary to sacrifice the compression ratio to reduce the time to encode and decode the text. A semi-static Huffman compression method works in two passes over the text. In a first pass, the modeler determines the probability distribution of the symbols and builds a coding tree according to this distribution. In a second pass, each next symbol is encoded according to the coding tree. Adaptive Huffman compression methods, instead, work in one single pass over the text updating the coding tree incrementally. The encoding of the symbols in the input text is also done during this single pass over the text. The main problem of adaptive Huffman methods is the cost of updating the coding tree as new symbols are read. As with Huffman-based methods, arithmetic coding methods can also be based on static, semi-static or adaptive algorithms. The main strength of arithmetic coding methods is that they can generate codes which are arbitrarily close to the entropy for any kind of probability distribution. Another strength of arithmetic coding. methods is that they do not need to store a coding tree explicitly. For adaptive algorithms, this implies that arithmetic coding uses less memory than Huffman-based coding. For static or semi-static algorithms, the use of canonical Huffman codes overcomes this memory problem (canonical Huffman trees are explained later on). In arithmetic coding, the input text is represented by an interval of real numbers between 0 and 1\. As the size of the input becomes larger, the interval becomes smaller and the number of bits needed to specify this interval increases. Compression is achieved because input symbols with higher probabilities reduce the interval less than symbols with smaller probabilities and hence add fewer bits to the output code. Arithmetic coding presents many disadvantages over Huffman coding in an IR environment. First, arithmetic coding is much slower than Huffman coding, especially with static and semi-static algorithms. Second, with arithmetic coding, decompression cannot start in the middle of a compressed file. This contrasts with Huffman coding, in which it is possible to index and to decode from    any position in the compressed text if static or semi-static algorithms are used. Third, word-based Huffman coding methods yield compression ratios as good as arithmetic coding ones. Consequently, Huffman coding is the method of choice in full-text retrieval, where both speed and random access are important. Thus, we will focus the remaining of our discussion on semi-static word-based Huffman coding. Huffman Coding Huffman coding is one of the best known compression methods [386]. The idea is to assign a variable-length encoding in bits to each symbol and encode each symbol in turn. Compression is achieved by assigning shorter codes' to more frequent symbols. Decompression uniqueness is guaranteed because no code is a prefix of another. A word-based semi-static model and Huffman coding form a good compression method for text. Figure 7.2 presents an example of compression using Huffman coding on words. In this example the set of symbols of the alphabet is {' ,U', a, each, for, is, rose}, whose frequencies are 1, 2,1,1,1, and 3, respectively. In this case the alphabet is unique for words and separators. Notice that the separator 'U' is not part of the alphabet because the single space that follows a word is considered as part of the word. These words are called spaceless words (see more about spaceless words in Section 7.4.3). The Huffman tree shown in Figure 7.2 is an example of a binary trie built on binary codes. Tries are explained in Chapter 8\. Decompression is accomplished as follows. The stream of bits in the compressed file is traversed from left to right. The sequence of bits read is used to also traverse the Huffman compression tree, starting at the root. Whenever a leaf node is reached, the corresponding word (which constitutes the decompressed symbol) is printed out and the tree traversal is restarted. Thus, according to the tree in Figure 7.2, the presence of the code 0110 in the compressed file leads to the decompressed symbol for. To build a Huffman tree, it is first necessary to obtain the symbols that constitute the alphabet and their probability distribution in the text to be compressed. The algorithm for building the tree then operates bottom up and starts Original text: for each rose, a rose is a rose  by creating for each symbol of the alphabet a node containing the symbol and its probability (or frequency). At this point there is a forest of one- node trees whose probabilities sum up to 1\. Next, the two nodes with the smallest probabilities become children of a newly created parent node. With this parent node is associated a probability equal to the sum of the probabilities of the two chosen children. The operation is repeated ignoring nodes that are already children, until there is only one node, which becomes the root of the decoding tree. By delaying the pairing of nodes with high probabilities, the algorithm necessarily places them closer to the root node, making their code smaller. The two branches from every internal node are consistently labeled 0 and 1 (or 1 and 0). Given s symbols and their frequencies in the text, the algorithm builds the Huffman tree in O(s log s) time. The number of Huffman trees which can be built for a given probability distribution is quite large. This happens because interchanging left and right subtrees of any internal node results in a different tree whenever the two subtrees are different in structure, but the weighted average code length is not affected. Instead of using any kind of tree, the preferred choice for most applications is to adopt a canonical tree which imposes a particular order to the coding bits. A Huffman tree is canonical when the height of the left subtree of any node is never smaller than that of the right subtree, and all leaves are in increasing order of probabilities from left to right. Figure 7.3 shows the canonical tree for the example of Figure 7.2. The deepest leaf at the leftmost position of the Huffman canonical tree, corresponding to one element with smallest probability, will contain only zeros, and the following codes will be in increasing order inside each level. At each change of level we shift left one bit in the counting. The table in Figure 7.3 shows the canonical codes for the example of Figure 7.2. A canonical code can be represented by an ordered sequence S of pairs (Xi, Yi), 1 ~ i ~ i, where Xi represents the number of symbols at level i, Yi represents the numerical value of the first code at level i, and i is the height of the tree. For our example in Figure 7.3, the ordered sequence is S = ((1,1), (1, 1), (0,00), (4,0)). For instance, the fourth pair (4,0) in S corresponds to the fourth level and indicates that there are four nodes at this level and that to the node most to the left is assigned a code, at this level, with value O. Since this is the fourth level, a value 0 corresponds to the codeword 0000.  elements 256 elements (b) Optimal byte tree 254 elements 256 elements 2 elements 254 empty nodes Figure 7.4 Example of byte Huffman tree. One of the properties of canonical codes is that the set of codes having the same length are the binary representations of consecutive integers. Interpreted as integers, the 4-bit codes of the table in Figure 7.3 are 0, 1, 2, and 3, the 2-bit code is 1 and the l-bit code is also 1\. In our example, if the first character read from the input stream is 1, a codeword has been identified and the corresponding symbol can be output. If this value is 0, a second bit is appended and the two bits are again interpreted as an integer and used to index the table and identify the corresponding symbol. Once we read '00' we know that the code has four bits and therefore we can read two more bits and use them as an index into the table. This fact can be exploited to enable efficient encoding and decoding with small overhead. Moreover, much less memory is required, which is especially important for large vocabularies. Byte-Oriented Huffman Code The original method proposed by Huffman [386] leads naturally to binary coding trees. In [577], however, it is proposed to build the code assigned to each symbol as a sequence of whole bytes. As a result, the Huffman tree has degree 256 instead of 2. Typically, the code assigned to each symbol contains between 1 and 5 bytes. For example, a possible code for the word rose could be the 3-byte code '47 131 8.' The construction of byte Huffman trees involves some details which must be dealt with. Care must be exercised to ensure that the first levels of the tree have no empty nodes when the code is not binary. Figure 7.4(a) illustrates a case where a naive extension of the binary Huffman tree construction algorithm might generate a non-optimal byte tree. In this example the alphabet has 512 symbols, all with the same probability. The root node has 254 empty spaces that could be occupied by symbols from the second level of the tree, changing their code lengths from 2 bytes to 1 byte. A way to ensure that the empty nodes always go to the lowest level of the tree follows. We calculate beforehand the number of empty nodes that will arise.    We then compose these empty nodes with symbols of smallest probabilities (for moving the empty nodes to the deepest level ofthe final tree). To accomplish this, we need only to select a number of symbols equal to 1 \+ ((v \- 256) mod 255), where v is the total number of symbols (i.e., the size of the vocabulary), for composing with the empty nodes. For instance, in the example in Figure 7.4(a), we have that 2 elements must be coupled with 254 empty nodes in the first step (because, 1 \+ ((512 - 256) mod 255) = 2). The remaining steps are similar to the binary Huffman tree construction algorithm. All techniques for efficient encoding and decoding mentioned previously can easily be extended to handle word-based byte Huffman coding. Moreover, no significant decrease of the compression ratio is experienced by using bytes instead of bits when the symbols are words. Further, decompression of byte Huffman code is faster than decompression of binary Huffman code. In fact, compression and decompression are very fast and compression ratios achieved are better than those of the Ziv-Lempel family [848, 849]. In practice byte processing is much faster than bit processing because bit shifts and masking operations are not necessary at decoding time or at searching time. One important consequence of using byte Huffman coding is the possibility of performing direct searching on compressed text. The searching algorithm is explained in Chapter 8\. The exact search can be done on the compressed text directly, using any known sequential pattern matching algorithm. Moreover, it allows a large number of variations of the exact and approximate compressed pattern matching problem, such as phrases, ranges, complements, wild cards, and arbitrary regular expressions. The algorithm is based on a word-oriented shiftor algorithm and on a fast Boyer-Moore-type filter. For approximate searching on the compressed text it is eight times faster than an equivalent approximate searching on the uncompressed text, thanks to the use of the vocabulary by the algorithm [577, 576]. This technique is not only useful in speeding up sequential search. It can also be used to improve indexed schemes that combine inverted files and sequential search, like Glimpse [540].  ",
2127,mir,mir-2127,7.4.4 Dictionary Methods," 7.4.4 Dictionary Methods Dictionary methods achieve compression by replacing groups of consecutive symbols (or phrases) with a pointer to an entry in a dictionary. Thus, the central decision in the design of a dictionary method is the selection of entries in the dictionary. The choice of phrases can be made by static, semi-adaptive, or adaptive algorithms. The simplest dictionary schemes use static dictionaries containing short phrases. Static dictionary encoders are fast as they demand little effort for achieving a small amount of compression. One example that has been proposed several times in different forms is the digram coding, where selected pairs of letters are replaced with codewords. At each step the next two characters are inspected and verified if they correspond to a digram in the dictionary. If so, they are coded together and the coding position is shifted by two characters; otherwise, the single character is represented by its normal code and the coding     position is shifted by one character. The main problem with static dictionary encoders is that the dictionary might be suitable for one text and unsuitable for another. One way to avoid this problem is to use a semi-static dictionary scheme, constructing a new dictionary for each text to be compressed. However, the problem of deciding which phrases should be put in the dictionary is not an easy task at all. One elegant solution to this problem is to use an adaptive dictionary scheme, such as the one proposed in the 1970s by Ziv and Lempel. The Ziv-Lempel type of adaptive dictionary scheme uses the idea of replacing strings of characters with a reference to a previous occurrence of the string. This approach is effective because most characters can be coded as part of a string that has occurred earlier in the text. If the pointer to an earlier occurrence of a string is stored in fewer bits than the string it replaces then compression is achieved. Adaptive dictionary methods present some disadvantages over the statistical word-based Huffman method. First, they do not allow decoding to start in the middle of a compressed file. As a consequence direct access to a position in the compressed text is not possible, unless the entire text is decoded from the beginning until the desired position is reached. Second, dictionary schemes are still popular for their speed and economy of memory, but the new results in statistical methods make them the method of choice in an IR environment. Moreover, the improvement of computing technology will soon make statistical methods feasible for general use, and the interest in dictionary methods will eventually decrease.  ",
2128,mir,mir-2128,7.4.5 Inverted File Compression," 7.4.5 Inverted File Compression As already discussed, an inverted file is typically composed of (a) a vector containing all the distinct words in the text collection (which is called the vocabulary) and (b) for each word in the vocabulary, a list of all documents in which that word occurs. Inverted files are widely used to index large text files. The size of an inverted file can be reduced by compressing the inverted lists. Because the list of document numbers within the inverted list is in ascending order, it can also be considered as a sequence of gaps between document numbers. Since processing is usually done sequentially starting from the beginning of the list, the original document numbers can always be recomputed through sums of the gaps. By observing that these gaps are small for frequent words and large for infrequent words, compression can be obtained by encoding small values with shorter codes. One possible coding scheme for this case is the unary code, in which an integer x is coded as (x -1) one bits followed by a zero bit, so the code for the integer 3 is 110\. The second column of Table 7.1 shows unary codes for integers between 1 and 10\. Elias [235] presented two other variable-length coding schemes for integers. One is Elias-v code, which represents the number x by a concatenation of two    Ga.p x Unary Elias-""Y Elias-6 Golomb b=3 1 0 0 0 00 2 10 100 1000 010 3 110 101 1001 011 4 1110 11000 10100 100 5 11110 11001 10101 1010 6 111110 11010 10110 1011 7 1111110 11011 10111 1100 8 11111110 1110000 11000000 11010 9 111111110 1110001 11000001 11011 10 1111111110 1110010 11000010 11100 Table 7.1 Example codes for integers. parts: (1) a unary code for 1+ [log z j and (2) a code of [log z] bits that represents the value of x \2LlogxJ in binary. For x = 5, we have that 1 \+ [log z] = 3 and that x \2LlogxJ = 1\. Thus, the Elias-v code for x = 5 is generated by combining the unary code for 3 (code 110) with the 2-bits binary number for 1 (code 01) which yields the codeword 11001. Other examples of Elias-v codes are shown in Table 7.1. The other coding scheme introduced by Elias is the Elias-d code, which represents the prefix indicating the number of binary bits by the Elias-v code rather than the unary code. For x = 5, the first part is then 101 instead of 110. Thus, the Elias-6 codeword for x = 5 is 10101\. In general, the Elias-6 code for an arbitrary integer x requires 1 \+ 2 [log log 2xj \+ [log z] bits. Table 7.1 shows other examples of Elias-8 codes. In general, for small values of x the Elias--y codes are shorter than the Elias-6 codes. However, in the limit, as x becomes large, the situation is reversed. Golomb [307] presented another run-length coding method for positive integers. The Golomb code is very effective when the probability distribution is geometric. With inverted files, the likelihood of a gap being of size x can be computed as the probability of having x-I non-occurrences (within consecutively numbered documents) of that particular word followed by one occurrence. If a word occurs within a document with a probability p, the probability of a gap of size x is then PrIx] = (1 _ p)x-l p which is the geometric distribution. In this case, the model is parameterized and makes use of the actual density of pointers in the inverted file. Let N be the number of documents in the system and V be the size of the vocabulary. Then, the probability p that any randomly selected document contains any randomly    chosen term can be estimated as number of pointers p= NxV where the number of pointers represent the 'size' of the index. The Golomb method works as follows. For some parameter b, a gap x &gt; 0 is coded as q \+ 1 in unary, where q = l(x -l)/bJ, followed by r = (x -1) \- q x b coded in binary, requiring either [log e] or [log e] bits. That is, if r &lt; 2LlogbJ-l then the number coded in binary requires [log b J bits, otherwise it requires flog b 1 bits where the first bit is 1 and the remaining bits assume the value r_2LlogbJ-l coded in [log b J binary digits. For example, with b = 3 there are three possible remainders, and those are coded as 0, 10, and 11, for r = 0, r = 1, and r = 2, respectively. Similarly, for b = 5 there are five possible remainders r, 0 through 4, and these are assigned codes 00, 01, 100, 101, and 110\. Then, if the value x = 9 is to be coded relative to b = 3, calculation yields q = 2 and r = 2, because 9 -1 = 2 x 3 \+ 2\. Thus, the encoding is 110 followed by 11\. Relative to b = 5, the values calculated are q = 1 and r = 1, resulting in a code of 10 followed by 101\. To operate with the Golomb compression method, it is first necessary to establish the parameter b for each term. For gap compression, an appropriate value is b ~ 0.69(N / It), where N is the total number of documents and ft is the number of documents that contain term i: Witten, Moffat and Bell [825) present a detailed study of different text collections. For all of their practical work on compression of inverted lists, they use Golomb code for the list of gaps. In this case Golomb code gives better compression than either Elias-v or Elias-d. However, it has the disadvantage of requiring two passes to be generated, since it requires knowledge of It, the number of documents containing term t. Moffat and Bell [572) show that the index for the 2 gigabytes TREC-3 collection, which contains 162,187,989 pointers and 894,406 distinct terms, when coded with Golomb code, occupies 132 megabytes. Considering the average number of bits per pointer, they obtained 5.73, 6.19, and 6.43 using Golomb, Elias-8, and Elias-')', respectively. 7.5 Comparing Text Compression Techniques Table 7.2 presents a comparison between arithmetic coding, character- based Huffman coding, word-based Huffman coding, and Ziv-Lempel coding, considering the aspects of compression ratio,' compression speed, decompression speed, memory space overhead, compressed pattern matching capability, and random access capability. One important objective of any compression method is to be able to obtain good compression ratios. It seems that two bits per character (or 25% compression ratio) is a very good result for natural language texts. Thus, 'very good' in the context of Table 7.2 means a compression ratio under 30%, 'good' means a compression ratio between 30% and 45%, and 'poor'means a compression ratio over 45%.  ",
2189,iir,iir-2189,8 Evaluation in information retrieval,"       151 8Evaluation in information retrieval We have seen in the preceding chapters many alternatives in designing an IR system. How do we know which of these techniques are effective in which applications? Should we use stop lists? Should we stem? Should we use inverse document frequency weighting? Information retrieval has developed as a highly empirical discipline, requiring careful and thorough evaluation to demonstrate the superior performance of novel techniques on representative document collections. In this chapter we begin with a discussion of measuring the effectiveness of IR systems (Section 8.1) and the test collections that are most often used for this purpose (Section 8.2). We then present the straightforward notion of relevant and nonrelevant documents and the formal evaluation methodology that has been developed for evaluating unranked retrieval results (Section 8.3). This includes explaining the kinds of evaluation measures that are standardly used for document retrieval and related tasks like text classification and why they are appropriate. We then extend these notions and develop further measures for evaluating ranked retrieval results (Section 8.4) and discuss developing reliable and informative test collections (Section 8.5). We then step back to introduce the notion of user utility, and how it is approximated by the use of document relevance (Section 8.6). The key utility measure is user happiness. Speed of response and the size of the index are factors in user happiness. It seems reasonable to assume that relevance of results is the most important factor: blindingly fast, useless answers do not make a user happy. However, user perceptions do not always coincide with system designers’ notions of quality. For example, user happiness commonly depends very strongly on user interface design issues, including the layout, clarity, and responsiveness of the user interface, which are independent of the quality of the results returned. We touch on other measures of the quality of a system, in particular the generation of high-quality result summary snippets, which strongly influence user utility, but are not measured in the basic relevance ranking paradigm (Section 8.7).   8 Evaluation in information retrieval 8.1 Information retrieval system evaluati ",8.1
2129,mir,mir-2129,8 Indexing and Searching, 8 Indexing and Searching with Gonzalo Navarro 8.1 Intro ,
2190,iir,iir-2190,8.1 Information retrieval system evaluation," 8.1 Information retrieval system evaluation To measure ad hoc information retrieval effectiveness in the standard way, we need a test collection consisting of three things: 1\. A document collection 2\. A test suite of information needs, expressible as queries 3\. A set of relevance judgments, standardly a binary assessment of either relevant or nonrelevant for each query-document pair. The standard approach to information retrieval system evaluation revolves around the notion of relevant and nonrelevant documents. With respect to a user information need, a document in the test collection is given a binary classification as either relevant or nonrelevant. This decision is referred to as the gold standard or ground truth judgment of relevance. The test document collection and suite of information needs have to be of a reasonable size: you need to average performance over fairly large test sets, as results are highly variable over different documents and information needs. As a rule of thumb, 50 information needs has usually been found to be a sufficient minimum. Relevance is assessed relative to an information need, not a query. For example, an information need might be: Information on whether drinking red wine is more effective at reducing your risk of heart attacks than white wine. This might be translated into a query such as: wine AND red AND white AND heart AND attack AND effective A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query. This distinction is often misunderstood in practice, because the information need is not overt. But, nevertheless, an information need is present. If a user types python into a web search engine, they might be wanting to know where they can purchase a pet python. Or they might be wanting information on the programming language Python. From a one word query, it is very difficult for a system to know what the information need is. But, nevertheless, the user has one, and can judge the returned results on the basis of their relevance to it. To evaluate a system, we require an overt expression of an information need, which can be used for judging returned documents as relevant or nonrelevant. At this point, we make a simplification: relevance can reasonably be thought of as a scale, with some documents highly relevant and others marginally so. But for the moment, we will use just a binary decision of relevance. We      ",8.1
2130,mir,mir-2130,8.1 Introduction," 8.1 Introduction Chapter 4 describes the query operations that can be performed on text databases. In this chapter we cover the main techniques we need to implement those query operations. We first concentrate on searching queries composed of words and on reporting the documents where they are found. The number of occurrences of a query in each document and even its exact positions in the text may also be required. Following that, we concentrate on algorithms dealing with Boolean operations. We then consider sequential search algorithms and pattern matching. Finally, we consider structured text and compression techniques. An obvious option in searching for a basic query is to scan the text sequentially. Sequential or online text searching involves finding the occurrences of a pattern in a text when the text is not preprocessed. Online searching is appropriate when the text is small (i.e., a few megabytes), and it is the only choice if the text collection is very volatile (i.e., undergoes modifications very frequently) or the index space overhead cannot be afforded. A second option is to build data structures over the text (called indices) to speed up the search. It is worthwhile building and maintaining an index when the text collection is large and semi-static. Semi-static collections can be updated at reasonably regular intervals (e.g., daily) but they are not deemed to support thousands of insertions of single words per second, say. This is the case for most real text databases, not only dictionaries or other slow growing literary works. For instance, it is the case for Web search engines or journal archives. Nowadays, the most successful techniques for medium size databases (say up to 200Mb) combine online and indexed searching. We cover three main indexing techniques: inverted files, suffix arrays, and signature files. Keyword-based search is discussed first. We emphasize inverted files, which are currently the best choice for most applications. Suffix trees and arrays are faster for phrase searches and other less common queries, but are harder to build and maintain. Finally, signature files were popular in the 1980s, but nowadays inverted files outperform them. For all the structures we pay attention not only to their search cost and space overhead, but also to the cost of building and updating them. We assume that the reader is familiar with basic data structures, such as sorted arrays, binary search trees, B-trees, hash tables, and tries. Since tries are heavily used we give a brief and simplified reminder here. Tries, or digital search trees, are multiway trees that store sets of strings and are able to retrieve any string in time proportional to its length (independent of the number of strings stored). A special character is added to the end of the string to ensure that no string is a prefix of another. Every edge of the tree is labeled with a letter. To search a string in a trie, one starts at the root and scans the string characterwise, descending by the appropriate edge of the trie. This continues until a leaf is found (which represents the searched string) or the appropriate edge to follow does not exist at some point (i.e., the string is not in the set). See Figure 8.3 for an example of a text and a trie built on its words. Although an index must be built prior to searching it, we present these tasks in the reverse order. We think that understanding first how a data structure is used makes it clear how it is organized, and therefore eases the understanding of the construction algorithm, which is usually more complex. Throughout this chapter we make the following assumptions. We call n the size of the text database. Whenever a pattern is searched, we assume that it is of length m, which is much smaller than n. We call M the amount of main memory available. We assume that the modifications which a text database undergoes are additions, deletions, and replacements (which are normally made by a deletion plus an addition) of pieces of text of size n' &lt; n. We give experimental measures for many algorithms to give the reader a grasp of the real times involved. To do this we use a reference architecture throughout the chapter, which is representative of the power of today's computers. We use a 32-bit Sun UltraSparc-1 of 167 MHz with 64 Mb of RAM, running Solaris. The code is written in C and compiled with all optimization options. For the text data, we use collections from TREC-2, specifically WSJ, DOE, FR, ZIFF and AP. These are described in more detail in Chapter 3\.  ",
2155,mir,mir-2155,8.10 Bibliographic Discussion," 8.10 Bibliographic Discussion A detailed explanation of a full inverted index and its construction and querying process can be found in [26]. This work also includes an analysis of the algorithms on inverted lists using the distribution of natural language. The in-place construction is described in [572]. Another construction algorithm is presented in [341]. The idea of block addressing inverted indices was first presented in a system called Glimpse [540], which also first exposed the idea of performing complex pattern matching using the vocabulary of the inverted index. Block addressing indices are analyzed in [42], where some performance improvements are proposed. The variant that indexes sequences instead of words has been implemented in a system called Grampse, which is described in [497]. Suffix arrays were presented in [538] together with the algorithm to build them in O(nlogn) character comparisons. They were independently discovered    228 INDEXING AND SEARCHING by [309] under the name of 'PAT arrays.' The algorithm to build large suffix arrays is presented in [311]. The use of supra-indices over suffix array is proposed in [37], while the modified binary search techniques to reduce disk seek time are presented in [56]. The linear-time construction of suffix trees is described in [780]. The material on signature files is based on [243]. The different alternative ways of storing the signature file are explained in [242]. The original references for the sequential search algorithms are: KMP [447], BM [110], BMH [376], BMS [751], Shift-Or [39], BDM [205] and BNDM [592]. The multipattern versions are found in [9, 179], and MultiBDM in [196]. Many enhancements of bit-parallelism to support extended patterns and allow errors are presented in [837]. Many ideas from that paper were implemented in a widely distributed software for online searching called Agrep [836]. The reader interested in more details about sequential searching algorithms may look for the original references or in good books on algorithms such as [310, 196]. One source for the classical solution to approximate string matching is [716]. An O(kn) worst-case algorithm is described in [480]. The use of a DFA is proposed in [781]. The bit-parallel approach to this problem started in [837], although currently the fastest bit-parallel algorithms are [583] and [43]. Among all the filtering algorithms, the fastest one in practice is based on an idea presented in [837], later enhanced in [45], and finally implemented in [43]. A good source from which to learn about regular expressions and building a DFA is [375]. The bit-parallel implementation ofthe NFA is explained in [837]. Regular expression searching on suffix trees is described in [40], while searching allowing errors is presented in [779]. The Huffman coding was first presented in [386], while the word-oriented alternative is proposed in [571]. Sequential searching on text compressed using that technique is described in [577]. Compression used in combination with inverted files is described in [850], with suffix trees in [430], with suffix arrays in [575], and with signature files in [243, 242]. A good general reference on compression is [78].  ",
2131,mir,mir-2131,8.2 Inverted Files," 8.2 Inverted Files An inverted file (or inverted index) is a word-oriented mechanism for indexing a text collection in order to speed up the searching task. The inverted file structure is composed of two elements: the vocabulary and the occurrences. The vocabulary is the set of all different words in the text. For each such word a list of all the text positions where the word appears is stored. The set of all those lists is called the 'occurrences' (Figure 8.1 shows an example). These positions can refer to words or characters. Word positions (i.e., position i refers to the i-t,h word) simplify. This is a text. A text has many words. Words are made from letters. Text VocabuIIrY Occu..letters 80... made so ... many 28 ... text 11,19 ... Inverted Index words 33,40 ... Figure 8.1 A sample text and an inverted index built on it. The words are converted to lower-case and some are not indexed. The occurrences point to character positions in the text. phrase and proximity queries, while character positions (i.e., the position i is the i-th character) facilitate direct access to the matching text positions. Some authors make the distinction between inverted files and inverted lists. In an inverted file, each element of a list points to a document or file name, while inverted lists match our definition. We prefer not to make such a distinction because, as we will see later, this is a matter of the addressing granularity, which can range from text positions to logical blocks. The space required for the vocabulary is rather small. According to Heaps' law (see Chapter 6) the vocabulary grows as O(n,B), where (3 is a constant between 0 and 1 dependent on the text, being between 0.4 and 0.6 in practice. For instance, for 1 Gb of the TREC-2 collection the vocabulary has a size of only 5 Mb. This may be further reduced by stemming and other normalization techniques as described in Chapter 7\. The occurrences demand much more space. Since each word appearing in the text is referenced once in that structure, the extra space is O(n). Even omitting stopwords (which is the default practice when words are indexed), in practice the space overhead of the occurrences is between 30% and 40% of the text size. To reduce space requirements, a technique called block addressing is used. The text is divided in blocks, and the occurrences point to the blocks where the word appears (instead of the exact positions). The classical indices which point to the exact occurrences are called 'full inverted indices.' By using block addressing not only can the pointers be smaller because there are fewer blocks than positions, but also all the occurrences of a word inside a single block are collapsed to one reference (see Figure 8.2). Indices of only 5% overhead over the text size are obtained with this technique. The price to pay is that, if the exact occurrence positions are required (for instance, for a proximity query), then an online search over the qualifying blocks has to be performed. For instance, block addressing indices with 256 blocks stop working well with texts of 200 Mb. Table 8.1 presents the projected space taken by inverted indices for texts of    Block 1 Block 2 Block 3 Block 4 This is a text. A text has many words. Words are made from letters. Text V_1wy OCCu""."".,.. letters 4... made 4 ... many 2... text 1.2 ... words 3... Inverted Index Figure 8.2 The sample text split into four blocks, and an inverted index using block addressing built on it. The occurrences denote block numbers. Notice that both occurrences of 'words' collapsed into one. different sizes, with and without the use of stopwords. The full inversion stands for inverting all the words and storing their exact positions, using four bytes per pointer. The document addressing index assumes that we point to documents which are of size 10 Kb (and the necessary number of bytes per pointer, i.e, one, two, and three bytes, depending on text size). The block addressing index assumes that we use 256 or 64K blocks (one or two bytes per pointer) independently of the text size. The space taken by the pointers can be significantly reduced by using compression. We assume that 45% of all the words are stopwords, and that there is one non-stopword each 11.5 characters. Our estimation for the vocabulary is based on Heaps' law with parameters V = 30nO. 5 • All these decisions were taken according to our experience and experimentally validated. The blocks can be of fixed size (imposing a logical block structure over the text database) or they can be defined using the natural division of the text collection into files, documents, Web pages, or others. The division into blocks of fixed size improves efficiency at retrieval time, i.e, the more variance in the block sizes, the more amount of text sequentially traversed on average. This is because larger blocks match queries more frequently and are more expensive to traverse. Alternatively, the division using natural cuts may eliminate the need for online traversal. For example, if one block per retrieval unit is used and the exact match positions are not required, there is no need to traverse the text for single-word queries, since it is enough to know which retrieval units to report. But if, on the other hand, many retrieval units are packed into a single block, the block has to be traversed to determine which units to retrieve. It is important to notice that in order to use block addressing, the text must be readily available at search time. This is not the case for remote text (as in Web search engines), or if the text is in a CD-ROM that has to be mounted, for instance. Some restricted queries not needing exact positions can still be solved if the blocks are retrieval units.    Index Small collection Medium collection Large collection (1 Mb) (200 Mb) (2 Gb) Addressing words 45% 73% 36% 64% 35% 63% Addressing documents 19% 26% 18% 32% 26% 47% Addressing 64K blocks 27% 41% 18% 32% 5% 9% Addressing 256 blocks 18% 25% 1.7% 2.4% 0.5% 0.7% Table 8.1 Sizes of an inverted file as approximate percentages of the size the whole text collection. Four granularities and three collections are considered. For each collection, the right column considers that stopwords are not indexed while the left column considers that all words are indexed.  ",
2191,iir,iir-2191,8.2 Standard test collections," 8.2 Standard test collections 153 discuss the reasons for using binary relevance judgments and alternatives in Section 8.5.1. Many systems contain various weights (often known as parameters) that can be adjusted to tune system performance. It is wrong to report results on a test collection which were obtained by tuning these parameters to maximize performance on that collection. That is because such tuning overstates the expected performance of the system, because the weights will be set to maximize performance on one particular set of queries rather than for a random sample of queries. In such cases, the correct procedure is to have one or more development test collections, and to tune the parameters on the development test collection. The tester then runs the system with those weights on the test collection and reports the results on that collection as an unbiased estimate of performance. 8.2 Standard test collections Here is a list of the most standard test collections and evaluation series. We focus particularly on test collections for ad hoc information retrieval system evaluation, but also mention a couple of similar test collections for text classification. The Cranfield collection. This was the pioneering test collection in allowing precise quantitative measures of information retrieval effectiveness, but is nowadays too small for anything but the most elementary pilot experiments. Collected in the United Kingdom starting in the late 1950s, it contains 1398 abstracts of aerodynamics journal articles, a set of 225 queries, and exhaustive relevance judgments of all (query, document) pairs. Text Retrieval Conference (TREC). The U.S. National Institute of Standards and Technology (NIST) has run a large IR test bed evaluation series since 1992. Within this framework, there have been many tracks over a range of different test collections, but the best known test collections are the ones used for the TREC Ad Hoc track during the first 8 TREC evaluations between 1992 and 1999. In total, these test collections comprise 6 CDs containing 1.89 million documents (mainly, but not exclusively, newswire articles) and relevance judgments for 450 information needs, which are called topics and specified in detailed text passages. Individual test collections are defined over different subsets of this data. The early TRECs each consisted of 50 information needs, evaluated over different but overlapping sets of documents. TRECs 6–8 provide 150 information needs over about 528,000 newswire and Foreign Broadcast Information Service articles. This is probably the best subcollection to use in future work, because it is the largest and the topics are more consistent. Because the test     154 8 Evaluation in information retrieval document collections are so large, there are no exhaustive relevance judgments. Rather, NIST assessors’ relevance judgments are available only for the documents that were among the top kreturned for some system which was entered in the TREC evaluation for which the information need was developed. In more recent years, NIST has done evaluations on larger document collections, including the 25 million page GOV2 web page collection. From the beginning, the NIST test document collections were orders of magnitude larger than anything available to researchers previously and GOV2 is now the largest Web collection easily available for research purposes. Nevertheless, the size of GOV2 is still more than 2 orders of magnitude smaller than the current size of the document collections indexed by the large web search companies. NII Test Collections for IR Systems (NTCIR). The NTCIR project has built various test collections of similar sizes to the TREC collections, focusing on East Asian language and cross-language information retrieval, where queries are made in one language over a document collection containing documents in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/dataen.html Cross Language Evaluation Forum (CLEF). This evaluation series has con-CLEF centrated on European languages and cross-language information retrieval. See: http://www.clef-campaign.org/ Reuters-21578 and Reuters-RCV1. For text classification, the most used test collection has been the Reuters-21578 collection of 21578 newswire articles; see Chapter 13, page 279. More recently, Reuters released the much larger Reuters Corpus Volume 1 (RCV1), consisting of 806,791 documents; see Chapter 4, page 69. Its scale and rich annotation makes it a better basis for future research. 20 Newsgroups. This is another widely used text classification collection,20 NEWSGROUPS collected by Ken Lang. It consists of 1000 articles from each of 20 Usenet newsgroups (the newsgroup name being regarded as the category). After the removal of duplicate articles, as it is usually used, it contains 18941 articles.  ",8.2
2132,mir,mir-2132,8.2.1 Searching," 8.2.1 Searching The search algorithm on an inverted index follows three general steps (some may be absent for specific queries): • Vocabulary search The words and patterns present in the query are isolated and searched in the vocabulary. Notice that phrases and proximity queries are split into single words. • Retrieval of occurrences The lists of the occurrences of all the words found are retrieved. • Manipulation of occurrences The occurrences are processed to solve phrases, proximity, or Boolean operations. If block addressing is used it may be necessary to directly search the text to find the information missing from the occurrences (e.g., exact word positions to form phrases). Hence, searching on an inverted index always starts in the vocabulary. Because of this it is a good idea to have it in a separate file. It is possible that this file fits in main memory even for large text collections. Single-word queries can be searched using any suitable data structure to speed up the search, such as hashing, tries, or B-trees. The first two give O(m) search cost (independent of the text size). However, simply storing the words in lexicographical order is cheaper in space and very competitive in performance, since the word can be binary searched at O(logn) cost. Prefix and range queries can also be solved with binary search, tries, or B-trees, but not with hashing. If the query is formed by single words, then the process ends by delivering the list of occurrences (we may need to make a union of many lists if the pattern matches many words).     Context queries are more difficult to solve with inverted indices. Each element must be searched separately and a list (in increasing positional order) generated for each one. Then, the lists of all elements are traversed in synchronization to find places where all the words appear in sequence (for a phrase) or appear close enough (for proximity). If one list is much shorter than the others, it may be better to binary search its elements into the longer lists instead of performing a linear merge. It is possible to prove using Zipf's law that this is normally the case. This is important because the most time-demanding operation on inverted indices is the merging or intersection of the lists of occurrences. If the index stores character positions the phrase query cannot allow the separators to be disregarded, and the proximity has to be defined in terms of character distance. Finally, note that if block addressing is used it is necessary to traverse the blocks for these queries, since the position information is needed. It is then better to intersect the lists to obtain the blocks which contain all the searched words and then sequentially search the context query in those blocks as explained in section 8.5. Some care has to be exercised at block boundaries, since they can split a match. This part of the search, if present, is also quite time consuming. Using Heaps' and the generalized Zipf's laws, it has been demonstrated that the cost of solving queries is sublinear in the text size, even for complex queries involving list merging. The time complexity is O(n Q ) , where a depends on the query and is close to 0.4 ..0.8 for queries with reasonable selectivity. Even if block addressing is used and the blocks have to be traversed, it is possible to select the block size as an increasing function of n, so that not only does the space requirement keep sublinear but also the amount of text traversed in all useful queries is also sublinear. Practical figures show, for instance, that both the space requirement and the amount of text traversed can be close to O(nO. 8 5 ) . Hence, inverted indices allow us to have sublinear search time at sublinear space requirements. This is not possible on the other indices. Search times on our reference machine for a full inverted index built on 250 Mb of text give the following results: searching a simple word took 0.08 seconds, while searching a phrase took 0.25 to 0.35 seconds (from two to five words).  ",
2133,mir,mir-2133,8.2.2 Construction," 8.2.2 Construction Building and maintaining an inverted index is a relatively low cost task. In principle, an inverted index on a text of n characters can be built in O(n) time. All the vocabulary known up to now is kept in a trie data structure, storing for each word a list of its occurrences (text positions). Each word of the text is read and searched in the trie. If it is not found, it is added to the trie with an empty list of occurrences. Once it is in the trie, the new position is added to the end of its list of occurrences. Figure 8.3 illustrates this process. Once the text is exhausted, the trie is written to disk together with the list of occurrences. It is good practice to split the index into two files. In the     This is a text, A text has many words. Words are made from letters. Vocabulary trle Text x ! letters: 60 I 'I' 'd'/I....""'::::""':""""';"";""-J ~ ' m ' 1 ' 's' 'n' L-----J I text; 11,19 I 'w' I words: 33, 40 , Figure 8.3 Building an inverted index for the sample text, first file, the lists of occurrences are stored contiguously. In this scheme, the file is typically called a 'posting file'. In the second file, the vocabulary is stored in lexicographical order and, for each word, a pointer to its list in the first file is also included. This allows the vocabulary to be kept in memory at search time in many cases. Further, the number of occurrences of a word can be immediately known from the vocabulary with little or no space overhead. We analyze now the construction time under this scheme. Since in the trie O( 1) operations are performed per text character, and the positions can be inserted at the end of the lists of occurrences in 0(1) time, the overall process is O(n) worst-case time. However, the above algorithm is not practical for large texts where the index does not fit in main memory. A paging mechanism will severely degrade the performance of the algorithm. We describe an alternative which is faster in practice. The algorithm already described is used until the main memory is exhausted (if the trie takes up too much space it can be replaced by a hash table or other structure), When no more memory is available, the partial index Ii obtained up to now is written to disk and erased from main memory before continuing with the rest of the text. Finally, a number of partial indices I, exist on disk. These indices are then merged in a hierarchical fashion. Indices It and 1 2 are merged to obtain the index !t,,2; 1 3 and 1 4 produce h.4; and so on. The resulting partial indices are now approximately twice the size. When all the indices at this level have been merged in this way, the merging proceeds at the next level, joining the index h,2 with the index 1 3 ,, 4 to form h,4' This is continued until there is just one index comprising the whole text, as illustrated in Figure 8.4. Merging two indices consists of merging the sorted vocabularies, and whenever the same word appears in both indices, merging both lists of occurrences, By construction, the occurrences of the smaller-numbered index are before those of the larger-numbered index, and therefore the lists are just concatenated. This is a very fast process in practice, and its complexity is O(nl +n2), where nl and n2 are the sizes of the indices.     Rectangles represent partial indices, while rounded rectangles represent merging operations. The numbers inside the merging operations show a possible merging order. The total time to generate the partial indices is O(n) as before. The number of partial indices is O(n/M). Each level of merging performs a linear process over the whole index (no matter how it is split into partial indices at this level) and thus its cost is O(n). To merge the O(n/M) partial indices, log2(n/M) merging levels are necessary, and therefore the cost of this algorithm is O( n log( n] M)). More than two indices can be merged at once. Although this does not change the complexity, it improves efficiency since fewer merging levels exist. On the other hand, the memory buffers for each partial index to merge will be smaller and hence more disk seeks will be performed. In practice it is a good idea to merge even 20 partial indices at once. Real times to build inverted indices on the reference machine are between 4-8 Mb / min for collections of up to 1 G b (the slowdown factor as the text grows is barely noticeable). Of this time, 20-30% is spent on merging the partial indices. To reduce build-time space requirements, it is possible to perform the merging in-place. That is, when two or more indices are merged, write the result in the same disk blocks of the original indices instead of on a new file. It is also a good idea to perform the hierarchical merging as soon as the files are generated (e.g., collapse It and Iz into h.2 as soon as 1 2 is produced). This also reduces space requirements because the vocabularies are merged and redundant words are eliminated (there is no redundancy in the occurrences). The vocabulary can be a significative part of the smaller partial indices, since they represent a small text. This algorithm changes very little if block addressing is used. Index maintenance is also cheap. Assume that a new text of size n' is added to the database. The inverted index for the new text is built and then both indices are merged   as is done for partial indices. This takes O( n \+ n' log( n' / M)). Deleting text can be done by an O(n) pass over the index eliminating the occurrences that point inside eliminated text areas (and eliminating words if their lists of occurrences disappear in the process).  ",
2192,iir,iir-2192,8.3 Evaluation of unranked retrieval sets," 8.3 Evaluation of unranked retrieval sets Given these ingredients, how is system effectiveness measured? The two most frequent and basic measures for information retrieval effectiveness are precision and recall. These are first defined for the simple case where an   8.3 Evaluation of unranked retrieval sets 155 IR system returns a set of documents for a query. We will see later how to extend these notions to ranked retrieval situations. Precision (P) is the fraction of retrieved documents that are relevant   Precision =#(relevant items retrieved) #(retrieved items)=P(relevant|retrieved) (8.1) Recall (R) is the fraction of relevant documents that are retrieved Recall =#(relevant items retrieved) #(relevant items)=P(retrieved|relevant) (8.2) These notions can be made clear by examining the following contingency table: (8.3) Relevant Nonrelevant Retrieved true positives (tp) false positives (fp) Not retrieved false negatives (fn) true negatives (tn) Then: P=tp/(tp +f p) (8.4) R=tp/(tp +f n) An obvious alternative that may occur to the reader is to judge an information retrieval system by its accuracy, that is, the fraction of its classifica- tions that are correct. In terms of the contingency table above, accuracy = (tp +tn)/(tp +f p +f n +tn). This seems plausible, since there are two actual classes, relevant and nonrelevant, and an information retrieval system can be thought of as a two-class classifier which attempts to label them as such (it retrieves the subset of documents which it believes to be relevant). This is precisely the effectiveness measure often used for evaluating machine learning classification problems. There is a good reason why accuracy is not an appropriate measure for information retrieval problems. In almost all circumstances, the data is extremely skewed: normally over 99.9% of the documents are in the nonrelevant category. A system tuned to maximize accuracy can appear to perform well by simply deeming all documents nonrelevant to all queries. Even if the system is quite good, trying to label some documents as relevant will almost always lead to a high rate of false positives. However, labeling all documents as nonrelevant is completely unsatisfying to an information retrieval system user. Users are always going to want to see some documents, and can be      156 8 Evaluation in information retrieval assumed to have a certain tolerance for seeing some false positives providing that they get some useful information. The measures of precision and recall concentrate the evaluation on the return of true positives, asking what percentage of the relevant documents have been found and how many false positives have also been returned. The advantage of having the two numbers for precision and recall is that one is more important than the other in many circumstances. Typical web surfers would like every result on the first page to be relevant (high precision) but have not the slightest interest in knowing let alone looking at every document that is relevant. In contrast, various professional searchers such as paralegals and intelligence analysts are very concerned with trying to get as high recall as possible, and will tolerate fairly low precision results in order to get it. Individuals searching their hard disks are also often interested in high recall searches. Nevertheless, the two quantities clearly trade off against one another: you can always get a recall of 1 (but very low precision) by retrieving all documents for all queries! Recall is a non-decreasing function of the number of documents retrieved. On the other hand, in a good system, precision usually decreases as the number of documents retrieved is increased. In general we want to get some amount of recall while tolerating only a certain percentage of false positives. A single measure that trades off precision versus recall is the F measure, which is the weighted harmonic mean of precision and recall: F=1 α1 P\+ (1−α)1 R =(β2+1)PR β2P+Rwhere β2=1−α α (8.5) where α∈[0, 1]and thus β2∈[0, ∞]. The default balanced F measure equally weights precision and recall, which means making α=1/2 or β=1. It is commonly written as F1, which is short for Fβ=1, even though the formulation in terms of αmore transparently exhibits the F measure as a weighted harmonic mean. When using β=1, the formula on the right simplifies to: Fβ=1=2PR P+R (8.6) However, using an even weighting is not the only choice. Values of β&lt;1 emphasize precision, while values of β&gt;1 emphasize recall. For example, a value of β=3 or β=5 might be used if recall is to be emphasized. Recall, precision, and the F measure are inherently measures between 0 and 1, but they are also very commonly written as percentages, on a scale between 0 and 100. Why do we use a harmonic mean rather than the simpler average (arithmetic mean)? Recall that we can always get 100% recall by just returning all documents, and therefore we can always get a 50% arithmetic mean by the   8.3 Evaluation of unranked retrieval sets 157 ◮Figure 8.1 Graph comparing the harmonic mean to other means. The graph shows a slice through the calculation of various means of precision and recall for the fixed recall value of 70%. The harmonic mean is always less than either the arithmetic or geometric mean, and often quite close to the minimum of the two numbers. When the precision is also 70%, all the measures coincide. same process. This strongly suggests that the arithmetic mean is an unsuitable measure to use. In contrast, if we assume that 1 document in 10,000 is relevant to the query, the harmonic mean score of this strategy is 0.02%. The harmonic mean is always less than or equal to the arithmetic mean and the geometric mean. When the values of two numbers differ greatly, the harmonic mean is closer to their minimum than to their arithmetic mean; see Figure 8.1. ?Exercise 8.1 [⋆] An IR system returns 8 relevant documents, and 10 nonrelevant documents. There are a total of 20 relevant documents in the collection. What is the precision of the system on this search, and what is its recall? Exercise 8.2 [⋆] The balanced F measure (a.k.a. F1) is defined as the harmonic mean of precision and recall. What is the advantage of using the harmonic mean rather than “averaging” (using the arithmetic mean)?      158 8 Evaluation in information retrieval 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Recall Precision ◮Figure 8.2 Precision/recall graph. Exercise 8.3 [⋆⋆] Derive the equivalence between the two formulas for F measure shown in Equation (8.5), given that α=1/(β2+1).  ",8.3
2134,mir,mir-2134,8.3 Other Indices for Text," 8.3 Other Indices for Text 8.3.1 Suffix Trees and Suffix Arrays Inverted indices assume that the text can be seen as a sequence of words. This restricts somewhat the kinds of queries that can be answered. Other queries such as phrases are expensive to solve. Moreover, the concept of word does not exist in some applications such ",
2135,mir,mir-2135,8.3.1 Suffix Trees and Suffix Arrays," 8.3.1 Suffix Trees and Suffix Arrays Inverted indices assume that the text can be seen as a sequence of words. This restricts somewhat the kinds of queries that can be answered. Other queries such as phrases are expensive to solve. Moreover, the concept of word does not exist in some applications such as genetic databases. In this section we present suffix arrays. Suffix arrays are a space efficient implementation of suffix trees. This type of index allows us to answer efficiently more complex queries. Its main drawbacks are its costly construction process, that the text must be readily available at query time, and that the results are not delivered in text position order. This structure can be used to index only words (without stopwords) as the inverted index as well as to index any text character. This makes it suitable for a wider spectrum of applications, such as genetic databases. However, for word-based applications, inverted files perform better unless complex queries are an important issue. This index sees the text as one long string. Each position in the text is considered as a text suffix (i.e., a string that goes from that text position to the end of the text). It is not difficult to see that two suffixes starting at different positions are lexicographically different (assume that a character smaller than all the rest is placed at the end of the text). Each suffix is thus uniquely identified by its position. Not all text positions need to be indexed. Index points are selected from the text, which point to the beginning of the text positions which will be retrievable. For instance, it is possible to index only word beginnings to have a functionality similar to inverted indices. Those elements which are not index points are not retrievable (as in an inverted index it is not possible to retrieve the middle of a word). Figure 8.5 illustrates this. Structure In essence, a suffix tree is a trie data structure built over all the suffixes of the text. The pointers to the suffixes are stored at the leaf nodes. To improve space utilization, this trie is compacted into a Patricia tree. This involves compressing unary paths, i.e. paths where each node has just one child. An indication of the next character position to consider is stored at the nodes which root a compressed path. Once unary paths are not present the tree has O(n) nodes instead of the worst-case O(n 2 ) of the trie (see Figure 8.6).    The sample text with the index points of interest marked. Below, the suffixes corresponding to those index points. 6 9 11 17 19 24 28 33 40 46 50 55 60 This is a text. A text has many words. Words are made from letters. Text SufflxTrie Figure 8.6 The suffix trie and suffix tree for the sample text. The problem with this structure is its space. Depending on the implementation, each node of the trie takes 12 to 24 bytes, and therefore even if only word beginnings are indexed, a space overhead of 120% to 240% over the text size is produced. Suffix arrays provide essentially the same functionality as suffix trees with much less space requirements. If the leaves of the suffix tree are traversed in left-to-right order (top to bottom in our figures), all the suffixes of the text are retrieved in lexicographical order. A suffix array is simply an array containing all the pointers to the text suffixes listed in lexicographical order, as shown in Figure 8.7. Since they store one pointer per indexed suffix, the space requirements     A supra-index over our suffix array. One out of three entries are sampled, keeping their first four characters. The pointers (arrows) are in fact unnecessary. are almost the same as those for inverted indices (disregarding compression techniques), i.e. close to 40% overhead over the text size. Suffix arrays are designed to allow binary searches done by comparing the contents of each pointer. If the suffix array is large (the usual case), this binary search can perform poorly because of the number of random disk accesses. To remedy this situation, the use of supra-indices over the suffix array has been proposed. The simplest supra-index is no more than a sampling of one out of b suffix array entries, where for each sample the first (, suffix characters are stored in the supra-index. This supra-index is then used as a first step of the search to reduce external accesses. Figure 8.8 shows an example. This supra-index does not in fact need to takesamples at fixed intervals, nor to take samples of the same length. For word-indexing suffix arrays it has been suggested that a new sample could be taken each time the first word of the suffix changes, and to store the word instead of (, characters. This is exactly the same as having a vocabulary of the text plus pointers to the array. In fact, the only important difference between this structure and an inverted index is that the occurrences of each word in an inverted index are sorted by text position, while in a suffix array they are sorted lexicographically by the text following the word. Figure 8.9 illustrates this relationship. The extra space requirements of supra-indices are modest. In particular, it is clear that the space requirements of the suffix array with a vocabulary supraindex are exactly the same as for inverted indices (except for compression, as we see later).    202 INDEXING AND SEARCHING 6 9 11 17 19 24 28 33 40 46 SO SS 60 This is a text. A text has many words. Words are made from letters. I letters \IJ~I/I ~ 19 111 140 133 Suffix Array ~ 11 119 133 I 40 I Inverted List Text Vocabulary Supra-Index Figure 8.9 Relationship between our inverted list and suffix array with vocabulary supra-index. Searching If a suffix tree on the text can be afforded, many basic patterns such as words, prefixes, and phrases can be searched in O(m) time by a simple trie search. However, suffix trees are not practical for large texts, as explained. Suffix arrays, on the other hand, can perform the same search operations in O(logn) time by doing a binary search instead of a trie search. This is achieved as follows: the search pattern originates two 'limiting patterns' PI and P 2 , so that we want any suffix S such that PI ~ S &lt; P 2 • We binary search both limiting patterns in the suffix array. Then, all the elements lying between both positions point to exactly those suffixes that start like the original pattern (i.e., to the pattern positions in the text). For instance, in our example of figure 8.9, in order to find the word 'text' we search for 'text' and 'texu', obtaining the portion of the array that contains the pointers 19 and II. All these queries retrieve a subtree of the suffix tree or an interval of the suffix array. The results have to be collected later, which may imply sorting them in ascending text order. This is a complication of suffix trees or arrays with respect to inverted indices. Simple phrase searching is a good case for these indices. A simple phrase of words can be searched as if it was a simple pattern. This is because the suffix tree/array sorts with respect to the complete suffixes and not only their first word. A proximity search, on the other hand, has to be solved element-wise. The matches for each element must be collected and sorted and then they have to be intersected as for inverted files. The binary search performed on suffix arrays, unfortunately, is done on disk, where the accesses to (random) text positions force a seek operation which spans the disk tracks containing the text. Since a random seek is O(n) in size, this makes the search cost O( n log n) time. Supra-indices are used as a first step in any binary search operation to alleviate this problem. To avoid performing o (log n) random accesses to the text on disk (and to the suffix array on disk), the search starts in the supra-index, which usually fits in main memory (text samples    OTHER INDICES FOR TEXT 203 included). After this search is completed, the suffix array block which is between the two selected samples is brought into memory and the binary search is completed (performing random accesses to the text on disk). This reduces disk search times to close to 25% of the original time. Modified binary search techniques that sacrifice the exact partition in the middle of the array taking into account the current disk head position allow a further reduction from 40% to 60%. Search times in a 250 Mb text in our reference machine are close to 1 second for a simple word or phrase, while the part corresponding to the accesses to the text sums up 0.6 seconds. The use of supra-indices should put the total time close to 0.3 seconds. Note that the times, although high for simple words, do not degrade for long phrases as with inverted indices. Construction in Main Memory A suffix tree for a text of n characters can be built in O(n) time. The algorithm, however, performs poorly if the suffix tree does not fit in main memory, which is especially stringent because of the large space requirements of the suffix trees. We do not cover the linear algorithm here because it is quite complex and only of theoretical interest. We concentrate on direct suffix array construction. Since the suffix array is no more than the set of pointers lexicographically sorted, the pointers are collected in ascending text order and then just sorted by the text they point to. Note that in order to compare two suffix array entries the corresponding text positions must be accessed. These accesses are basically random. Hence, both the suffix array and the text must be in main memory. This algorithm costs O(nlogn) string comparisons. An algorithm to build the suffix array in O(nlogn) character comparisons follows. All the suffixes are bucket-sorted in O(n) time according to the first letter only. Then, each bucket is bucket-sorted again, now according to their first two letters. At iteration i, the suffixes begin already sorted by their 2 i \- 1 first letters and end up sorted by their first 2 i letters. As at each iteration the total cost of all the bucket sorts is O(n), the total time is O(nlogn), and the average is O(n log log n) (since O(logn) comparisons are necessary on average to distinguish two suffixes of a text). This algorithm accesses the text only in the first stage (bucket sort for the first letter). In order to sort the strings in the i-th iteration, notice that since all suffixes are sorted by their first 2 i \- 1 letters, to sort the text positions T a ... and T b ... in the suffix array (assuming that they are in the same bucket, i.e., they share their first 2 i \- 1 letters), it is enough to determine the relative order between text positions T a \+ 2i-l and Tb+2i-1 in the current stage of the search. This can be done in constant time by storing the reverse permutation. We do not enter here into further detail. Construction of Suffix Arrays for Large Texts There is still the problem that large text databases will not fit in main memory. It could be possible to apply an external memory sorting algorithm. However,    204 INDEXING AND SEARCHING each comparison involves accessing the text at random positions on the disk. This will severely degrade the performance of the sorting process. We explain an algorithm especially designed for large texts. Split the text into blocks that can be sorted in main memory. Then, for each block, build its suffix array in main memory and merge it with the rest of the array already built for the previous text. That is: • build the suffix array for the first block, • build the suffix array for the second block, • merge both suffix' arrays, • build the suffix array for the third block, • merge the new suffix array with the previous one, • build the suffix array for the fourth block, • merge the new suffix array with the previous one, • ... and so on. The difficult part is how to merge a large suffix array (already built) with the small suffix array (just built). The merge needs to compare text positions which are spread in a large text, so the problem persists. The solution is to first determine how many elements of the large array are to be placed between each pair of elements in the small array, and later use that information to merge the arrays without accessing the text. Hence, the information that we need is how many suffixes of the large text lie between each pair of positions of the small suffix array. We compute counters that store this information. The counters are computed without using the large suffix array. The text corresponding to the large array is sequentially read into main memory. Each suffix of that text is searched in the small suffix array (in main memory). Once we find the inter-element position where the suffix lies, we just increment the appropriate counter. Figure 8.10 illustrates this process. We analyze this algorithm now. If there is O(M) main memory to index, then there will be O( n] M) text blocks. Each block is merged against an array of size O(n), where all the O(n) suffixes of the large text are binary searched in the small suffix array. This gives a total CPU complexity of O(n 2Iog(M)/M). Notice that this same algorithm can be used for index maintenance. If a new text of size n' is added to the database, it can be split into blocks as before and merged block-wise into the current suffix array. This will take O(nn'log(M)/M). To delete some text it suffices to perform an O(n) pass over the array eliminating all the text positions which lie in the deleted areas. As can be seen, the construction process is in practice more costly for suffix arrays than for inverted files. The construction of the supra-index consists of a fast final sequential pass over the suffix array. Indexing times for 250 Mb of text are close to 0.8 Mb/min on the reference machine. This is five to ten times slower than the construction of inverted indices.    OTHER INDICES FOR TEXT 205 (a) (b) T I small suffix array I counters (c) small text long Suffij array I small suffix array , I counters f-.1 final suffix array Figure 8.10 A step of the suffix array c~nstruction for large texts: (a) the local suffix array is built, (b) the counters are computed, (c) the suffix arrays are merged.  ",
2136,mir,mir-2136,8.3.2 Signature Files," 8.3.2 Signature Files Signature files are word-oriented index structures based on hashing. They pose a low overhead (10% to 20% over the text size), at the cost of forcing a sequential search over the index. However, although their search complexity is linear (instead of sublinear as with the previous approaches), its constant is rather low, which makes the technique suitable for not very large texts. Nevertheless, inverted files outperform signature files for most applications. Structure A signature file uses a hash function (or 'signature') that maps words to bit masks of B bits. It divides the text in blocks of b words each. To each text block of size b, a bit mask of size B will be assigned. This mask is obtained by bitwise ORing the signatures of all the words in the text block. Hence, the signature file is no more than the sequence of bit masks of all blocks (plus a pointer to each block). The main idea is that if a word is present in a text block, then all the bits set in its signature are also set in the bit mask of the text block. Hence, whenever a bit is set in the mask of the query word and not in the mask of the text block, then the word is not present in the text block. Figure 8.11 shows an example. However, it is possible that all the corresponding bits are set even though the word is not there. This is called a false drop. The most delicate part of the design of a signature file is to ensure that the probability of a false drop is low enough while keeping the signature file as short as possible. The hash function is forced to deliver bit masks which have at least i bits set. A good model assumes that i bits are randomly set in the mask (with possible repetition). Let 0: = if B. Since each of the b words sets i bits at    206 INDEXING AND SEARCHING Block 1 Block 2 Block 3 Block 4 This is a text. A text has many words. Words are made from letters. ~ 000101 t I 100100 I ~ 101101 Text h(text) = 000101 h(many) = 110000 h(words) = 100100 Signaturefunction h(made) = 001100 h(1etters) = 100001 Figure 8.11 A signature file for our sample text cut into blocks. random, the probability that a given bit of the mask is set in a word signature is 1 - (1 1/ B)bl ~ 1 eba . Hence, the probability that the f. random bits set in the query are also set in the mask of the text block is which is minimized for a = In(2)/b. The false drop probability under the optimal selection f. = Bln(2)/b is (1/2 In ( 2 ») B / b = 1/2 1 • Hence, a reasonable proportion B/b must be determined. The space overhead of the index is approximately (1/80) x (B/b) because B is measured in bits and b in words. Then, the false drop probability is a function of the overhead to pay. For instance, a 10% overhead implies a false drop probability close to 2%, while a 20% overhead errs with probability 0.046%. This error probability corresponds to the expected amount of sequential searching to perform while checking if a match is a false drop or not. Searching Searching a single word is carried out by hashing it to a bit mask W, and then comparing the bit masks B, of all the text blocks. Whenever (W &amp; B, = W), where &amp; is the bitwise AND, all the bits set in W are also set in B, and therefore the text block may contain the word. Hence, for all candidate text blocks, an online traversal must be performed to verify if the word is actually there. This traversal cannot be avoided as in inverted files (except if the risk of a false drop is accepted). No other types of patterns can be searched in this scheme. On the other hand, the scheme is more efficient to search phrases and reasonable proximity queries. This is because all the words must be present in a block in order for that block to hold the phrase or the proximity query. Hence, the bitwise OR of all the query masks is searched, so that all their bits must be present. This    BOOLEAN QUERIES 207 reduces the probability of false drops. This is the only indexing scheme which improves in phrase searching. Some care has to be exercised at block boundaries, however, to avoid missing a phrase which crosses a block limit. To allow searching phrases of j words or proximities of up to j words, consecutive blocks must overlap in j words. If the blocks correspond to retrieval units, simple Boolean conjunctions involving words or phrases can also be improved by forcing all the relevant words to be in the block. We were only able to find real performance estimates from 1992, run on a Sun 3/50 with local disk. Queries on a small 2.8 Mb database took 0.42 seconds. Extrapolating to today's technology, we find that the performance should be close to 20 Mb/sec (recall that it is linear time), and hence the example of 250 Mb of text would take 12 seconds, which is quite slow. Construction The construction of a signature file is rather easy. The text is simply cut in blocks, and for each block an entry of the signature file is generated. This entry is the bitwise OR of the signatures of all the words in the block. Adding text is also easy, since it is only necessary to keep adding records to the signature file. Text deletion is carried out by deleting the appropriate bit masks. Other storage proposals exist apart from storing all the bit masks in sequence. For instance, it is possible to make a different file for each bit of. the mask, i.e. one file holding all the first bits, another file for all the second bits, etc. This reduces the disk times to search for a query, since only the files corresponding to the l bits which are set in the query have to be traversed.  ",
2137,mir,mir-2137,8.4 Boolean Queries," 8.4 Boolean Queries We now cover set manipulation algorithms. These algorithms are used when operating on sets of results, which is the case in Boolean queries. Boolean queries are described in Chapter 4, where the concept of query syntax tree is defined. Once the leaves of the query syntax tree are solved (using the algorithms to find the documents containing the basic queries given), the relevant documents must be worked on by composition operators. Normally the search proceeds in three phases: the first phase determines which documents classify, the second determines the relevance of the classifying documents so as to present them appropriately to the user, and the final phase retrieves the exact positions of the matches to highlight them in those documents that the user actually wants to see. This scheme avoids doing unnecessary work on documents which will not classify at last (first phase), or will not be read at last (second phase). However, some phases can be merged if doing the extra operations is not expensive. Some phases may not be present at all in some scenarios.    208 INDEXING AND SEARCHING A/lIl (a) / ""\14' 011 /""\141 1"" (b) 14\. .UI1 .. A/lIl AID A/lIl AND r &gt;. r &gt;. r &gt;. r &gt;. 0lIl1. OR.. 0111\. OR. /""\- /""\- /""\/""\AND. MDI / -, / ""\OIl • 011 r / &lt;, / -, Figure 8.12 Processing the internal nodes of the query syntax tree. In (a) full evaluation is used. In (b) we show lazy evaluation in more detail. Once the leaves of the query syntax tree find the classifying sets of documents, these sets are further operated by the internal nodes of the tree. It is possible to algebraically optimize the tree using identities such as a OR (a AN D b) = a, for instance, or sharing common subexpressions, but we do not cover this issue here. As all operations need to pair the same document in both their operands, it is good practice to keep the sets sorted, so that operations like intersection, union, etc. can proceed sequentially on both lists and also generate a sorted list. Other representations for sets not consisting of the list of matching documents (such as bit vectors) are also possible. Under this scheme, it is possible to evaluate the syntax tree in full or lazy form. In the full evaluation form, both operands are first completely obtained and then the complete result is generated. In lazy evaluation, results are delivered only when required, and to obtain that result some data is recursively required to both operands. Full evaluation allows some optimizations to be performed because the sizes of the results are known in advance (for instance, merging a very short list against a very long one can proceed by binary searching the elements of the short list in the long one). Lazy evaluation, on the other hand, allows the application to control when to do the work of obtaining new results, instead of blocking it for a long time. Hybrid schemes are possible, for example obtain all the leaves at once and then proceed in lazy form. This may be useful, for instance, to implement some optimizations or to ensure that all the accesses to the index are sequential (thus reducing disk seek times). Figure 8.12 illustrates this. The complexity of solving these types of queries, apart from the cost of obtaining the results at the leaves, is normally linear in the total size of all the intermediate results. This is why this time may dominate the others, when there are huge intermediate results. This is more noticeable to the user when the final result is small.    SEQUENTIAL SEARCHING 209 ,a,b,r,a,c,alb,r,a,cla,d,a,b,r,a, r a cad a bra cad a bra Figure 8.13 Brute-force search algorithm for the pattern 'abracadabra.' Squared areas show the comparisons performed.  ",
2193,iir,iir-2193,8.4 Evaluation of ranked retrieval results," 8.4 Evaluation of ranked retrieval results Precision, recall, and the F measure are set-based measures. They are computed using unordered sets of documents. We need to extend these measures (or to define new measures) if we are to evaluate the ranked retrieval results that are now standard with search engines. In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top kretrieved documents. For each such set, precision and recall values can be plotted to give a precision-recall curve, such as the one shown in Figure 8.2 . Precision-recall curves have a distinctive saw-tooth shape: if the (k+1)th document retrieved is nonrelevant then recall is the same as for the top k documents, but precision has dropped. If it is relevant, then both precision and recall increase, and the curve jags up and to the right. It is often useful to remove these jiggles and the standard way to do this is with an interpolated precision: the interpolated precision pinterp at a certain recall level ris defined   8.4 Evaluation of ranked retrieval results 159 Recall Interp. Precision 0.0 1.00 0.1 0.67 0.2 0.63 0.3 0.55 0.4 0.45 0.5 0.41 0.6 0.36 0.7 0.29 0.8 0.13 0.9 0.10 1.0 0.08 ◮Table 8.1 Calculation of 11-point Interpolated Average Precision. This is for the precision-recall curve shown in Figure 8.2. as the highest precision found for any recall level r′≥r: pinterp(r) = max r′≥rp(r′)(8.7) The justification is that almost anyone would be prepared to look at a few more documents if it would increase the percentage of the viewed set that were relevant (that is, if the precision of the larger set is higher). Interpolated precision is shown by a thinner line in Figure 8.2. With this definition, the interpolated precision at a recall of 0 is well-defined (Exercise 8.4). Examining the entire precision-recall curve is very informative, but there is often a desire to boil this information down to a few numbers, or perhaps even a single number. The traditional way of doing this (used for instance in the first 8 TREC Ad Hoc evaluations) is the 11-point interpolated average precision. For each information need, the interpolated precision is measured at the 11 recall levels of 0.0, 0.1, 0.2, . .., 1.0. For the precision-recall curve in Figure 8.2, these 11 values are shown in Table 8.1. For each recall level, we then calculate the arithmetic mean of the interpolated precision at that recall level for each information need in the test collection. A composite precisionrecall curve showing 11 points can then be graphed. Figure 8.3 shows an example graph of such results from a representative good system at TREC 8. In recent years, other measures have become more common. Most standard among the TREC community is Mean Average Precision (MAP), which provides a single-figure measure of quality across recall levels. Among evaluation measures, MAP has been shown to have especially good discrimination and stability. For a single information need, Average Precision is the      160 8 Evaluation in information retrieval 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Recall Precision ◮Figure 8.3 Averaged 11-point precision/recall graph across 50 queries for a representative TREC system. The Mean Average Precision for this system is 0.2553. average of the precision value obtained for the set of top kdocuments existing after each relevant document is retrieved, and this value is then averaged over information needs. That is, if the set of relevant documents for an information need qj∈Qis {d1, . . . dmj}and Rjk is the set of ranked retrieval results from the top result until you get to document dk, then MAP(Q) = 1 |Q| |Q| ∑ j=1 1 mj mj ∑ k=1 Precision(Rjk) (8.8) When a relevant document is not retrieved at all,1the precision value in the above equation is taken to be 0. For a single information need, the average precision approximates the area under the uninterpolated precision-recall curve, and so the MAP is roughly the average area under the precision-recall curve for a set of queries. Using MAP, fixed recall levels are not chosen, and there is no interpolation. The MAP value for a test collection is the arithmetic mean of average 1. A system may not fully order all documents in the collection in response to a query or at any rate an evaluation exercise may be based on submitting only the top kresults for each information need.   8.4 Evaluation of ranked retrieval results 161 precision values for individual information needs. (This has the effect of weighting each information need equally in the final reported number, even if many documents are relevant to some queries whereas very few are relevant to other queries.) Calculated MAP scores normally vary widely across information needs when measured within a single system, for instance, between 0.1 and 0.7. Indeed, there is normally more agreement in MAP for an individual information need across systems than for MAP scores for different information needs for the same system. This means that a set of test information needs must be large and diverse enough to be representative of system effectiveness across different queries. The above measures factor in precision at all recall levels. For many prominent applications, particularly web search, this may not be germane to users. What matters is rather how many good results there are on the first page or the first three pages. This leads to measuring precision at fixed low levels of retrieved results, such as 10 or 30 documents. This is referred to as “Precision at k”, for example “Precision at 10”. It has the advantage of not requiring any estimate of the size of the set of relevant documents but the disadvantages that it is the least stable of the commonly used evaluation measures and that it does not average well, since the total number of relevant documents for a query has a strong influence on precision at k. An alternative, which alleviates this problem, is R-precision. It requires having a set of known relevant documents Rel, from which we calculate the precision of the top Rel documents returned. (The set Rel may be incomplete, such as when Rel is formed by creating relevance judgments for the pooled top kresults of particular systems in a set of experiments.) R-precision adjusts for the size of the set of relevant documents: A perfect system could score 1 on this metric for each query, whereas, even a perfect system could only achieve a precision at 20 of 0.4 if there were only 8 documents in the collection relevant to an information need. Averaging this measure across queries thus makes more sense. This measure is harder to explain to naive users than Precision at kbut easier to explain than MAP. If there are |Rel| relevant documents for a query, we examine the top |Rel|results of a system, and find that rare relevant, then by definition, not only is the precision (and hence R-precision) r/|Rel|, but the recall of this result set is also r/|Rel|. Thus, R-precision turns out to be identical to the break-even point, another measure which is sometimes used, defined in terms of this equality relationship holding. Like Precision at k, R-precision describes only one point on the precision-recall curve, rather than attempting to summarize effectiveness across the curve, and it is somewhat unclear why you should be interested in the break-even point rather than either the best point on the curve (the point with maximal F-measure) or a retrieval level of interest to a particular application (Precision at k). Nevertheless, R-precision turns out to be highly correlated with MAP empirically, despite measuring only a single point on      162 8 Evaluation in information retrieval 0.0 0.2 0.4 0.6 0.8 1.0 0 0.2 0.4 0.6 0.8 1 1 − specificity sensitivity ( = recall) ◮Figure 8.4 The ROC curve corresponding to the precision-recall curve in Figure 8.2. . the curve. Another concept sometimes used in evaluation is an ROC curve. (“ROC” stands for “Receiver Operating Characteristics”, but knowing that doesn’t help most people.) An ROC curve plots the true positive rate or sensitivity against the false positive rate or (1 −specificity). Here, sensitivity is just another term for recall. The false positive rate is given by f p/(f p +tn). Figure 8.4 shows the ROC curve corresponding to the precision-recall curve in Figure 8.2. An ROC curve always goes from the bottom left to the top right of the graph. For a good system, the graph climbs steeply on the left side. For unranked result sets, specificity, given by tn/(f p +tn), was not seen as a very useful notion. Because the set of true negatives is always so large, its value would be almost 1 for all information needs (and, correspondingly, the value of the false positive rate would be almost 0). That is, the “interesting” part of Figure 8.2 is 0 &lt;recall &lt;0.4, a part which is compressed to a small corner of Figure 8.4. But an ROC curve could make sense when looking over the full retrieval spectrum, and it provides another way of looking at the data. In many fields, a common aggregate measure is to report the area under the ROC curve, which is the ROC analog of MAP. Precision-recall curves are sometimes loosely referred to as ROC curves. This is understandable, but not accurate. A final approach that has seen increasing adoption, especially when employed with machine learning approaches to ranking (see Section 15.4, page 341) is measures of cumulative gain, and in particular normalized discounted cumulative gain (NDCG). NDCG is designed for situations of non-binary notions  of relevance (cf. Section 8.5.1). Like precision at k, it is evaluated over some number kof top search results. For a set of queries Q, let R(j,d)be the relevance score assessors gave to document dfor query j. Then, NDCG(Q,k) = 1 |Q| |Q| ∑ j=1 Zkj k ∑ m=1 2R(j,m)−1 log2(1+m), (8.9) where Zkj is a normalization factor calculated to make it so that a perfect ranking’s NDCG at kfor query jis 1. For queries for which k′&lt;kdocuments are retrieved, the last summation is done up to k′. ?Exercise 8.4 [⋆] What are the possible values for interpolated precision at a recall level of 0? Exercise 8.5 [⋆⋆] Must there always be a break-even point between precision and recall? Either show there must be or give a counter-example. Exercise 8.6 [⋆⋆] What is the relationship between the value of F1and the break-even point? Exercise 8.7 [⋆⋆] The Dice coefficient of two sets is a measure of their intersection scaled by their size (giving a value in the range 0 to 1): Dice(X,Y) = 2|X∩Y| |X|+|Y| Show that the balanced F-measure (F1) is equal to the Dice coefficient of the retrieved and relevant document sets. Exercise 8.8 [⋆] Consider an information need for which there are 4 relevant documents in the collection. Contrast two systems run on this collection. Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result): System 1 R N R N N N N N R R System 2 N R N N R R R N N N a. What is the MAP of each system? Which has a higher MAP? b. Does this result intuitively make sense? What does it say about what is important in getting a good MAP score? c. What is the R-precision of each system? (Does it rank the systems the same as MAP?)     164 8 Evaluation in information retrieval Exercise 8.9 [⋆⋆] The following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents. The top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant documents. Assume that there are 8 relevant documents in total in the collection. R R N N N N N N R N R N N N R N N N N R a. What is the precision of the system on the top 20? b. What is the F1on the top 20? c. What is the uninterpolated precision of the system at 25% recall? d. What is the interpolated precision at 33% recall? e. Assume that these 20 documents are the complete result set of the system. What is the MAP for the query? Assume, now, instead, that the system returned the entire 10,000 documents in a ranked list, and these are the first 20 results returned. f. What is the largest possible MAP that this system could have? g. What is the smallest possible MAP that this system could have? h. In a set of experiments, only the top 20 results are evaluated by hand. The result in (e) is used to approximate the range (f)–(g). For this example, how large (in absolute terms) can the error for the MAP be by calculating (e) instead of (f) and (g) for this query?  ",8.4
2194,iir,iir-2194,8.5 Assessing relevance," 8.5 Assessing relevance To properly evaluate a system, your test information needs must be germane to the documents in the test document collection, and appropriate for predicted usage of the system. These information needs are best designed by domain experts. Using random combinations of query terms as an information need is generally not a good idea because typically they will not resemble the actual distribution of information needs. Given information needs and documents, you need to collect relevance assessments. This is a time-consuming and expensive process involving human beings. For tiny collections like Cranfield, exhaustive judgments of relevance for each query and document pair were obtained. For large modern collections, it is usual for relevance to be assessed only for a subset of the documents for each query. The most standard approach is pooling, where relevance is assessed over a subset of the collection that is formed from the top kdocuments returned by a number of different IR systems (usually the ones to be evaluated), and perhaps other sources such as the results of Boolean keyword searches or documents found by expert searchers in an interactive process.   8.5 Assessing relevance 165 Judge 2 Relevance Yes No Total Judge 1 Yes 300 20 320 Relevance No 10 70 80 Total 310 90 400 Observed proportion of the times the judges agreed P(A) = (300 +70)/400 =370/400 =0.925 Pooled marginals P(nonrelevant) = (80 +90)/(400 +400) = 170/800 =0.2125 P(relevant) = (320 +310)/(400 +400) = 630/800 =0.7878 Probability that the two judges agreed by chance P(E) = P(nonrelevant)2+P(relevant)2=0.21252+0.78782=0.665 Kappa statistic κ= (P(A)−P(E))/(1−P(E)) = (0.925 −0.665)/(1−0.665) = 0.776 ◮Table 8.2 Calculating the kappa statistic. A human is not a device that reliably reports a gold standard judgment of relevance of a document to a query. Rather, humans and their relevance judgments are quite idiosyncratic and variable. But this is not a problem to be solved: in the final analysis, the success of an IR system depends on how good it is at satisfying the needs of these idiosyncratic humans, one information need at a time. Nevertheless, it is interesting to consider and measure how much agreement between judges there is on relevance judgments. In the social sciences, a common measure for agreement between judges is the kappa statistic. It is designed for categorical judgments and corrects a simple agreement rate for the rate of chance agreement. kappa =P(A)−P(E) 1−P(E) (8.10) where P(A)is the proportion of the times the judges agreed, and P(E)is the proportion of the times they would be expected to agree by chance. There are choices in how the latter is estimated: if we simply say we are making a two-class decision and assume nothing more, then the expected chance agreement rate is 0.5. However, normally the class distribution assigned is skewed, and it is usual to use marginal statistics to calculate expected agreement.2There are still two ways to do it depending on whether one pools 2. For a contingency table, as in Table 8.2, a marginal statistic is formed by summing a row or column. The marginal ai.k=∑jaijk.     166 8 Evaluation in information retrieval the marginal distribution across judges or uses the marginals for each judge separately; both forms have been used, but we present the pooled version because it is more conservative in the presence of systematic differences in assessments across judges. The calculations are shown in Table 8.2. The kappa value will be 1 if two judges always agree, 0 if they agree only at the rate given by chance, and negative if they are worse than random. If there are more than two judges, it is normal to calculate an average pairwise kappa value. As a rule of thumb, a kappa value above 0.8 is taken as good agreement, a kappa value between 0.67 and 0.8 is taken as fair agreement, and agreement below 0.67 is seen as data providing a dubious basis for an evaluation, though the precise cutoffs depend on the purposes for which the data will be used. Interjudge agreement of relevance has been measured within the TREC evaluations and for medical IR collections. Using the above rules of thumb, the level of agreement normally falls in the range of “fair” (0.67–0.8). The fact that human agreement on a binary relevance judgment is quite modest is one reason for not requiring more fine-grained relevance labeling from the test set creator. To answer the question of whether IR evaluation results are valid despite the variation of individual assessors’ judgments, people have experimented with evaluations taking one or the other of two judges’ opinions as the gold standard. The choice can make a considerable absolute difference to reported scores, but has in general been found to have little impact on the relative effectiveness ranking of either different systems or variants of a single system which are being compared for effectiveness.  ",8.5
2138,mir,mir-2138,8.5 Sequential Searching," 8.5 Sequential Searching We now cover the algorithms for text searching when no data structure has been built on the text. As shown, this is a basic part of some indexing techniques as well as the only option in some cases. We cover exact string matching in this section. Later we cover matching of more complex patterns. Our exposition is mainly conceptual and the implementation details are not shown (see the Bibliographic Discussion at the end of this chapter for more information). The problem of exact string matching is: given a short pattern P of length m and a long text T of length n, find all the text positions where the pattern occurs. With minimal changes this problem subsumes many basic queries, such as word, prefix, suffix, and substring search. This is a classical problem for which a wealth of solutions exists. We sketch the main algorithms, and leave aside a lot of the theoretical work that is not competitive in practice. For example, we do not include the Karp-Rabin algorithm, which is a nice application of hashing to string searching, but is not practical. We also briefly cover multipattern algorithms (that search many patterns at once), since a query may have many patterns and it may be more efficient to retrieve them all at once. Finally, we also mention how to do phrases and proximity searches. We assume that the text and the pattern are sequences of characters drawn from an alphabet of size a, whose first character is at position 1\. The averagecase analysis assumes random text and patterns. 8.5.1 Brute Force The brute-force (BF) algorithm is the simplest possible one. It consists of merely trying all possible pattern positions in the text. For each such position, it verifies whether the pa ",
2139,mir,mir-2139,8.5.1 Brute Force," 8.5.1 Brute Force The brute-force (BF) algorithm is the simplest possible one. It consists of merely trying all possible pattern positions in the text. For each such position, it verifies whether the pattern matches at that position. See Figure 8.13. Since there are O(n) text positions and each one is examined at O(m) worstcase cost, the worst-case of brute-force searching is O(mn). However, its average    r 210 INDEXING AND SEARCHING case is O(n) (since on random text a mismatch is found after 0(1) comparisons on average). This algorithm does not need any pattern preprocessing. Many algorithms use a modification of this scheme. There is a window of length m which is slid over the text. It is checked whether the text in the window is equal to the pattern (if it is, the window position is reported as a match). Then, the window is shifted forward. The algorithms mainly differ in the way they check and shift the window. 8.5.2 Knuth-Marris-Pratt The KMP algorithm was the first with linear worst-case behavior, although on average it is not much faster than BF. This algorithm also slides a window over the text. However, it does not try all window positions as BF does. Instead, it reuses information from previous checks. After the window is checked, whether it matched the pattern or not, a number of pattern letters were compared to the text window, and they all matched except possibly the last one compared. Hence, when the window has to be shifted, there is a prefix of the pattern that matched the text. The algorithm takes advantage of this information to avoid trying window positions which can be deduced not to match. The pattern is preprocessed in O( m) time and space to build a table called next. The next table at position j says which is the longest proper prefix of P l .. j \- l which is also a suffix: and the characters following prefix and suffix are different. Hence j \- nextfj] \+ 1 window positions can be safely skipped if the characters up to j \- 1 matched, and the j-th did not. For instance, when searching the word 'abracadabra,' if a text window matched up to 'abracab,' five positions can be safely skipped since next[7] = 1\. Figure 8.14 shows an example. The crucial observation is that this information depends only on the pattern, because if the text in the window matched up to position j \- 1, then that text is equal to the pattern. The algorithm moves a window over the text and a pointer inside the window. Each time a character matches, the pointer is advanced (a match is reported if the pointer reaches the end of the window). Each time a character is not matched, the window is shifted forward in the text, to the position given by next, but the pointer position in the text does not change. Since at each text comparison the window or the pointer advance by at least one position, the algorithm performs at most 2n comparisons (and at least n). The Aho-Corasick algorithm can be regarded as an extension of KMP in matching a set of patterns. The patterns are arranged in a trie-like data structure. Each trie node represents having matched a prefix of some pattern(s). The next function is replaced by a more general set of failure transitions. Those transitions go between nodes of the trie. A transition leaving from a node representing the prefix x leads to a node representing a prefix y, such that y is the longest prefix in the set of patterns which is also a proper suffix of x. Figure 8.15 illustrates this.  ",
2195,iir,iir-2195,8.5.1 Critiques and justifications of the concept of relevance," 8.5.1 Critiques and justifications of the concept of relevance The advantage of system evaluation, as enabled by the standard model of relevant and nonrelevant documents, is that we have a fixed setting in which we can vary IR systems and system parameters to carry out comparative experiments. Such formal testing is much less expensive and allows clearer diagnosis of the effect of changing system parameters than doing user studies of retrieval effectiveness. Indeed, once we have a formal measure that we have confidence in, we can proceed to optimize effectiveness by machine learning methods, rather than tuning parameters by hand. Of course, if the formal measure poorly describes what users actually want, doing this will not be effective in improving user satisfaction. Our perspective is that, in practice, the standard formal measures for IR evaluation, although a simplification, are good enough, and recent work in optimizing formal evaluation measures in IR has succeeded brilliantly. There are numerous examples of techniques developed in formal evaluation settings, which improve effectiveness in operational settings, such as the development of document length normalization methods within the context of TREC (Sections 6.4.4 and 11.4.3)     8.5 Assessing relevance 167 and machine learning methods for adjusting parameter weights in scoring (Section 6.1.2). That is not to say that there are not problems latent within the abstractions used. The relevance of one document is treated as independent of the relevance of other documents in the collection. (This assumption is actually built into most retrieval systems – documents are scored against queries, not against each other – as well as being assumed in the evaluation methods.) Assessments are binary: there aren’t any nuanced assessments of relevance. Relevance of a document to an information need is treated as an absolute, objective decision. But judgments of relevance are subjective, varying across people, as we discussed above. In practice, human assessors are also imperfect measuring instruments, susceptible to failures of understanding and attention. We also have to assume that users’ information needs do not change as they start looking at retrieval results. Any results based on one collection are heavily skewed by the choice of collection, queries, and relevance judgment set: the results may not translate from one domain to another or to a different user population. Some of these problems may be fixable. A number of recent evaluations, including INEX, some TREC tracks, and NTCIR have adopted an ordinal notion of relevance with documents divided into 3 or 4 classes, distinguishing slightly relevant documents from highly relevant documents. See Section 10.4 (page 210) for a detailed discussion of how this is implemented in the INEX evaluations. One clear problem with the relevance-based assessment that we have presented is the distinction between relevance and marginal relevance: whether a document still has distinctive usefulness after the user has looked at certain other documents (Carbonell and Goldstein 1998). Even if a document is highly relevant, its information can be completely redundant with other documents which have already been examined. The most extreme case of this is documents that are duplicates – a phenomenon that is actually very common on the World Wide Web – but it can also easily occur when several documents provide a similar precis of an event. In such circumstances, marginal relevance is clearly a better measure of utility to the user. Maximizing marginal relevance requires returning documents that exhibit diversity and novelty. One way to approach measuring this is by using distinct facts or entities as evaluation units. This perhaps more directly measures true utility to the user but doing this makes it harder to create a test collection. ?Exercise 8.10 [⋆⋆] Below is a table showing how two human judges rated the relevance of a set of 12 documents to a particular information need (0 = nonrelevant, 1 = relevant). Let us assume that you’ve written an IR system that for this query returns the set of documents {4, 5, 6, 7, 8}.     168 8 Evaluation in information retrieval docID Judge 1 Judge 2 1 0 0 2 0 0 3 1 1 4 1 1 5 1 0 6 1 0 7 1 0 8 1 0 9 0 1 10 0 1 11 0 1 12 0 1 a. Calculate the kappa measure between the two judges. b. Calculate precision, recall, and F1of your system if a document is considered relevant only if the two judges agree. c. Calculate precision, recall, and F1of your system if a document is considered relevant if either judge thinks it is relevant.  ",8.5
2140,mir,mir-2140,8.5.2 Knuth-Morris-Pratt,"  r 210 INDEXING AND SEARCHING case is O(n) (since on random text a mismatch is found after 0(1) comparisons on average). This algorithm does not need any pattern preprocessing. Many algorithms use a modification of this scheme. There is a window of length m which is slid over the text. It is checked whether the text in the window is equal to the pattern (if it is, the window position is reported as a match). Then, the window is shifted forward. The algorithms mainly differ in the way they check and shift the window. 8.5.2 Knuth-Marris-Pratt The KMP algorithm was the first with linear worst-case behavior, although on average it is not much faster than BF. This algorithm also slides a window over the text. However, it does not try all window positions as BF does. Instead, it reuses information from previous checks. After the window is checked, whether it matched the pattern or not, a number of pattern letters were compared to the text window, and they all matched except possibly the last one compared. Hence, when the window has to be shifted, there is a prefix of the pattern that matched the text. The algorithm takes advantage of this information to avoid trying window positions which can be deduced not to match. The pattern is preprocessed in O( m) time and space to build a table called next. The next table at position j says which is the longest proper prefix of P l .. j \- l which is also a suffix: and the characters following prefix and suffix are different. Hence j \- nextfj] \+ 1 window positions can be safely skipped if the characters up to j \- 1 matched, and the j-th did not. For instance, when searching the word 'abracadabra,' if a text window matched up to 'abracab,' five positions can be safely skipped since next[7] = 1\. Figure 8.14 shows an example. The crucial observation is that this information depends only on the pattern, because if the text in the window matched up to position j \- 1, then that text is equal to the pattern. The algorithm moves a window over the text and a pointer inside the window. Each time a character matches, the pointer is advanced (a match is reported if the pointer reaches the end of the window). Each time a character is not matched, the window is shifted forward in the text, to the position given by next, but the pointer position in the text does not change. Since at each text comparison the window or the pointer advance by at least one position, the algorithm performs at most 2n comparisons (and at least n). The Aho-Corasick algorithm can be regarded as an extension of KMP in matching a set of patterns. The patterns are arranged in a trie-like data structure. Each trie node represents having matched a prefix of some pattern(s). The next function is replaced by a more general set of failure transitions. Those transitions go between nodes of the trie. A transition leaving from a node representing the prefix x leads to a node representing a prefix y, such that y is the longest prefix in the set of patterns which is also a proper suffix of x. Figure 8.15 illustrates this.    next = 0 0 0 0 1 0 1 0 0 0 0 4 '0~ I SEQUENTIAL SEARCHING 211 r\----, la bra cad! .-=b::.l.-r-a-c-a--d-a-b-r-a', Figure 8.14 KMP algorithm searching 'abracadabra.' On the left, an illustration of the next function. Notice that after matching 'abracada' we do not try to match the last 'a' with the first one since what follows cannot be a 'b.' On the right, a search example. Grayed areas show the prefix information reused. Figure 8.15 Aho-Corasick trie example for the set 'hello,' 'elbow' and 'eleven' showing only one of all the failure transitions. This trie, together with its failure transitions, is built in O(m) time and space (where m is the total length of all the patterns). Its search time is O(n) no matter how many patterns are searched. Much as KMP, it makes at most 2n inspections.  ",
2141,mir,mir-2141,8.5.3 Boyer-Moore Family," 8.5.3 Boyer-Moore Family BM algorithms are based on the fact that the check inside the window can proceed backwards. When a match or mismatch is determined, a suffix of the pattern has been compared and found equal to the text in the window. This can be used in a way very similar to the next table of KMP, i.e. compute for every pattern position j the next-to-last occurrence of P j .. m inside P. This is called the 'match heuristic.' This is combined with what is called the 'occurrence heuristic.' It states that the text character that produced the mismatch (if a mismatch occurred) has to be aligned with the same character in the pattern after the shift. The heuristic which gives the longest shift is selected. For instance, assume that 'abracadabra' is searched in a text which starts with 'abracaQabra.' After matching the suffix 'abra' the underlined text character 'b' will cause a mismatch. The match heuristic states that since 'abra' was matched a shift of 7 is safe. The occurrence heuristic states that since the underlined 'b' must match the pattern, a shift of 5 is safe. Hence, the pattern is    212 INDEXING AND SEARCHING lalblrlaIClalblrla,Clald,alb,rlal ~ 1'\--~-'b'\-----r-a--'5II!-:d'-a-:;b~r-a I Figure 8.16 BM algorithm searching 'abracadabra.' Squared areas show the comparisons performed. Grayed areas have already been compared (but the algorithm compares them again). The dashed box shows the match heuristic, which was not chosen. shifted by 7\. See Figure 8.16. The preprocessing time and space of this algorithm is O(m+a). Its search time is O(nlog(m)/m) on average, which is 'sublinear' in the sense that not all characters are inspected. On the other hand, its worst case is O( mn) (unlike KMP, the old suffix information is not kept to avoid further comparisons). Further simplifications of the BM algorithm lead to some of the fastest algorithms on average. The Simplified BM algorithm uses only the occurrence heuristic. This obtains almost the same shifts in practice. The BM-Horspool (BMH) algorithm does the same, but it notices that it is not important any more that the check proceeds backwards, and uses the occurrence heuristic on the last character of the window instead of the one that caused the mismatch. This gives longer shifts on average. Finally, the BM-Sunday (BMS) algorithm modifies BMH by using the character following the last one, which improves the shift especially on short patterns. The Commentz- Walter algorithm is an extension of BM to multipattern search. It builds a trie on the reversed patterns, and instead of a backward window check, it enters into the trie with the window characters read backwards. A shift function is computed by a natural extension of BM. In general this algorithm improves over Aho-Corasick for not too many patterns.  ",
2142,mir,mir-2142,8.5.4 Shift-Or," 8.5.4 Shift-Or Shift-Or is based on bit-parallelism. This technique involves taking advantage of the intrinsic parallelism of the bit operations inside a computer word (of w bits). By cleverly using this fact, the number of operations that an algorithm performs can be cut by a factor of at most w. Since in current architectures w is 32 or 64, the speedup is very significant in practice. The Shift-Or algorithm uses bit-parallelism to simulate the operation of a non-deterministic automaton that searches the pattern in the text (see Figure 8.17). As this automaton is simulated in time O( mn), the Shift-Or algorithm achieves O(mn/w) worst-case time (optimal speedup). The algorithm first builds a table B which for each character stores a bit mask bm...b 1 . The mask in B[c] has the i-th bit set to zero if and only if    SEQUENTIAL SEARCHING 213 B[a] = 1 0 0 1 0 1 0 1 0 0 1 B[b] = O· 1 0 0 0 0 0 0 1 0 0 B[r] = 0 0 1 0 0 0 0 0 0 1 0 B[c] = 0 0 0 0 1 0 0 0 0 0 0 B[d] = 0 0 0 0 0 0 1 0 0 0 0 Bf*l= 0 000 00 0 0 0 0 0 Figure 8.17 Non-deterministic automaton that searches 'abracadabra,' and the associated B table. The initial self-loop matches any character. Each table column corresponds to an edge of the automaton. Pi = c (see Figure 8.17). The state of the search is kept in a machine word D = d m ... d ll where d; is zero whenever the state numbered i in Figure 8.17 is active. Therefore, a match is reported whenever d m is zero. In the following, we use 'I' to denote the bitwise OR and '&amp;' to denote the bitwise AND. D is set to all ones originally, and for each new text character T j , D is updated using the formula D' ..(D« 1) I B[T j ] (where '«' means shifting all the bits in D one position to the left and setting the rightmost bit to zero). It is not hard to relate the formula to the movement that occurs in the non-deterministic automaton for each new text character. For patterns longer than the computer word (i.e., m &gt; w), the algorithm uses r mf u: 1 computer words for the simulation (not all them are active all the time). The algorithm is O(n) on average and the preprocessing is O(m+O') time and 0(0') space. It is easy to extend Shift-Or to handle classes of characters by manipulating the B table and keeping the search algorithm unchanged. This paradigm also can search a large set of extended patterns, as well as multiple patterns (where the complexity is the same as before if we consider that m is the total length of all the patterns).  ",
2143,mir,mir-2143,8.5.5 Suffix Automaton," 8.5.5 Suffix Automaton The Backward DAWG matching (BDM) algorithm is based on a suffix automaton. A suffix automaton on a pattern P is an automaton that recognizes all the suffixes of P. The non-deterministic version of this automaton has a very regular structure and is shown in Figure 8.18. The BDM algorithm converts this automaton to deterministic. The size and construction time of this automaton is O(m). This is basically the preprocessing effort of the algorithm. Each path from the initial node to any internal    214 INDEXING AND SEARCHING Figure 8.18 A non-deterministic suffix automaton. Dashed lines represent etransitions (i.e., they occur without consuming any input). I is the initial state of the automaton. xl x xl Figure 8.19 The BDM algorithm for the pattern 'abracadabra.' The rectangles represent elements compared to the text window. The Is show the positions where a pattern prefix was recognized. node represents a substring of the pattern. The final nodes represent pattern suffixes. To search a pattern P, the suffix automaton of P"" (the reversed pattern) is built. The algorithm searches backwards inside the text window for a substring of the pattern P using the suffix automaton. Each time a terminal state is reached before hitting the beginning of the window, the position inside the window is remembered. This corresponds to finding a prefix of the pattern equal to a suffix of the window (since the reverse suffixes of P"" are the prefixes of P). The last prefix recognized backwards is the longest prefix of P in the window. A match is found if the complete window is read, while the check is abandoned when there is no transition to follow in the automaton. In either case, the window is shifted to align with the longest prefix recognized. See Figure 8.19. This algorithm is O(mn) time in the worst case and O(n log(m)/m) on average. There exists also a multipattern version of this algorithm called MultiBDM, which is the fastest for many patterns or very long patterns. BDM rarely beats the best BM algorithms. However, a recent bit-parallel implementation called BNDM improves over BM in a wide range of cases. This algorithm simulates the non-deterministic suffix automaton using bit-parallelism. The algorithm supports some extended patterns and other applications mentioned in Shift-Or, while keeping more efficient than Shift-Or.  ",
2144,mir,mir-2144,8.5.6 Practical Comparison," 8.5.6 Practical Comparison Figure 8.20 shows a practical comparison between string matching algorithms run on our reference machine. The values are correct within 5% of accuracy with a 95% confidence interval. We tested English text from the TREe collection, DNA (corresponding to 'h.influenzae') and random text uniformly generated over 64 letters. The patterns were randomly selected from the text except for random    PATTERN MATCHING 215 text, where they were randomly generated. We tested over 10 Mb of text and measured CPU time. We tested short patterns on English and random text and long patterns on DNA, which are the typical cases. We first analyze the case of random text, where except for very short patterns the clear winners are BNDM (the bit-parallel implementation of BDM) and the BMS (Sunday) algorithm. The more classical Boyer-Moore and BDM algorithms are also very close. Among the algorithms that do not improve with the pattern length, Shift-Or is the fastest, and KMP is much slower than the naive algorithm. The picture is similar for English text, except that we have included the Agrep software in this comparison, which worked well only on English text. Agrep turns out to be much faster than others. This is not because of using a special algorithm (it uses a BM-family algorithm) but because the code is carefully optimized. This shows the importance of careful coding as well as using good algorithms, especially in text searching where a few operations per text character are performed. Longer patterns are shown for a DNA text. BNDM is the fastest for moderate patterns, but since it does not improve with the length after m &gt; w, the classical BDM finally obtains better times. They are much better than the BoyerMoore family because the alphabet is small and the suffix automaton technique makes better use of the information on the pattern. We have not shown the case of extended patterns, that is, where flexibility plays a role. For this case, BNDM is normally the fastest when it can be applied (e.g., it supports classes of characters but not wild cards), otherwise Shift-Or is the best option. Shift-Or is also the best option when the text must be accessed sequentially and it is not possible to skip characters.  ",
2145,mir,mir-2145,8.5.7 Phrases and Proximity," 8.5.7 Phrases and Proximity If a sequence of words is searched to appear in the text exactly as in the pattern (i.e., with the same separators) the problem is similar to that of exact search of a single pattern, by just forgetting the fact that there are many words. If any separator between words is to be allowed, it is possible to arrange it using an extended pattern or regular expression search. The best way to search a phrase element-wise is to search for the element which is less frequent or can be searched faster (both criteria normally match). For instance, longer patterns are better than shorter ones; allowing fewer errors is better than allowing more errors. Once such an element is found, the neighboring words are checked to see if a complete match is found. A similar algorithm can be used to search a proximity query. 8.6 Pattern Matching We present in this section the main techniques to deal with complex patterns. We divide it into two main groups: searching allowing errors and searching for extended patterns.  ",
2196,iir,iir-2196,8.6 A broader perspective: System quality and user utility," 8.6 A broader perspective: System quality and user utility Formal evaluation measures are at some distance from our ultimate interest in measures of human utility: how satisfied is each user with the results the system gives for each information need that they pose? The standard way to measure human satisfaction is by various kinds of user studies. These might include quantitative measures, both objective, such as time to complete a task, as well as subjective, such as a score for satisfaction with the search engine, and qualitative measures, such as user comments on the search interface. In this section we will touch on other system aspects that allow quantitative evaluation and the issue of user utility. 8.6.1 System issues There are many practical benchmarks on which to rate an information retrieval system beyond its retrieval quality. These include: •How fast does it index, that is, how many documents per hour does it index for a certain distribution over document lengths? (cf. Chapter 4) •How fast does it search, that is, what is its latency as a function of index size? •How expressive is its query language? How fast is it on complex queries?  ",8.6
2146,mir,mir-2146,8.6 Pattern Matching," 8.6 Pattern Matching We present in this section the main techniques to deal with complex patterns. We divide it into two main groups: searching allowing errors and searching for extended patterns.    Figure 8.20 Practical comparison among algorithms. The upper left plot is for short patterns on English text. The upper right one is for long patterns on DNA. The lower plot is for short patterns on random text (on 64 letters). Times are in tenths of seconds per megabyte. 8.6.1 String Matching Allowing errors This problem (called 'approximate string matching') can be stated as follows: given a short pattern P of length m, a long text T of length n, and a maximum allowed number of errors k, find all the text positions where the pattern occurs with at most k errors. This statement corresponds to the Levenshtein distance. With minimal modifications it is adapted to searching whole words matching the pattern with k errors. This problem is newer than exact string matching, although there are already a number of solutions. We sketch the main approaches. Dynamic Programming The classical solution to approximate string matching is based on dynamic programming. A matrix C[O .. m,O .. n] is filled column by column, where C[i,j]  ",
2147,mir,mir-2147,8.6.1 String Matching Allowing Errors,"  Figure 8.20 Practical comparison among algorithms. The upper left plot is for short patterns on English text. The upper right one is for long patterns on DNA. The lower plot is for short patterns on random text (on 64 letters). Times are in tenths of seconds per megabyte. 8.6.1 String Matching Allowing errors This problem (called 'approximate string matching') can be stated as follows: given a short pattern P of length m, a long text T of length n, and a maximum allowed number of errors k, find all the text positions where the pattern occurs with at most k errors. This statement corresponds to the Levenshtein distance. With minimal modifications it is adapted to searching whole words matching the pattern with k errors. This problem is newer than exact string matching, although there are already a number of solutions. We sketch the main approaches. Dynamic Programming The classical solution to approximate string matching is based on dynamic programming. A matrix C[O .. m,O .. n] is filled column by column, where C[i,j]    PATTERN MATCHING 217 represents the minimum number of errors needed to match Pl.. i to a suffix of Tl.. j ' This is computed as follows C[O,jJ = ° C[i,OJ = i C[i,jJ = if (Pi = T j ) then Cli -1,j -IJ else 1 +min(C[i -1,jJ,C[i,j -IJ,C[i -1,j -1]) where a match is reported at text positions j such that C[m,jJ ~ k (the final positions of the occurrences are reported). Therefore, the algorithm is O( mn) time. Since only the previous column of the matrix is needed, it can be implemented in O( m) space. Its preprocessing time is Oem) . Figure 8.21 illustrates this algorithm. In recent years several algorithms have been presented that achieve O(kn) time in the worst case or even less in the average case, by taking advantage of the properties of the dynamic programming matrix (e.g., values in neighbor cells differ at most by one). Automaton It is interesting to note that the problem can be reduced to a non-deterministic finite automaton (NFA). Consider the NFA for k = 2 errors shown in Figure 8.22. Each row denotes the number of errors seen. The first one 0, the second one 1, and so on. Every column represents matching the pattern up to a given position. At each iteration, a new text character is read and the automaton changes its states. Horizontal arrows represent matching a character, vertical arrows represent insertions into the pattern, solid diagonal arrows represent replacements, and dashed diagonal arrows represent deletions in the pattern (they are s-transitions). The automaton accepts a text position as the end of a match s u r g e r y ° ° ° ° °°°° s 1 0 1 1 1 1 1 1 u 2 1 0 1 2 2 2 2 r 3 2 1 ° 1 2 2 3 v 4 3 2 1 1 2 3 3 e 5 4 3 2 2 1 2 3 y 6 5 4 3 3 2 2 2 Figure 8.21 The dynamic programming algorithm search 'survey' in the text 'surgery' with two errors. Bold entries indicate matching positions.    218 INDEXING AND SEARCHING I . 2 errors Figure 8.22 An NFA for approximate string matching of the pattern 'survey' with two errors. The shaded states are those active after reading the text 'surgery'. Unlabelled transitions match any character. with k errors whenever the (k \+ 1)-th rightmost state is active. It is not hard to see that once a state in the automaton is active, all the states of the same column and higher rows are active too. Moreover, at a given text character, if we collect the smallest active rows at each column, we obtain the current column of the dynamic programming algorithm. Figure 8.22 illustrates this (compare the figure with Figure 8.21). One solution is to make this automaton deterministic (DFA). Although the search phase is O(n), the DFA can be huge. An alternative solution is based on bit-parallelism and is explained next. Bit-Parallelism Bit-parallelism has been used to parallelize the computation of the dynamic programming matrix (achieving average complexity O(knjw)) and to parallelize the computation of the NFA (without converting it to deterministic), obtaining O(kmnjw) time in the worst case. Such algorithms achieve O(n) search time for short patterns and are currently the fastest ones in many cases, running at 6 to 10 Mb per second on our reference machine. Filtering Finally, other approaches first filter the text, reducing the area where dynamic programming needs to be used. These algorithms achieve 'sublinear' expected time in many cases for low error ratios (i.e., not all text characters are inspected,    PATTERN MATCHING 219 O( kn log, (m) / m) is a typical figure), although the filtration is not effective for more errors. Filtration is based on the fact that some portions of the pattern must appear with no errors even in an approximate occurrence. The fastest algorithm for low error levels is based on filtering: if the pattern is split into k \+ 1 pieces, any approximate occurrence must contain at least one of the pieces with no errors, since k errors cannot alter all the k \+ 1 pieces. Hence, the search begins with a multipattern exact search for the pieces and it later verifies the areas that may contain a match (using another algorithm).  ",
2197,iir,iir-2197,8.6.1 System issues," 8.6.1 System issues There are many practical benchmarks on which to rate an information retrieval system beyond its retrieval quality. These include: •How fast does it index, that is, how many documents per hour does it index for a certain distribution over document lengths? (cf. Chapter 4) •How fast does it search, that is, what is its latency as a function of index size? •How expressive is its query language? How fast is it on complex queries?     8.6 A broader perspective: System quality and user utility 169 •How large is its document collection, in terms of the number of documents or the collection having information distributed across a broad range of topics? All these criteria apart from query language expressiveness are straightforwardly measurable: we can quantify the speed or size. Various kinds of feature checklists can make query language expressiveness semi-precise.  ",8.6
2148,mir,mir-2148,8.6.2 Regular Expressions and Extended Patterns," 8.6.2 Regular Expressions and Extended Patterns General regular expressions are searched by building an automaton which finds all their occurrences in a text. This process first builds a non- deterministic finite automaton of size O(m), where m is the length of the regular expression. The classical solution is to convert this automaton to deterministic form. A deterministic automaton can search any regular expression in O(n) time. However, its size and construction time can be exponential in m, i.e. O(m2 m ) . See Figure 8.23. Excluding preprocessing, this algorithm runs at 6 Mb/sec in the reference machine. Recently the use of bit-parallelism has been proposed to avoid the construction of the deterministic automaton. The non-deterministic automaton is simulated instead. One bit per automaton state is used to represent whether the state is active or not. Due to the algorithm used to build the non- deterministic automaton, all the transitions move forward except for s-transitions. The idea is that for each text character two steps are carried out. The first one moves forward, and the second one takes care of all the s-transitions. A function E from bit masks to bit masks is precomputed so that all the corresponding bits are moved according to the e-transitions. Since this function is very large (i.e., 2 m entries) its domain is split in many functions from 8- or 16-bit submasks to m-bit masks. This is possible because E(B1 ... B j) = E(Bdl ... IE(Bj), where B, Figure 8.23 The non-deterministic (a) and deterministic (b) automata for the regular expression b bO [b I bOa).    220 INDEXING AND SEARCHING are the submasks. Hence, the scheme performs f m/81 or fm/161 operations per text character and needs fm/812 8fm/wl or fm/1612 1 6fm/wl machine words of memory. Extended patterns can be rephrased as regular expressions and solved as before. However, in many cases it is more efficient to give them a specialized solution, as we saw for the extensions of exact searching (bit-parallel algorithms). Moreover, extended patterns can be combined with approximate search for maximum flexibility. In general, the bit-parallel approach is the best equipped to deal with extended patterns. Real times for regular expressions and extended pattern searching using this technique are between 2-8 Mb/sec, 8.6.3 Pattern Matching Using Indices We end this section by explaining how the indexing techniques we presented for simple searching of words can in fact be extended to search for more complex patterns. Inverted Files Ai&gt; inverted files are word-oriented, other types of queries such as suffix or substring queries, searching allowing errors and regular expressions, are solved by a sequential (i.e., online) search over the vocabulary. This is not too bad since the size of the vocabulary is small with respect to the text size. After either type of search, a list of vocabulary words that matched the query is obtained. All their lists of occurrences are now merged to retrieve a list of documents and (if required) the matching text positions. If block addressing is used and the positions are required or the blocks do not coincide with the retrieval unit, the search must be completed with a sequential search over the blocks. Notice that an inverted index is word-oriented. Because of that it is not surprising that it is not able to efficiently find approximate matches or regular expressions that span many words. This is a restriction of this scheme. Variations that are not subject to this restriction have been proposed for languages which do not have a clear concept of word, like Finnish. They collect text samples or ngrams, which are fixed-length strings picked at regular text intervals. Searching is in general more powerful but more expensive. In a full-inverted index, search times for simple words allowing errors on 250 Mb of text took our reference machine from 0.6 to 0.85 seconds, while very complex expressions on extended patterns took from 0.8 to 3 seconds. As a comparison, the same collection cut in blocks of 1 Mb size takes more than 8 seconds for an approximate search with one error and more than 20 for two errors.  ",
2198,iir,iir-2198,8.6.2 User utility," 8.6.2 User utility What we would really like is a way of quantifying aggregate user happiness, based on the relevance, speed, and user interface of a system. One part of this is understanding the distribution of people we wish to make happy, and this depends entirely on the setting. For a web search engine, happy search users are those who find what they want. One indirect measure of such users is that they tend to return to the same engine. Measuring the rate of return of users is thus an effective metric, which would of course be more effective if you could also measure how much these users used other search engines. But advertisers are also users of modern web search engines. They are happy if customers click through to their sites and then make purchases. On an eCommerce web site, a user is likely to be wanting to purchase something. Thus, we can measure the time to purchase, or the fraction of searchers who become buyers. On a shopfront web site, perhaps both the user’s and the store owner’s needs are satisfied if a purchase is made. Nevertheless, in general, we need to decide whether it is the end user’s or the eCommerce site owner’s happiness that we are trying to optimize. Usually, it is the store owner who is paying us. For an “enterprise” (company, government, or academic) intranet search engine, the relevant metric is more likely to be user productivity: how much time do users spend looking for information that they need. There are also many other practical criteria concerning such matters as information security, which we mentioned in Section 4.6 (page 80). User happiness is elusive to measure, and this is part of why the standard methodology uses the proxy of relevance of search results. The standard direct way to get at user satisfaction is to run user studies, where people engage in tasks, and usually various metrics are measured, the participants are observed, and ethnographic interview techniques are used to get qualitative information on satisfaction. User studies are very useful in system design, but they are time consuming and expensive to do. They are also difficult to do well, and expertise is required to design the studies and to interpret the results. We will not discuss the details of human usability testing here.  ",8.6
2149,mir,mir-2149,8.6.3 Pattern Matching Using Indicies,"  220 INDEXING AND SEARCHING are the submasks. Hence, the scheme performs f m/81 or fm/161 operations per text character and needs fm/812 8fm/wl or fm/1612 1 6fm/wl machine words of memory. Extended patterns can be rephrased as regular expressions and solved as before. However, in many cases it is more efficient to give them a specialized solution, as we saw for the extensions of exact searching (bit-parallel algorithms). Moreover, extended patterns can be combined with approximate search for maximum flexibility. In general, the bit-parallel approach is the best equipped to deal with extended patterns. Real times for regular expressions and extended pattern searching using this technique are between 2-8 Mb/sec, 8.6.3 Pattern Matching Using Indices We end this section by explaining how the indexing techniques we presented for simple searching of words can in fact be extended to search for more complex patterns. Inverted Files Ai&gt; inverted files are word-oriented, other types of queries such as suffix or substring queries, searching allowing errors and regular expressions, are solved by a sequential (i.e., online) search over the vocabulary. This is not too bad since the size of the vocabulary is small with respect to the text size. After either type of search, a list of vocabulary words that matched the query is obtained. All their lists of occurrences are now merged to retrieve a list of documents and (if required) the matching text positions. If block addressing is used and the positions are required or the blocks do not coincide with the retrieval unit, the search must be completed with a sequential search over the blocks. Notice that an inverted index is word-oriented. Because of that it is not surprising that it is not able to efficiently find approximate matches or regular expressions that span many words. This is a restriction of this scheme. Variations that are not subject to this restriction have been proposed for languages which do not have a clear concept of word, like Finnish. They collect text samples or ngrams, which are fixed-length strings picked at regular text intervals. Searching is in general more powerful but more expensive. In a full-inverted index, search times for simple words allowing errors on 250 Mb of text took our reference machine from 0.6 to 0.85 seconds, while very complex expressions on extended patterns took from 0.8 to 3 seconds. As a comparison, the same collection cut in blocks of 1 Mb size takes more than 8 seconds for an approximate search with one error and more than 20 for two errors.    PATTERN MATCHING 221 Suffix Trees and Suffix Arrays If the suffix tree indexes all text positions it ca •. search for words, prefixes, suffixes and substrings with the same search algoriLm and cost described for word search. However, indexing all positions makes the index 10 to 20 times the text size for suffix trees. Range queries are easily solved too, by just searching both extremes in the trie and then collecting all the leaves which lie in the middle. In this case the cost is the height of the tree, which is o (log n) on average (excluding the tasks of collecting and sorting the leaves). Regular expressions can be searched in the suffix tree. The algorithm simply simulates sequential searching of the regular expression. It begins at the root, since any possible match starts there too. For each child of the current node labeled by the character c, it assumes that the next text character is c and recursively enters into that subtree. This is done for each of the children of the current node. The search stops only when the automaton has no transition to follow. It has been shown that for random text only O(nO:polylog(n)) nodes are traversed (for 0 &lt; a &lt; 1 dependent on the regular expression). Hence, the search time is sub linear for regular expressions without the restriction that they must occur inside a word. Extended patterns can be searched in the same way by taking them as regular expressions. Unrestricted approximate string matching is also possible using the same idea. We present a simplified version here. Imagine that the search is online and traverse the tree recursively as before. Since all suffixes start at the root, any match starts at the root too, and therefore do not allow the match to start later. The search will automatically stop at depth m \+ k at most (since at that point more than k errors have occurred). This implies constant search time if n is large enough (albeit exponential on m and k). Other problems such as approximate search of extended patterns can be solved in the same way, using the appropriate online algorithm. Suffix trees are able to perform other complex searches that we have not considered in our query language (see Chapter 4). These are specialized operations which are useful in specific areas. Some examples are: find the longest substring in the text that appears more than once, find the most common substring of a fixed size, etc. If a suffix array indexes all text positions, any algorithm that works on suffix trees at C(n) cost will work on suffix arrays at O(C(n) logn) cost. This is because the operations performed on the suffix tree consist of descending to a child node, which is done in 0(1) time. This operation can be simulated in the suffix array in O(logn) time by binary searching the new boundaries (each suffix tree node corresponds to a string, which can be mapped to the suffix array interval holding all suffixes starting with that string). Some patterns can be searched directly in the suffix array in O(log n) total search time without simulating the suffix tree. These are: word, prefix, suffix and subword search, as well as range search. However, again, indexing all text positions normally makes the suffix array    222 INDEXING AND SEARCHING size four times or more the text size. A different alternative for suffix arrays is to index only word beginnings and to use a vocabulary supra-index, using the same search algorithms used for the inverted lists. 8.7 Structural Queries The algorithms to search on structured text (see Chapter 4) are largely dependent on each model. We extract their common features in this section. A first concern about this problem is how to store the structural information. Some implementations build an ad hoc index to store the structure. This is potentially more efficient and independent of any consideration about the text. However, it requires extra development and maintenance effort. Other techniques assume that the structure is marked in the text using 'tags' (i.e., strings that identify the structural elements). This is the case with HTML text but not the case with C code where the marks are implicit and are inherent to C. The technique relies on the same index to query content (such as inverted files), using it to index and search those tags as if they were words. In many cases this is as efficient as an ad hoc index, and its integration into an existing text database is simpler. Moreover, it is possible to define the structure dynamically, since the appropriate tags can be selected at search time. For that goal, inverted files are better since they naturally deliver the results in text order, which makes the structure information easier to obtain. On the other hand, some queries such as direct ancestry are hard to answer without an ad hoc index. Once the content and structural elements have been found by using some index, a set of answers is generated. The models allow further operations to be applied on those answers, such as 'select all areas in the left-hand argument which contain an area of the right-hand argument.' This is in general solved in a way very similar to the set manipulation techniques already explained in section 8.4. However, the operations tend to be more complex, and it is not always possible to find an evaluation algorithm which has linear time with respect to the size of the intermediate results. It is worth mentioning that some models use completely different algorithms, such as exhaustive search techniques for tree pattern matching. Those problems are NP-complete in many cases. 8.8 Compression In this section we discuss the issues of searching compressed text directly and of searching compressed indices. Compression is important when available storage is a limiting factor, as is the case of indexing the Web. Searching and compression were traditionally regarded as exclusive operations. Texts which were not to be searched could be compressed, and to search  ",
2199,iir,iir-2199,8.6.3 Refining a deployed system," 8.6.3 Refining a deployed system If an IR system has been built and is being used by a large number of users, the system’s builders can evaluate possible changes by deploying variant versions of the system and recording measures that are indicative of user satisfaction with one variant vs. others as they are being used. This method is frequently used by web search engines. The most common version of this is A/B testing, a term borrowed from theA/B TEST advertising industry. For such a test, precisely one thing is changed between the current system and a proposed system, and a small proportion of traffic (say, 1–10% of users) is randomly directed to the variant system, while most users use the current system. For example, if we wish to investigate a change to the ranking algorithm, we redirect a random sample of users to a variant system and evaluate measures such as the frequency with which people click on the top result, or any result on the first page. (This particular analysis method is referred to as clickthrough log analysis or clickstream mining. It is further discussed as a method of implicit feedback in Section 9.1.7 (page 187).) The basis of A/B testing is running a bunch of single variable tests (either in sequence or in parallel): for each test only one parameter is varied from the control (the current live system). It is therefore easy to see whether varying each parameter has a positive or negative effect. Such testing of a live system can easily and cheaply gauge the effect of a change on users, and, with a large enough user base, it is practical to measure even very small positive and negative effects. In principle, more analytic power can be achieved by varying multiple things at once in an uncorrelated (random) way, and doing standard multivariate statistical analysis, such as multiple linear regression. In practice, though, A/B testing is widely used, because A/B tests are easy to deploy, easy to understand, and easy to explain to management. 8.7 Results snippets Having chosen or ranked the documents matching a query, we  ",8.6
2200,iir,iir-2200,8.7 Results snippets," 8.7 Results snippets Having chosen or ranked the documents matching a query, we wish to present a results list that will be informative to the user. In many cases the user will not want to examine all the returned documents and so we want to make the results list informative enough that the user can do a final ranking of the documents for themselves based on relevance to their information need.3The standard way of doing this is to provide a snippet, a short summary of the document, which is designed so as to allow the user to decide its relevance. Typically, the snippet consists of the document title and a short 3. There are exceptions, in domains where recall is emphasized. For instance, in many legal disclosure cases, a legal associate will review every document that matches a keyword search.   8.7 Results snippets 171 summary, which is automatically extracted. The question is how to design the summary so as to maximize its usefulness to the user. The two basic kinds of summaries are static, which are always the same regardless of the query, and dynamic (or query-dependent), which are customized according to the user’s information need as deduced from a query. Dynamic summaries attempt to explain why a particular document was retrieved for the query at hand. A static summary is generally comprised of either or both a subset of the document and metadata associated with the document. The simplest form of summary takes the first two sentences or 50 words of a document, or extracts particular zones of a document, such as the title and author. Instead of zones of a document, the summary can instead use metadata associated with the document. This may be an alternative way to provide an author or date, or may include elements which are designed to give a summary, such as the description metadata which can appear in the meta element of a web HTML page. This summary is typically extracted and cached at indexing time, in such a way that it can be retrieved and presented quickly when displaying search results, whereas having to access the actual document content might be a relatively expensive operation. There has been extensive work within natural language processing (NLP) on better ways to do text summarization. Most such work still aims only to choose sentences from the original document to present and concentrates on how to select good sentences. The models typically combine positional factors, favoring the first and last paragraphs of documents and the first and last sentences of paragraphs, with content factors, emphasizing sentences with key terms, which have low document frequency in the collection as a whole, but high frequency and good distribution across the particular document being returned. In sophisticated NLP approaches, the system synthesizes sentences for a summary, either by doing full text generation or by editing and perhaps combining sentences used in the document. For example, it might delete a relative clause or replace a pronoun with the noun phrase that it refers to. This last class of methods remains in the realm of research and is seldom used for search results: it is easier, safer, and often even better to just use sentences from the original document. Dynamic summaries display one or more “windows” on the document, aiming to present the pieces that have the most utility to the user in evaluating the document with respect to their information need. Usually these windows contain one or several of the query terms, and so are often referred to as keyword-in-context (KWIC) snippets, though sometimes they may still be pieces of the text such as the title that are selected for their queryindependent information value just as in the case of static summarization. Dynamic summaries are generated in conjunction with scoring. If the query is found as a phrase, occurrences of the phrase in the document will be      172 8 Evaluation in information retrieval ... In recent years, Papua New Guinea has faced severe economic difficulties and economic growth has slowed, partly as a result of weak governance and civil war, and partly as a result of external factors such as the Bougainville civil war which led to the closure in 1989 of the Panguna mine (at that time the most important foreign exchange earner and contributor to Government finances), the Asian financial crisis, a decline in the prices of gold and copper, and a fall in the production of oil. PNG’s economic development record over the past few years is evidence that governance issues underly many of the country’s problems. Good governance, which may be defined as the transparent and accountable management of human, natural, economic and financial resources for the purposes of equitable and sustainable development, flows from proper public sector management, efficient fiscal and accounting mechanisms, and a willingness to make service delivery a priority in practice. ... ◮Figure 8.5 An example of selecting text for a dynamic snippet. This snippet was generated for a document in response to the query new guinea economic development. The figure shows in bold italic where the selected snippet text occurred in the original document. shown as the summary. If not, windows within the document that contain multiple query terms will be selected. Commonly these windows may just stretch some number of words to the left and right of the query terms. This is a place where NLP techniques can usefully be employed: users prefer snippets that read well because they contain complete phrases. Dynamic summaries are generally regarded as greatly improving the usability of IR systems, but they present a complication for IR system design. A dynamic summary cannot be precomputed, but, on the other hand, if a system has only a positional index, then it cannot easily reconstruct the context surrounding search engine hits in order to generate such a dynamic summary. This is one reason for using static summaries. The standard solution to this in a world of large and cheap disk drives is to locally cache all the documents at index time (notwithstanding that this approach raises various legal, information security and control issues that are far from resolved) as shown in Figure 7.5 (page 147). Then, a system can simply scan a document which is about to appear in a displayed results list to find snippets containing the query words. Beyond simply access to the text, producing a good KWIC snippet requires some care. Given a variety of keyword occurrences in a document, the goal is to choose fragments which are: (i) maximally informative about the discussion of those terms in the document, (ii) self-contained enough to be easy to read, and (iii) short enough to fit within the normally strict constraints on the space available for summaries.      ",8.7
2150,mir,mir-2150,8.7 Struactual Queries,"  222 INDEXING AND SEARCHING size four times or more the text size. A different alternative for suffix arrays is to index only word beginnings and to use a vocabulary supra-index, using the same search algorithms used for the inverted lists. 8.7 Structural Queries The algorithms to search on structured text (see Chapter 4) are largely dependent on each model. We extract their common features in this section. A first concern about this problem is how to store the structural information. Some implementations build an ad hoc index to store the structure. This is potentially more efficient and independent of any consideration about the text. However, it requires extra development and maintenance effort. Other techniques assume that the structure is marked in the text using 'tags' (i.e., strings that identify the structural elements). This is the case with HTML text but not the case with C code where the marks are implicit and are inherent to C. The technique relies on the same index to query content (such as inverted files), using it to index and search those tags as if they were words. In many cases this is as efficient as an ad hoc index, and its integration into an existing text database is simpler. Moreover, it is possible to define the structure dynamically, since the appropriate tags can be selected at search time. For that goal, inverted files are better since they naturally deliver the results in text order, which makes the structure information easier to obtain. On the other hand, some queries such as direct ancestry are hard to answer without an ad hoc index. Once the content and structural elements have been found by using some index, a set of answers is generated. The models allow further operations to be applied on those answers, such as 'select all areas in the left-hand argument which contain an area of the right-hand argument.' This is in general solved in a way very similar to the set manipulation techniques already explained in section 8.4. However, the operations tend to be more complex, and it is not always possible to find an evaluation algorithm which has linear time with respect to the size of the intermediate results. It is worth mentioning that some models use completely different algorithms, such as exhaustive search techniques for tree pattern matching. Those problems are NP-complete in many cases.  ",
2151,mir,mir-2151,8.8 Compression," 8.8 Compression In this section we discuss the issues of searching compressed text directly and of searching compressed indices. Compression is important when available storage is a limiting factor, as is the case of indexing the Web. Searching and compression were traditionally regarded as exclusive operations. Texts which were not to be searched could be compressed, and to search    COMPRESSION 223 a compressed text it had to be decompressed first. In recent years, very efficient compression techniques have appeared that allow searching directly in the compressed text. Moreover, the search performance is improved, since the CPU times are similar but the disk times are largely reduced. This leads to a win-win situation. Discussion on how common text and lists of numbers can be compressed has been covered in Chapter 7\.  ",
2201,iir,iir-2201,8.8 References and further reading," 8.8 References and further reading 173 Generating snippets must be fast since the system is typically generating many snippets for each query that it handles. Rather than caching an entire document, it is common to cache only a generous but fixed size prefix of the document, such as perhaps 10,000 characters. For most common, short documents, the entire document is thus cached, but huge amounts of local storage will not be wasted on potentially vast documents. Summaries of documents whose length exceeds the prefix size will be based on material in the prefix only, which is in general a useful zone in which to look for a document summary anyway. If a document has been updated since it was last processed by a crawler and indexer, these changes will be neither in the cache nor in the index. In these circumstances, neither the index nor the summary will accurately reflect the current contents of the document, but it is the differences between the summary and the actual document content that will be more glaringly obvious to the end user. 8.8 References and further reading Definition and implementation of the notion of relevance to a query got off to a rocky start in 1953. Swanson (1988) reports that in an evaluation in that year between two teams, they agreed that 1390 documents were variously relevant to a set of 98 questions, but disagreed on a further 1577 documents, and the disagreements were never resolved. Rigorous formal testing of IR systems was first completed in the Cranfield experiments, beginning in the late 1950s. A retrospective discussion of the Cranfield test collection and experimentation with it can be found in (Cleverdon 1991). The other seminal series of early IR experiments were those on the SMART system by Gerard Salton and colleagues (Salton 1971b;1991). The TREC evaluations are described in detail by Voorhees and Harman (2005). Online information is available at http://trec.nist.gov/. Initially, few researchers computed the statistical significance of their experimental results, but the IR community increasingly demands this (Hull 1993). User studies of IR system effectiveness began more recently (Saracevic and Kantor 1988;1996). The notions of recall and precision were first used by Kent et al. (1955), although the term precision did not appear until later. The F measure (or,FMEASURE rather its complement E=1−F) was introduced by van Rijsbergen (1979). He provides an extensive theoretical discussion, which shows how adopting a principle of decreasing marginal relevance (at some point a user will be unwilling to sacrifice a unit of precision for an added unit of recall) leads to the harmonic mean being the appropriate method for combining precision and recall (and hence to its adoption rather than the minimum or geometric mean).     174 8 Evaluation in information retrieval Buckley and Voorhees (2000) compare several evaluation measures, including precision at k, MAP, and R-precision, and evaluate the error rate of each measure. R-precision was adopted as the official evaluation metric in the TREC HARD track (Allan 2005). Aslam and Yilmaz (2005) examine its surprisingly close correlation to MAP, which had been noted in earlier studies (Tague-Sutcliffe and Blustein 1995,Buckley and Voorhees 2000). A standard program for evaluating IR systems which computes many measures of ranked retrieval effectiveness is Chris Buckley’s trec_eval program used in the TREC evaluations. It can be downloaded from: http://trec.nist.gov/trec_eval/. Kekäläinen and Järvelin (2002) argue for the superiority of graded relevance judgments when dealing with very large document collections, and Järvelin and Kekäläinen (2002) introduce cumulated gain-based methods for IR system evaluation in this context. Sakai (2007) does a study of the stability and sensitivity of evaluation measures based on graded relevance judgments from NTCIR tasks, and concludes that NDCG is best for evaluating document ranking. Schamber et al. (1990) examine the concept of relevance, stressing its multidimensional and context-specific nature, but also arguing that it can be measured effectively. (Voorhees 2000) is the standard article for examining variation in relevance judgments and their effects on retrieval system scores and ranking for the TREC Ad Hoc task. Voorhees concludes that although the numbers change, the rankings are quite stable. Hersh et al. (1994) present similar analysis for a medical IR collection. In contrast, Kekäläinen (2005) analyze some of the later TRECs, exploring a 4-way relevance judgment and the notion of cumulative gain, arguing that the relevance measure used does substantially affect system rankings. See also Harter (1998). Zobel (1998) studies whether the pooling method used by TREC to collect a subset of documents that will be evaluated for relevance is reliable and fair, and concludes that it is. The kappa statistic and its use for language-related purposes is discussed by Carletta (1996). Many standard sources (e.g., Siegel and Castellan 1988) present pooled calculation of the expected agreement, but Di Eugenio and Glass (2004) argue for preferring the unpooled agreement (though perhaps presenting multiple measures). For further discussion of alternative measures of agreement, which may in fact be better, see Lombard et al. (2002) and Krippendorff (2003). Text summarization has been actively explored for many years. Modern work on sentence selection was initiated by Kupiec et al. (1995). More recent work includes (Barzilay and Elhadad 1997) and (Jing 2000), together with a broad selection of work appearing at the yearly DUC conferences and at other NLP venues. Tombros and Sanderson (1998) demonstrate the advantages of dynamic summaries in the IR context. Turpin et al. (2007) address how to generate snippets efficiently.   8.8 References and further reading 175 Clickthrough log analysis is studied in (Joachims 2002b,Joachims et al. 2005). In a series of papers, Hersh, Turpin and colleagues show how improvements in formal retrieval effectiveness, as evaluated in batch experiments, do not always translate into an improved system for users (Hersh et al. 2000a;b; 2001,Turpin and Hersh 2001;2002). User interfaces for IR and human factors such as models of human information seeking and usability testing are outside the scope of what we cover in this book. More information on these topics can be found in other textbooks, including (Baeza-Yates and Ribeiro-Neto 1999, ch. 10) and (Korfhage 1997), and collections focused on cognitive aspects (Spink and Cole 2005).  ",
2152,mir,mir-2152,8.8.1 Sequential Searching," 8.8.1 Sequential Searching A few approaches to directly searching compressed text exist. One of the most successful techniques in practice relies on Huffman coding taking words as symbols. That is, consider each different text word as a symbol, count their frequencies, and generate a Huffman codefor the words. Then, compress the text by replacing each word with its code. To improve compression/decompression efficiency, the Huffman code uses an alphabet of bytes instead of bits. This scheme compresses faster and better than known commercial systems, even those based on Ziv-Lempel coding. Since Huffman coding needs to store the codes of each symbol, this scheme has to store the whole vocabulary of the text, i.e, the list of all different text words. This is fully exploited to efficiently search complex queries. Although according to Heaps' law the vocabulary (i.e., the alphabet) grows as O(n 13 ) for o &lt; {3 &lt; 1, the generalized Zipf's law shows that the distribution is skewed enough so that the entropy remains constant (i.e., the compression ratio will not degrade as the text grows). Those laws are explained in Chapter 6\. Any single-word or pattern query is first searched in the vocabulary. Some queries can be binary searched, while others such as approximate searching or regular expression searching must traverse sequentially all the vocabulary. This vocabulary is rather small compared to the text size, thanks to Heaps' law. Notice that this process is exactly the same as the vocabulary searching performed by inverted indices, either for simple or complex pattern matching. Once that search is complete, the list of different words that match the query is obtained. The Huffman codes of all those words are collected and they are searched in the compressed text. One alternative is to traverse byte-wise the compressed text and traverse the Huffman decoding tree in synchronization, so that each time that a leaf is reached, it is checked whether the leaf (i.e., word) was marked as 'matching' the query or not. This is illustrated in Figure 8.24. Boyer-Moore filtering can be used to speed up the search. Solving phrases is a little more difficult. Each element is searched in the vocabulary. For each word of the vocabulary we define a bit mask. We set the i-th bit in the mask of all words which match with the i-th element of the phrase query. This is used together with the Shift-Or algorithm. The text is traversed byte-wise, and only when a leaf is reached, does the Shift-Or algorithm consider that a new text symbol has been read, whose bit mask is that of the leaf (see Figure 8.24). This algorithm is surprisingly simple and efficient.    224 INDEXING AND SEARCHING CiiiiiJ [X] c::::::J c:::J c::::::J CJ c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J ~ [X] c::::::J c:::J c:=J c:::J c:::::Jt:::J lJiiill c:=J CJ c::::::J c:::J ~ c:::J c==J c:::J ~ c:::J U2iLJ []M] [-=:J [X] [-=:J am I_I [X] ~ rnoJ ~ c:::J c==J c:::J Huffman tree Vocabulary Marks Huffman tree Vocabulary Marks Figure 8.24 On the left, searching for the simple pattern 'rose' allowing one error. On the right, searching for the phrase 'ro. rose is,' where 'ro.' represents a prefix search. This scheme is especially fast when it comes to solving a complex query (regular expression, extended pattern, approximate search, etc.) that would be slow with a normal algorithm. This is because the complex search is done only in the small vocabulary, after which the algorithm is largely insensitive to the complexity of the originating query. Its CPU times for a simple pattern are slightly higher than those of Agrep (briefly described in section 8.5.6). However, if the I/O times are considered, compressed searching is faster than all the online algorithms. For complex queries, this scheme is unbeaten by far. On the reference machine, the CPU times are 14 Mb/sec for any query, while for simple queries this improves to 18 Mb/sec if the speedup technique is used. Agrep, on the other hand, runs at 15 Mb/sec on simple searches and at 1-4 Mb/sec for complex ones. Moreover, I/O times are reduced to one third on the compressed text. 8.8.2 Compressed Indices Inverted Files Inverted files are quite amenable to compression. This is because the lists of occurrences are in increasing order of text position. Therefore, an obvious choice is to represent the differences between the previous position and the current one. These differences can be represented using less space by using techniques that favor small numbers (see Chapter 7). Notice that, the longer the lists, the smaller the differences. Reductions in 90% for block-addressing indices with blocks of 1 Kb size have been reported. It is important to notice that compression does not necessarily degrade time performance. Most of the time spent in answering a query is in the disk transfer. Keeping the index compressed allows the transference of less data, and it may be worth the CPU work of decompressing. Notice also that the lists of  ",
2153,mir,mir-2153,8.8.2 Compressed Indicies,"  224 INDEXING AND SEARCHING CiiiiiJ [X] c::::::J c:::J c::::::J CJ c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J c::::::J c:::J ~ [X] c::::::J c:::J c:=J c:::J c:::::Jt:::J lJiiill c:=J CJ c::::::J c:::J ~ c:::J c==J c:::J ~ c:::J U2iLJ []M] [-=:J [X] [-=:J am I_I [X] ~ rnoJ ~ c:::J c==J c:::J Huffman tree Vocabulary Marks Huffman tree Vocabulary Marks Figure 8.24 On the left, searching for the simple pattern 'rose' allowing one error. On the right, searching for the phrase 'ro. rose is,' where 'ro.' represents a prefix search. This scheme is especially fast when it comes to solving a complex query (regular expression, extended pattern, approximate search, etc.) that would be slow with a normal algorithm. This is because the complex search is done only in the small vocabulary, after which the algorithm is largely insensitive to the complexity of the originating query. Its CPU times for a simple pattern are slightly higher than those of Agrep (briefly described in section 8.5.6). However, if the I/O times are considered, compressed searching is faster than all the online algorithms. For complex queries, this scheme is unbeaten by far. On the reference machine, the CPU times are 14 Mb/sec for any query, while for simple queries this improves to 18 Mb/sec if the speedup technique is used. Agrep, on the other hand, runs at 15 Mb/sec on simple searches and at 1-4 Mb/sec for complex ones. Moreover, I/O times are reduced to one third on the compressed text. 8.8.2 Compressed Indices Inverted Files Inverted files are quite amenable to compression. This is because the lists of occurrences are in increasing order of text position. Therefore, an obvious choice is to represent the differences between the previous position and the current one. These differences can be represented using less space by using techniques that favor small numbers (see Chapter 7). Notice that, the longer the lists, the smaller the differences. Reductions in 90% for block-addressing indices with blocks of 1 Kb size have been reported. It is important to notice that compression does not necessarily degrade time performance. Most of the time spent in answering a query is in the disk transfer. Keeping the index compressed allows the transference of less data, and it may be worth the CPU work of decompressing. Notice also that the lists of    COMPRESSION 225 occurrences are normally traversed in a sequential manner, which is not affected by a differential compression. Query times on compressed or decompressed indices are reported to be roughly similar. The text can also be compressed independently of the index. The text will be decompressed only to display it, or to traverse it in case of block addressing. Notice in particular that the online search technique described for compressed text in section 8.8.1 uses a vocabulary. It is possible to integrate both techniques (compression and indexing) such that they share the same vocabulary for both tasks and they do not decompress the text to index or to search. Suffix Trees and Suffix Arrays Some efforts to compress suffix trees have been pursued. Important reductions of the space requirements have been obtained at the cost of more expensive searching. However, the reduced space requirements happen to be similar to those of uncompressed suffix arrays, which impose much smaller performance penalties. Suffix arrays are very hard to compress further. This is because they represent an almost perfectly random permutation of the pointers to the text. However, the subject of building suffix arrays on compressed text has been pursued. Apart from reduced space requirements (the index plus the compressed text take less space than the uncompressed text), the main advantage is that both index construction and querying almost double their performance. Construction is faster because more compressed text fits in the same memory space, and therefore fewer text blocks are needed. Searching is faster because a large part of the search time is spent in disk seek operations over the text area to compare suffixes. If the text is smaller, the seeks reduce proportionally. A compression technique very similar to that shown in section 8.8.1 is used. However, the Huffman code on words is replaced by a Hu-Tucker coding. The Hu- Tucker code respects the lexicographical relationships between the words, and therefore direct binary search over the compressed text is possible (this is necessary at construction and search time). This code is suboptimal by a very small percentage (2-3% in practice, with an analytical upper bound of 5%). Indexing times for 250 Mb of text on the reference machine are close to 1.6 Mb/rnin if compression is used, while query times are reduced to 0.5 seconds in total and 0.3 seconds for the text alone. Supra-indices should reduce the total search time to 0.15 seconds. Signature Files There are many alternative ways to compress signature files. All of them are based on the fact that only a few bits are set in the whole file. It is then possible    226 INDEXING AND SEARCHING to use efficient methods to code the bits which are not set, for instance run-length encoding. Different considerations arise if the file is stored as a sequence of bit masks or with one file per bit of the mask. They allow us to reduce space and hence disk times, or alternatively to increase B (so as to reduce the false drop probability) keeping the same space overhead. Compression ratios near 70% are reported.  ",
2154,mir,mir-2154,8.9 Trends and Research Issues," 8.9 Trends and Research Issues , # In this Chapter we covered extensively the current techniques of dealing with text retrieval. We first covered indices and then online searching. We then reviewed set manipulation, complex pattern matching and finally considered compression techniques. Figure 8.25 summarizes the 1ifadeoff between the space needed for the index and the time to search one single word. Space Complexity Suffix tries Suffix trees n O.ln n'""l Suffix arrays (full inversion) (block addressing) m Sequential search I Boyer-Moore and BDM famili~ \KMP \+ Shift-or Brute force m mlogn n'""l O.ln n mn Time Complexity Figure 8.25 Tradeoff of index space versus word searching time.    BIBLIOGRAPHIC DISCUSSION 227 Probably the most adequate indexing technique in practice is the inverted file. As we have shown throughout the chapter, many hidden details in other structures make them harder to use and less efficient in practice, as well as less flexible for dealing with new types of queries. These structures, however, still find application in restricted areas such as genetic databases (for suffix trees and arrays, for the relatively small texts used and their need to pose specialized queries) or some office systems (for signature files, because the text is rarely queried in fact). The main trends in indexing and searching textual databases today are • Text collections are becoming huge. This poses more demanding requirements at all levels, and solutions previously affordable are not any more. On the other hand, the speed of the processors and the relative slowness of external devices have changed what a few years ago were reasonable options (e.g., it is better to keep a text compressed because reading less text from disk and decompressing in main memory pays off). • Searching is becoming more complex. As the text databases grow and become more heterogeneous and error-prone, enhanced query facilities are required, such as exploiting the text structure or allowing errors in the text. Good support for extended queries is becoming important in the evaluation of a text retrieval system. • Compression is becoming a star in the field. Because of the changes mentioned in the time cost of processors and external devices, and because of new developments in the area, text retrieval and compression are no longer regarded as disjoint activities. Direct indexing and searching on compressed text provides better (sometimes much better) time performance and less space overhead at the same time. Other techniques such as block addressing trade space for processor time.  ",
2202,iir,iir-2202,9 Relevance feedback and query expansion,"       177 9Relevance feedback and query expansion In most collections, the same concept may be referred to using different words. This issue, known as synonymy, has an impact on the recall of most information retrieval systems. For example, you would want a search for aircraft to match plane (but only for references to an airplane, not a woodworking plane), and for a search on thermodynamics to match references to heat in appropriate discussions. Users often attempt to address this problem themselves by manually refining a query, as was discussed in Section 1.4; in this chapter we discuss ways in which a system can help with query refinement, either fully automatically or with the user in the loop. The methods for tackling this problem split into two major classes: global methods and local methods. Global methods are techniques for expanding or reformulating query terms independent of the query and results returned from it, so that changes in the query wording will cause the new query to match other semantically similar terms. Global methods include: •Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2) •Query expansion via automatic thesaurus generation (Section 9.2.3) •Techniques like spelling correction (discussed in Chapter 3) Local methods adjust a query relative to the documents that initially appear to match the query. The basic methods here are: •Relevance feedback (Section 9.1) •Pseudo relevance feedback, also known as Blind relevance feedback (Section 9.1.6) •(Global) indirect relevance feedback (Section 9.1.7) In this chapter, we will mention all of these approaches, but we will concentrate on relevance feedback, which is one of the most used and most successful approaches.   9 Relevance feedback and query expansion 9.1 Relevance feedback and pseudo relevan ",9.1
2203,iir,iir-2203,9.1 Relevance feedback and pseudo relevance feedback," 9.1 Relevance feedback and pseudo relevance feedback The idea of relevance feedback (RF) is to involve the user in the retrieval process so as to improve the final result set. In particular, the user gives feedback on the relevance of documents in an initial set of results. The basic procedure is: •The user issues a (short, simple) query. •The system returns an initial set of retrieval results. •The user marks some returned documents as relevant or nonrelevant. •The system computes a better representation of the information need based on the user feedback. •The system displays a revised set of retrieval results. Relevance feedback can go through one or more iterations of this sort. The process exploits the idea that it may be difficult to formulate a good query when you don’t know the collection well, but it is easy to judge particular documents, and so it makes sense to engage in iterative query refinement of this sort. In such a scenario, relevance feedback can also be effective in tracking a user’s evolving information need: seeing some documents may lead users to refine their understanding of the information they are seeking. Image search provides a good example of relevance feedback. Not only is it easy to see the results at work, but this is a domain where a user can easily have difficulty formulating what they want in words, but can easily indicate relevant or nonrelevant images. After the user enters an initial query for bike on the demonstration system at: http://nayana.ece.ucsb.edu/imsearch/imsearch.html the initial results (in this case, images) are returned. In Figure 9.1 (a), the user has selected some of them as relevant. These will be used to refine the query, while other displayed results have no effect on the reformulation. Figure 9.1 (b) then shows the new top-ranked results calculated after this round of relevance feedback. Figure 9.2 shows a textual IR example where the user wishes to find out about new applications of space satellites. 9.1.1 The Rocchio algorithm for relevance feedback The Rocchio Algorithm is the cl ",9.1
2204,iir,iir-2204,9.1.1 The Rocchio algorithm for relevance feedback," 9.1.1 The Rocchio algorithm for relevance feedback The Rocchio Algorithm is the classic algorithm for implementing relevance feedback. It models a way of incorporating relevance feedback information into the vector space model of Section 6.3.      9.1 Relevance feedback and pseudo relevance feedback 179 (a) (b) ◮Figure 9.1 Relevance feedback searching over images. (a) The user views the initial query results for a query of bike, selects the first, third and fourth result in the top row and the fourth result in the bottom row as relevant, and submits this feedback. (b) The users sees the revised result set. Precision is greatly improved. From http://nayana.ece.ucsb.edu/imsearch/imsearch.html (Newsam et al. 2001).     180 9 Relevance feedback and query expansion (a) Query: New space satellite applications (b) + 1. 0.539, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer + 2. 0.533, 07/09/91, NASA Scratches Environment Gear From Satellite Plan 3. 0.528, 04/04/90, Science Panel Backs NASA Satellite Plan, But Urges Launches of Smaller Probes 4. 0.526, 09/09/91, A NASA Satellite Project Accomplishes Incredible Feat: Staying Within Budget 5. 0.525, 07/24/90, Scientist Who Exposed Global Warming Proposes Satellites for Climate Research 6. 0.524, 08/22/90, Report Provides Support for the Critics Of Using Big Satellites to Study Climate 7. 0.516, 04/13/87, Arianespace Receives Satellite Launch Pact From Telesat Canada + 8. 0.509, 12/02/87, Telecommunications Tale of Two Companies (c) 2.074 new 15.106 space 30.816 satellite 5.660 application 5.991 nasa 5.196 eos 4.196 launch 3.972 aster 3.516 instrument 3.446 arianespace 3.004 bundespost 2.806 ss 2.790 rocket 2.053 scientist 2.003 broadcast 1.172 earth 0.836 oil 0.646 measure (d) * 1. 0.513, 07/09/91, NASA Scratches Environment Gear From Satellite Plan * 2. 0.500, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer 3. 0.493, 08/07/89, When the Pentagon Launches a Secret Satellite, Space Sleuths Do Some Spy Work of Their Own 4. 0.493, 07/31/89, NASA Uses ‘Warm’ Superconductors For Fast Circuit * 5. 0.492, 12/02/87, Telecommunications Tale of Two Companies 6. 0.491, 07/09/91, Soviets May Adapt Parts of SS-20 Missile For Commercial Use 7. 0.490, 07/12/88, Gaping Gap: Pentagon Lags in Race To Match the Soviets In Rocket Launchers 8. 0.490, 06/14/90, Rescue of Satellite By Space Agency To Cost $90 Million ◮Figure 9.2 Example of relevance feedback on a text collection. (a) The initial query (a). (b) The user marks some relevant documents (shown with a plus sign). (c) The query is then expanded by 18 terms with weights as shown. (d) The revised top results are then shown. A * marks the documents which were judged relevant in the relevance feedback phase.      9.1 Relevance feedback and pseudo relevance feedback 181 ◮Figure 9.3 The Rocchio optimal query for separating relevant and nonrelevant documents. The underlying theory. We want to find a query vector, denoted as ~q, that maximizes similarity with relevant documents while minimizing similarity with nonrelevant documents. If Cris the set of relevant documents and Cnr is the set of nonrelevant documents, then we wish to find:1 ~qopt =arg max ~q [sim(~q,Cr)−sim(~q,Cnr)],(9.1) where sim is defined as in Equation 6.10. Under cosine similarity, the optimal query vector~qopt for separating the relevant and nonrelevant documents is: ~qopt =1 |Cr|∑ ~ dj∈Cr ~ dj−1 |Cnr|∑ ~ dj∈Cnr ~ dj (9.2) That is, the optimal query is the vector difference between the centroids of the relevant and nonrelevant documents; see Figure 9.3. However, this observation is not terribly useful, precisely because the full set of relevant documents is not known: it is what we want to find. The Rocchio (1971) algorithm. This was the relevance feedback mechanism . In the equation, arg maxxf(x)returns a value of xwhich maximizes the value of the function f(x). Similarly, arg minxf(x)returns a value of xwhich minimizes the value of the function f(x).      182 9 Relevance feedback and query expansion ◮Figure 9.4 An application of Rocchio’s algorithm. Some documents have been labeled as relevant and nonrelevant and the initial query vector is moved in response to this feedback. nism introduced in and popularized by Salton’s SMART system around 1970. In a real IR query context, we have a user query and partial knowledge of known relevant and nonrelevant documents. The algorithm proposes using the modified query ~qm: ~qm=α~q0+β1 |Dr|∑ ~ dj∈Dr ~ dj−γ1 |Dnr|∑ ~ dj∈Dnr ~ dj (9.3) where q0is the original query vector, Drand Dnr are the set of known relevant and nonrelevant documents respectively, and α,β, and γare weights attached to each term. These control the balance between trusting the judged document set versus the query: if we have a lot of judged documents, we would like a higher βand γ. Starting from q0, the new query moves you some distance toward the centroid of the relevant documents and some distance away from the centroid of the nonrelevant documents. This new query can be used for retrieval in the standard vector space model (see Section 6.3). We can easily leave the positive quadrant of the vector space by subtracting off a nonrelevant document’s vector. In the Rocchio algorithm, negative term weights are ignored. That is, the term weight is set to 0. Figure 9.4 shows the effect of applying relevance feedback. Relevance feedback can improve both recall and precision. But, in practice, it has been shown to be most useful for increasing recall in situations     9.1 Relevance feedback and pseudo relevance feedback 183 where recall is important. This is partly because the technique expands the query, but it is also partly an effect of the use case: when they want high recall, users can be expected to take time to review results and to iterate on the search. Positive feedback also turns out to be much more valuable than negative feedback, and so most IR systems set γ&lt;β. Reasonable values might be α=1, β=0.75, and γ=0.15. In fact, many systems, such as the image search system in Figure 9.1, allow only positive feedback, which is equivalent to setting γ=0. Another alternative is to use only the marked nonrelevant document which received the highest ranking from the IR system as negative feedback (here, |Dnr|=1 in Equation (9.3)). While many of the experimental results comparing various relevance feedback variants are rather inconclusive, some studies have suggested that this variant, called Ide-dec-hi is the most effective or at least the most consistent performer. ✄ ",9.1
2205,iir,iir-2205,9.1.2 Probabilistic relevance feedback," 9.1.2 Probabilistic relevance feedback Rather than reweighting the query in a vector space, if a user has told us some relevant and nonrelevant documents, then we can proceed to build a classifier. One way of doing this is with a Naive Bayes probabilistic model. If Ris a Boolean indicator variable expressing the relevance of a document, then we can estimate P(xt=1|R), the probability of a term tappearing in a document, depending on whether it is relevant or not, as: ˆ P(xt=1|R=1) = |VRt|/|VR| (9.4) ˆ P(xt=1|R=0) = (d ft−|VRt|)/(N− |VR|) where Nis the total number of documents, d ftis the number that contain t,VR is the set of known relevant documents, and VRtis the subset of this set containing t. Even though the set of known relevant documents is a perhaps small subset of the true set of relevant documents, if we assume that the set of relevant documents is a small subset of the set of all documents then the estimates given above will be reasonable. This gives a basis for another way of changing the query term weights. We will discuss such probabilistic approaches more in Chapters 11 and 13, and in particular outline the application to relevance feedback in Section 11.3.4 (page 228). For the moment, observe that using just Equation (9.4) as a basis for term-weighting is likely insufficient. The equations use only collection statistics and information about the term distribution within the documents judged relevant. They preserve no memory of the original query. 9.1.3 When does relevance feedback work? The success of relevance feedback depends on certain assumptions. Firstly, the user has to have sufficient knowledge to be able to make an initial query  ",9.1
2206,iir,iir-2206,9.1.3 When does relevance feedback work?," 9.1.3 When does relevance feedback work? The success of relevance feedback depends on certain assumptions. Firstly, the user has to have sufficient knowledge to be able to make an initial query     184 9 Relevance feedback and query expansion which is at least somewhere close to the documents they desire. This is needed anyhow for successful information retrieval in the basic case, but it is important to see the kinds of problems that relevance feedback cannot solve alone. Cases where relevance feedback alone is not sufficient include: •Misspellings. If the user spells a term in a different way to the way it is spelled in any document in the collection, then relevance feedback is unlikely to be effective. This can be addressed by the spelling correction techniques of Chapter 3. •Cross-language information retrieval. Documents in another language are not nearby in a vector space based on term distribution. Rather, documents in the same language cluster more closely together. •Mismatch of searcher’s vocabulary versus collection vocabulary. If the user searches for laptop but all the documents use the term notebook computer, then the query will fail, and relevance feedback is again most likely ineffective. Secondly, the relevance feedback approach requires relevant documents to be similar to each other. That is, they should cluster. Ideally, the term distribution in all relevant documents will be similar to that in the documents marked by the users, while the term distribution in all nonrelevant documents will be different from those in relevant documents. Things will work well if all relevant documents are tightly clustered around a single prototype, or, at least, if there are different prototypes, if the relevant documents have significant vocabulary overlap, while similarities between relevant and nonrelevant documents are small. Implicitly, the Rocchio relevance feedback model treats relevant documents as a single cluster, which it models via the centroid of the cluster. This approach does not work as well if the relevant documents are a multimodal class, that is, they consist of several clusters of documents within the vector space. This can happen with: •Subsets of the documents using different vocabulary, such as Burma vs. Myanmar •A query for which the answer set is inherently disjunctive, such as Pop stars who once worked at Burger King. •Instances of a general concept, which often appear as a disjunction of more specific concepts, for example, felines. Good editorial content in the collection can often provide a solution to this problem. For example, an article on the attitudes of different groups to the situation in Burma could introduce the terminology used by different parties, thus linking the document clusters.     9.1 Relevance feedback and pseudo relevance feedback 185 Relevance feedback is not necessarily popular with users. Users are often reluctant to provide explicit feedback, or in general do not wish to prolong the search interaction. Furthermore, it is often harder to understand why a particular document was retrieved after relevance feedback is applied. Relevance feedback can also have practical problems. The long queries that are generated by straightforward application of relevance feedback techniques are inefficient for a typical IR system. This results in a high computing cost for the retrieval and potentially long response times for the user. A partial solution to this is to only reweight certain prominent terms in the relevant documents, such as perhaps the top 20 terms by term frequency. Some experimental results have also suggested that using a limited number of terms like this may give better results (Harman 1992) though other work has suggested that using more terms is better in terms of retrieved document quality (Buckley et al. 1994b).  ",9.1
2207,iir,iir-2207,9.1.4 Relevance feedback on the web," 9.1.4 Relevance feedback on the web Some web search engines offer a similar/related pages feature: the user indicates a document in the results set as exemplary from the standpoint of meeting his information need and requests more documents like it. This can be viewed as a particular simple form of relevance feedback. However, in general relevance feedback has been little used in web search. One exception was the Excite web search engine, which initially provided full relevance feedback. However, the feature was in time dropped, due to lack of use. On the web, few people use advanced search interfaces and most would like to complete their search in a single interaction. But the lack of uptake also probably reflects two other factors: relevance feedback is hard to explain to the average user, and relevance feedback is mainly a recall enhancing strategy, and web search users are only rarely concerned with getting sufficient recall. Spink et al. (2000) present results from the use of relevance feedback in the Excite search engine. Only about 4% of user query sessions used the relevance feedback option, and these were usually exploiting the “More like this” link next to each result. About 70% of users only looked at the first page of results and did not pursue things any further. For people who used relevance feedback, results were improved about two thirds of the time. An important more recent thread of work is the use of clickstream data (what links a user clicks on) to provide indirect relevance feedback. Use of this data is studied in detail in (Joachims 2002b,Joachims et al. 2005). The very successful use of web link structure (see Chapter 21) can also be viewed as implicit feedback, but provided by page authors rather than readers (though in practice most authors are also readers).     186 9 Relevance feedback and query expansion ?Exercise 9.1 In Rocchio’s algorithm, what weight setting for α/β/γdoes a “Find pages like this one” search correspond to? Exercise 9.2 [⋆] Give three reasons why relevance feedback has been little used in web search.  ",9.1
2208,iir,iir-2208,9.1.5 Evaluation of relevance feedback strategies," 9.1.5 Evaluation of relevance feedback strategies Interactive relevance feedback can give very substantial gains in retrieval performance. Empirically, one round of relevance feedback is often very useful. Two rounds is sometimes marginally more useful. Successful use of relevance feedback requires enough judged documents, otherwise the process is unstable in that it may drift away from the user’s information need. Accordingly, having at least five judged documents is recommended. There is some subtlety to evaluating the effectiveness of relevance feedback in a sound and enlightening way. The obvious first strategy is to start with an initial query q0and to compute a precision-recall graph. Following one round of feedback from the user, we compute the modified query qm and again compute a precision-recall graph. Here, in both rounds we assess performance over all documents in the collection, which makes comparisons straightforward. If we do this, we find spectacular gains from relevance feedback: gains on the order of 50% in mean average precision. But unfortunately it is cheating. The gains are partly due to the fact that known relevant documents (judged by the user) are now ranked higher. Fairness demands that we should only evaluate with respect to documents not seen by the user. A second idea is to use documents in the residual collection (the set of documents minus those assessed relevant) for the second round of evaluation. This seems like a more realistic evaluation. Unfortunately, the measured performance can then often be lower than for the original query. This is particularly the case if there are few relevant documents, and so a fair proportion of them have been judged by the user in the first round. The relative performance of variant relevance feedback methods can be validly compared, but it is difficult to validly compare performance with and without relevance feedback because the collection size and the number of relevant documents changes from before the feedback to after it. Thus neither of these methods is fully satisfactory. A third method is to have two collections, one which is used for the initial query and relevance judgments, and the second that is then used for comparative evaluation. The performance of both q0and qmcan be validly compared on the second collection. Perhaps the best evaluation of the utility of relevance feedback is to do user studies of its effectiveness, in particular by doing a time-based comparison:      9.1 Relevance feedback and pseudo relevance feedback 187 Precision at k=50 Term weighting no RF pseudo RF lnc.ltc 64.2% 72.7% Lnu.ltu 74.2% 87.0% ◮Figure 9.5 Results showing pseudo relevance feedback greatly improving performance. These results are taken from the Cornell SMART system at TREC 4 (Buckley et al. 1995), and also contrast the use of two different length normalization schemes (L vs. l); cf. Figure 6.15 (page 128). Pseudo relevance feedback consisted of adding 20 terms to each query. how fast does a user find relevant documents with relevance feedback vs. another strategy (such as query reformulation), or alternatively, how many relevant documents does a user find in a certain amount of time. Such notions of user utility are fairest and closest to real system usage.  ",9.1
2209,iir,iir-2209,9.1.6 Pseudo relevance feedback," 9.1.6 Pseudo relevance feedback Pseudo relevance feedback, also known as blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top kranked documents are relevant, and finally to do relevance feedback as before under this assumption. This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis (Section 9.2). It has been found to improve performance in the TREC ad hoc task. See for example the results in Figure 9.5. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. 9.1.7 Indirect relevance feedback We can also use indirect sources of evidence rather than explicit feedback on relevance as the basis for relevance feedback. This is often called implicit (relevance) feedback. Implicit feedback is less reliable than explicit feedback, but is more useful than pseudo relevance feedback, which contains no evidence of user judgments. Moreover, while users are often reluctant to provide explicit feedback, it is easy to collect implicit feedback in large quantities for a high volume system, such as a web search engine.  ",9.1
2210,iir,iir-2210,9.1.7 Indirect relevance feedback," 9.1.7 Indirect relevance feedback We can also use indirect sources of evidence rather than explicit feedback on relevance as the basis for relevance feedback. This is often called implicit (relevance) feedback. Implicit feedback is less reliable than explicit feedback, but is more useful than pseudo relevance feedback, which contains no evidence of user judgments. Moreover, while users are often reluctant to provide explicit feedback, it is easy to collect implicit feedback in large quantities for a high volume system, such as a web search engine.     188 9 Relevance feedback and query expansion On the web, DirectHit introduced the idea of ranking more highly documents that users chose to look at more often. In other words, clicks on links were assumed to indicate that the page was likely relevant to the query. This approach makes various assumptions, such as that the document summaries displayed in results lists (on whose basis users choose which documents to click on) are indicative of the relevance of these documents. In the original DirectHit search engine, the data about the click rates on pages was gathered globally, rather than being user or query specific. This is one form of the general area of clickstream mining. Today, a closely related approach is used in ranking the advertisements that match a web search query (Chapter 19).  ",9.1
2211,iir,iir-2211,9.1.8 Summary," 9.1.8 Summary Relevance feedback has been shown to be very effective at improving relevance of results. Its successful use requires queries for which the set of relevant documents is medium to large. Full relevance feedback is often onerous for the user, and its implementation is not very efficient in most IR systems. In many cases, other types of interactive retrieval may improve relevance by about as much with less work. Beyond the core ad hoc retrieval scenario, other uses of relevance feedback include: •Following a changing information need (e.g., names of car models of interest change over time) •Maintaining an information filter (e.g., for a news feed). Such filters are discussed further in Chapter 13. •Active learning (deciding which examples it is most useful to know the class of to reduce annotation costs). ?Exercise 9.3 Under what conditions would the modified query qmin Equation 9.3 be the same as the original query q0? In all other cases, is qmcloser than q0to the centroid of the relevant documents? Exercise 9.4 Why is positive feedback likely to be more useful than negative feedback to an IR system? Why might only using one nonrelevant document be more effective than using several? Exercise 9.5 Suppose that a user’s initial query is cheap CDs cheap DVDs extremely cheap CDs. The user examines two documents, d1and d2. She judges d1, with the content CDs cheap software cheap CDs relevant and d2with content cheap thrills DVDs nonrelevant. Assume that we are using direct term frequency (with no scaling and no document      ",9.1
2212,iir,iir-2212,9.2 Global methods for query reformulation," 9.2 Global methods for query reformulation 189 frequency). There is no need to length-normalize vectors. Using Rocchio relevance feedback as in Equation (9.3) what would the revised query vector be after relevance feedback? Assume α=1, β=0.75, γ=0.25. Exercise 9.6 [⋆] Omar has implemented a relevance feedback web search system, where he is going to do relevance feedback based only on words in the title text returned for a page (for efficiency). The user is going to rank 3 results. The first user, Jinxing, queries for: banana slug and the top three titles returned are: banana slug Ariolimax columbianus Santa Cruz mountains banana slug Santa Cruz Campus Mascot Jinxing judges the first two documents relevant, and the third nonrelevant. Assume that Omar’s search engine uses term frequency but no length normalization nor IDF. Assume that he is using the Rocchio relevance feedback mechanism, with α=β= γ=1. Show the final revised query that would be run. (Please list the vector elements in alphabetical order.) 9.2 Global methods for query reformulation In this section we more briefly discuss three global methods for expanding a query: by simply aiding the user in doing so, by using a manual thesaurus, and through building a thesaurus automatically. 9.2.1 Vocabulary tools for query refo ",9.2
2213,iir,iir-2213,9.2.1 Vocabulary tools for query reformulation," 9.2.1 Vocabulary tools for query reformulation Various user supports in the search process can help the user see how their searches are or are not working. This includes information about words that were omitted from the query because they were on stop lists, what words were stemmed to, the number of hits on each term or phrase, and whether words were dynamically turned into phrases. The IR system might also suggest search terms by means of a thesaurus or a controlled vocabulary. A user can also be allowed to browse lists of the terms that are in the inverted index, and thus find good terms that appear in the collection. 9.2.2 Query expansion In relevance feedback, users give additional input on documents (by marking documents in the results set as relevant or not), and this input is used to reweight the terms in the query for documents. In query expansion on the other hand, users give additional input on query words or phrases, possibly suggesting additional query terms. Some search engines (especially on the  ",9.2
2214,iir,iir-2214,9.2.2 Query expansion," 9.2.2 Query expansion In relevance feedback, users give additional input on documents (by marking documents in the results set as relevant or not), and this input is used to reweight the terms in the query for documents. In query expansion on the other hand, users give additional input on query words or phrases, possibly suggesting additional query terms. Some search engines (especially on the      190 9 Relevance feedback and query expansion ◮Figure 9.6 An example of query expansion in the interface of the Yahoo! web search engine in 2006. The expanded query suggestions appear just below the “Search Results” bar. web) suggest related queries in response to a query; the users then opt to use one of these alternative query suggestions. Figure 9.6 shows an example of query suggestion options being presented in the Yahoo! web search engine. The central question in this form of query expansion is how to generate alternative or expanded queries for the user. The most common form of query expansion is global analysis, using some form of thesaurus. For each term tin a query, the query can be automatically expanded with synonyms and related words of tfrom the thesaurus. Use of a thesaurus can be combined with ideas of term weighting: for instance, one might weight added terms less than original query terms. Methods for building a thesaurus for query expansion include: •Use of a controlled vocabulary that is maintained by human editors. Here, there is a canonical term for each concept. The subject headings of traditional library subject indexes, such as the Library of Congress Subject Headings, or the Dewey Decimal system are examples of a controlled vocabulary. Use of a controlled vocabulary is quite common for wellresourced domains. A well-known example is the Unified Medical Language System (UMLS) used with MedLine for querying the biomedical research literature. For example, in Figure 9.7,neoplasms was added to a      9.2 Global methods for query reformulation 191 • User query: cancer • PubMed query: (“neoplasms”[TIAB] NOT Medline[SB]) OR “neoplasms”[MeSH Terms] OR cancer[Text Word] • User query: skin itch • PubMed query: (“skin”[MeSH Terms] OR (“integumentary system”[TIAB] NOT Medline[SB]) OR “integumentary system”[MeSH Terms] OR skin[Text Word]) AND ((“pruritus”[TIAB] NOT Medline[SB]) OR “pruritus”[MeSH Terms] OR itch[Text Word]) ◮Figure 9.7 Examples of query expansion via the PubMed thesaurus. When a user issues a query on the PubMed interface to Medline at http://www.ncbi.nlm.nih.gov/entrez/, their query is mapped on to the Medline vocabulary as shown. search for cancer. This Medline query expansion also contrasts with the Yahoo! example. The Yahoo! interface is a case of interactive query expansion, whereas PubMed does automatic query expansion. Unless the user chooses to examine the submitted query, they may not even realize that query expansion has occurred. •A manual thesaurus. Here, human editors have built up sets of synonymous names for concepts, without designating a canonical term. The UMLS metathesaurus is one example of a thesaurus. Statistics Canada maintains a thesaurus of preferred terms, synonyms, broader terms, and narrower terms for matters on which the government collects statistics, such as goods and services. This thesaurus is also bilingual English and French. •An automatically derived thesaurus. Here, word co-occurrence statistics over a collection of documents in a domain are used to automatically induce a thesaurus; see Section 9.2.3. •Query reformulations based on query log mining. Here, we exploit the manual query reformulations of other users to make suggestions to a new user. This requires a huge query volume, and is thus particularly appropriate to web search. Thesaurus-based query expansion has the advantage of not requiring any user input. Use of query expansion generally increases recall and is widely used in many science and engineering fields. As well as such global analysis techniques, it is also possible to do query expansion by local analysis, for instance, by analyzing the documents in the result set. User input is now      192 9 Relevance feedback and query expansion Word Nearest neighbors absolutely absurd, whatsoever, totally, exactly, nothing bottomed dip, copper, drops, topped, slide, trimmed captivating shimmer, stunningly, superbly, plucky, witty doghouse dog, porch, crawling, beside, downstairs makeup repellent, lotion, glossy, sunscreen, skin, gel mediating reconciliation, negotiate, case, conciliation keeping hoping, bring, wiping, could, some, would lithographs drawings, Picasso, Dali, sculptures, Gauguin pathogens toxins, bacteria, organisms, bacterial, parasite senses grasp, psyche, truly, clumsy, naive, innate ◮Figure 9.8 An example of an automatically generated thesaurus. This example is based on the work in Schütze (1998), which employs latent semantic indexing (see Chapter 18). usually required, but a distinction remains as to whether the user is giving feedback on documents or on query terms.  ",9.2
2215,iir,iir-2215,9.2.3 Automatic thesaurus generation," 9.2.3 Automatic thesaurus generation As an alternative to the cost of a manual thesaurus, we could attempt to generate a thesaurus automatically by analyzing a collection of documents. There are two main approaches. One is simply to exploit word cooccurrence. We say that words co-occurring in a document or paragraph are likely to be in some sense similar or related in meaning, and simply count text statistics to find the most similar words. The other approach is to use a shallow grammatical analysis of the text and to exploit grammatical relations or grammatical dependencies. For example, we say that entities that are grown, cooked, eaten, and digested, are more likely to be food items. Simply using word cooccurrence is more robust (it cannot be misled by parser errors), but using grammatical relations is more accurate. The simplest way to compute a co-occurrence thesaurus is based on termterm similarities. We begin with a term-document matrix A, where each cell At,dis a weighted count wt,dfor term tand document d, with weighting so Ahas length-normalized rows. If we then calculate C=AAT, then Cu,vis a similarity score between terms uand v, with a larger number being better. Figure 9.8 shows an example of a thesaurus derived in basically this manner, but with an extra step of dimensionality reduction via Latent Semantic Indexing, which we discuss in Chapter 18. While some of the thesaurus terms are good or at least suggestive, others are marginal or bad. The quality of the associations is typically a problem. Term ambiguity easily introduces irrel      ",9.2
2216,iir,iir-2216,9.3 References and further reading," 9.3 References and further reading 193 evant statistically correlated terms. For example, a query for Apple computer may expand to Apple red fruit computer. In general these thesauri suffer from both false positives and false negatives. Moreover, since the terms in the automatic thesaurus are highly correlated in documents anyway (and often the collection used to derive the thesaurus is the same as the one being indexed), this form of query expansion may not retrieve many additional documents. Query expansion is often effective in increasing recall. However, there is a high cost to manually producing a thesaurus and then updating it for scientific and terminological developments within a field. In general a domainspecific thesaurus is required: general thesauri and dictionaries give far too little coverage of the rich domain-particular vocabularies of most scientific fields. However, query expansion may also significantly decrease precision, particularly when the query contains ambiguous terms. For example, if the user searches for interest rate, expanding the query to interest rate fascinate evaluate is unlikely to be useful. Overall, query expansion is less successful than relevance feedback, though it may be as good as pseudo relevance feedback. It does, however, have the advantage of being much more understandable to the system user. ?Exercise 9.7 If Ais simply a Boolean cooccurrence matrix, then what do you get as the entries in C? 9.3 References and further reading Work in information retrieval quickly confronted the problem of variant expression which meant that the words in a query might not appear in a document, despite it being relevant to the query. An early experiment about 1960 cited by Swanson (1988) found that only 11 out of 23 documents properly indexed under the subject toxicity had any use of a word containing the stem toxi. There is also the issue of translation, of users knowing what terms a document will use. Blair and Maron (1985) conclude that “it is impossibly difficult for users to predict the exact words, word combinations, and phrases that are used by all (or most) relevant documents and only (or primarily) by those documents”. The main initial papers on relevance feedback using vector space models all appear in Salton (1971b), including the presentation of the Rocchio algorithm (Rocchio 1971) and the Ide dec-hi variant along with evaluation of several variants (Ide 1971). Another variant is to regard all documents in the collection apart from those judged relevant as nonrelevant, rather than only ones that are explicitly judged nonrelevant. However, Schütze et al. (1995) and Singhal et al. (1997) show that better results are obtained for routing by using only documents close to the query of interest rather than all     194 9 Relevance feedback and query expansion documents. Other later work includes Salton and Buckley (1990), Riezler et al. (2007) (a statistical NLP approach to RF) and the recent survey paper Ruthven and Lalmas (2003). The effectiveness of interactive relevance feedback systems is discussed in (Salton 1989,Harman 1992,Buckley et al. 1994b). Koenemann and Belkin (1996) do user studies of the effectiveness of relevance feedback. Traditionally Roget’s thesaurus has been the best known English language thesaurus (Roget 1946). In recent computational work, people almost always use WordNet (Fellbaum 1998), not only because it is free, but also because of its rich link structure. It is available at: http://wordnet.princeton.edu. Qiu and Frei (1993) and Schütze (1998) discuss automatic thesaurus generation. Xu and Croft (1996) explore using both local and global query expansion.  ",9.3